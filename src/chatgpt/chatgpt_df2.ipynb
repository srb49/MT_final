{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import sys\\nsys.path.append(\"../my_lib/my_func1.py\")\\nfrom my_lib.my_func1 import *'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import statistics\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import language_tool_python  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gingerit.gingerit import GingerIt\n",
    "import pprint\n",
    "from spacy import displacy\n",
    "from sklearn.svm import SVC\n",
    "import textstat\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import math\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "'''import sys\n",
    "sys.path.append(\"../my_lib/my_func1.py\")\n",
    "from my_lib.my_func1 import *'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(df, column_index, feature):\n",
    "    # Get the column names\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Insert the new feature at the specified index\n",
    "    columns.insert(column_index, feature)\n",
    "\n",
    "    # Reindex the dataframe with the updated column order\n",
    "    df = df.reindex(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def walk_tree1(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        dep=[]\n",
    "        d=0\n",
    "        for child in node.children:\n",
    "            d=walk_tree1(child, depth + 1)\n",
    "            dep.append(d)\n",
    "        return max(dep)\n",
    "    \n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "def avg_sentence_complexity(str1):\n",
    "    final_list=[]\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    temp_list=[]\n",
    "    sents=[]\n",
    "    txt = str1\n",
    "    txt=\" \".join(txt.split())\n",
    "    text = nlp(txt)\n",
    "    sent_tokenize1 = (list(text.sents))\n",
    "    sents.append(sent_tokenize1[0])\n",
    "    for sent in text.sents:\n",
    "        d=walk_tree1(sent.root, 0)\n",
    "        temp_list.append(d)\n",
    "            #displacy.render(sent, style=\"dep\")\n",
    "        final_list.append(temp_list)\n",
    "        #print(\"essay first line \",sents[len(sents)-1],\"max is \",max(temp_list),\" min is \",min(temp_list),\" mean is \", round(statistics.mean(temp_list),3),\n",
    "        #  \" median is \",statistics.median(temp_list),\" mode is \",statistics.mode(temp_list))\n",
    "    return round(statistics.mean(temp_list),3)\n",
    "\n",
    "def grammar_check(str1):\n",
    "\n",
    "    my_tool = language_tool_python.LanguageTool('en-GB')###########\n",
    "    matches = my_tool.check(str1)\n",
    "    list_s1=[]\n",
    "    if(matches):\n",
    "        fs=str1.split(\".\")[0]\n",
    "        #print(i,\" index | essay first sentences \",fs)\n",
    "        for match in matches:\n",
    "            print(match)\n",
    "    #print(match.ruleId)\n",
    "    #print('Message:',match.message,\"\\n\")\n",
    "            #print(\"####################### \",match.context)\n",
    "            if match.replacements:\n",
    "                chopped_string = match.replacements[0].replace(',','').strip()\n",
    "                list_s1.append(chopped_string)\n",
    "            else:\n",
    "                list_s1.append(match.ruleId)    \n",
    "        #print(\"final list\",list_s1)\n",
    "    return len(list_s1)\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list1=[\" \".join(ngram) for ngram in ngrams]\n",
    "    #return [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list1\n",
    "\n",
    "def rep_count(str1):\n",
    "    tot_rp=[]\n",
    "    max_rp=[]\n",
    "    #print(ess[i],\"\\n\",\"new\",\"\\n\")\n",
    "    list1=generate_ngrams(str1, 5)\n",
    "    myset = set(list1)\n",
    "    rept = len(list1) - len(myset)\n",
    "    tot_rp.append(rept)\n",
    "    c = collections.Counter(list1)\n",
    "    c.most_common(1)[0][1]\n",
    "    max_rp.append(c.most_common(1)[0][1])\n",
    "    #print(\"\\n\\nnew essay index \",i,\": for essay \",list1[0],\":\")\n",
    "    print(\"repeat \",rept,\" times | most common phrase is **\",c.most_common(1)[0][0], \"** |number of repeataion happened: \",c.most_common(1)[0][1],\" times\\n\")\n",
    "    #print(\"\\ncommon phrases: \\n\",c)\n",
    "    #print(\"\\nmy set: \\n\",myset)\n",
    "    #print(\"list \",tot_rp,\" \",max_rp)\n",
    "    #print(\"individual \",rept,\" \",c.most_common(1)[0][1])\n",
    "    return rept,c.most_common(1)[0][1],c.most_common(10)\n",
    "\n",
    "def most_frequent(str1):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    str1=\" \".join(str1.split())\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop!= True and token.is_punct!= True]\n",
    "    w_c=Counter(words)\n",
    "    m_f=w_c.most_common(1)[0][1]\n",
    "    \n",
    "    tags=[]\n",
    "    for w in text:\n",
    "        tags.append(w.pos_)\n",
    "    tag_freq = Counter(tags)\n",
    "    #print (\"=====tag FREQUENCY=====\")\n",
    "    #print(tag_freq.values())\n",
    "\n",
    "    w_l=word_cnt(str1)\n",
    "    for key in tag_freq:\n",
    "        #print(\"hi\")\n",
    "        if key=='NOUN':\n",
    "            n=tag_freq[key]/w_l\n",
    "            #print(\"n \",n)\n",
    "        elif key == 'VERB':\n",
    "            v=tag_freq[key]/w_l\n",
    "            #print(\"v \",v)\n",
    "        elif key =='ADJ':\n",
    "            a=tag_freq[key]/w_l\n",
    "            #print(\"a \",a)\n",
    "        elif key =='PRON':\n",
    "            p=tag_freq[key]/w_l\n",
    "\n",
    "        elif key=='MD':\n",
    "            print(\"the number of modal verbs \",key, tag_freq.values())\n",
    "            #print(\"p \",p)\n",
    "        #print(\"key is \",key,\" value \",tag_freq[key], \" frequency \", tag_freq[key]/w_l)\n",
    "    return m_f,n,v,a,p\n",
    "\n",
    "def uniq_w(str1):\n",
    "    return len(set(str1.split()))\n",
    "\n",
    "def stop_w(str1):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    return(len(words))\n",
    "\n",
    "def stop_w_r(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    \n",
    "    l=len(words)\n",
    "    swr=l/n\n",
    "    return swr\n",
    "\n",
    "def sen_len(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    i=0\n",
    "    avg=0\n",
    "    sl=[]\n",
    "    for num in number_of_sentences:\n",
    "        i+=1\n",
    "        words=num.split()\n",
    "        avg+=len(words)\n",
    "        sl.append(len(words))\n",
    "    #print(num, len(num),len(words))\n",
    "    avg=avg/i\n",
    "    md=statistics.mode(sl)\n",
    "    mx=max(sl)\n",
    "    mn=min(sl)\n",
    "    return avg,md,mx,mn\n",
    "\n",
    "def sen_num(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "    #print(n)\n",
    "    return n\n",
    "\n",
    "def word_cnt(str1):\n",
    "    words=str1.split()\n",
    "    return len(words)\n",
    "\n",
    "def lexical_diversity(str1):\n",
    "    tokens = word_tokenize(str1)\n",
    "\n",
    "    lex_div = len(set(tokens)) / len(tokens)\n",
    "    print(\"Lexical Diversity Score of essay:\", lex_div)\n",
    "    return lex_div\n",
    "\n",
    "def discourse_conn(str1):\n",
    "    tokens = nltk.word_tokenize(str1)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Identify discourse connectors based on part-of-speech patterns\n",
    "    discourse_connectors = [token for token, pos in pos_tags if pos in ['CC', 'RB', 'IN']]\n",
    "\n",
    "    # Output the identified discourse connectors\n",
    "    print(\"Discourse connectors:\", discourse_connectors)\n",
    "    print(\"Discourse connector count:\", len(discourse_connectors))\n",
    "    return len(discourse_connectors)\n",
    "\n",
    "def sentence_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    sent_tokenize = (list(text.sents))\n",
    "    i=0\n",
    "    sensim=0.0\n",
    "    sensim_list=[]\n",
    "    for sent2 in sent_tokenize:\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            sent1=sent2\n",
    "            continue\n",
    "        #print (sent2.similarity(sent1))\n",
    "        sensim+=sent2.similarity(sent1)\n",
    "        sensim_score=sent2.similarity(sent1)\n",
    "        sent1=sent2\n",
    "        sensim_list.append(sensim_score)\n",
    "    sensim=sensim/i\n",
    "    #print(\"average sentence similarity is \",sensim)\n",
    "\n",
    "    return sensim,sensim_list\n",
    "\n",
    "def word_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    p=0\n",
    "    ps=0\n",
    "    n=0\n",
    "    ns=0\n",
    "    total=0\n",
    "    word = [token.text for token in text if token.is_punct != True]\n",
    "    for i in range(len(word)):\n",
    "    #x.append(i)\n",
    "    #print(i)\n",
    "    #print(words[i])\n",
    "        w2=word[i]\n",
    "        if i == 0:\n",
    "            w1=w2\n",
    "            continue\n",
    "    #print (nlp(w2).similarity(nlp(w1)))\n",
    "        #print(w1+ \" \"+w2+\" \",nlp(w2).similarity(nlp(w1)))\n",
    "        if nlp(w2).similarity(nlp(w1))>0:\n",
    "            p=p+1\n",
    "            ps+=nlp(w2).similarity(nlp(w1))\n",
    "        elif nlp(w2).similarity(nlp(w1))<0:\n",
    "            n=n+1\n",
    "            ns+=nlp(w2).similarity(nlp(w1))\n",
    "    #y1=y1+float(nlp(w2).similarity(nlp(w1)))\n",
    "    #print(y1)\n",
    "    #y.append(y1)\n",
    "        total+=nlp(w2).similarity(nlp(w1))\n",
    "        w1=w2\n",
    "    #print(p,n)\n",
    "    return p,n,total,ps,ns\n",
    "\n",
    "def get_modal_verbs(text):\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    cnt=0\n",
    "    modal_verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.tag_ == 'MD':\n",
    "            modal_verbs.append(token.lemma_)\n",
    "            cnt+=1\n",
    "\n",
    "    return cnt,modal_verbs\n",
    "\n",
    "def calculate_readability(text):\n",
    "    # Calculate the Flesch-Kincaid Grade Level\n",
    "    flesch_grade = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    # Calculate the Automated Readability Index (ARI)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    # Calculate the Coleman-Liau Index\n",
    "    coleman_liau = textstat.coleman_liau_index(text)\n",
    "\n",
    "    # Return the readability scores\n",
    "    return flesch_grade, ari, coleman_liau\n",
    "\n",
    "def calculate_cohesion(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Convert generator to a list of bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # Calculate bigram frequencies\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Calculate pointwise mutual information (PMI) for bigrams\n",
    "    total_bigrams = len(bigrams)\n",
    "    pmi_scores = {}\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        pmi = math.log(freq / total_bigrams / ((tokens.count(bigram[0]) / len(tokens)) * (tokens.count(bigram[1]) / len(tokens))))\n",
    "        pmi_scores[bigram] = pmi\n",
    "\n",
    "    # Calculate average PMI as a measure of cohesion\n",
    "    avg_pmi = sum(pmi_scores.values()) / len(pmi_scores)\n",
    "\n",
    "    return avg_pmi\n",
    "\n",
    "def calculate_uniqueness_score(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Calculate the frequency distribution of words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Calculate the uniqueness score as the ratio of unique words to total words\n",
    "    uniqueness_score = len(word_freq) / len(words)\n",
    "\n",
    "    return uniqueness_score\n",
    "\n",
    "def detect_emotional_tone(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "def para_cnt(str1):\n",
    "    paragraphs = str1.split(\"\\n\\n\")\n",
    "    return len(paragraphs)\n",
    "\n",
    "def discourse_marker():\n",
    "    with open('../other/da.txt',\"r\") as file:\n",
    "        text=file.read()\n",
    "    d_list = text.split(\",\")\n",
    "    #print(d_list)\n",
    "    d_list.remove('')\n",
    "    #index = d_list.index('')\n",
    "    #print(index)\n",
    "    #d_list.remove('')\n",
    "    return d_list\n",
    "\n",
    "def discourse_marker_check(essay):\n",
    "    d_list=discourse_marker()\n",
    "    count=0\n",
    "    for marker in d_list:\n",
    "        occurrences = re.findall(r'\\b' + re.escape(marker) + r'\\b', essay, flags=re.IGNORECASE)\n",
    "        count += len(occurrences)\n",
    "        #print(\"occurances \",occurrences,\" for marker \",marker)\n",
    "    #print(\"count \",count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write an ielts essay on Protect wild animals don't add titles, and heading of each paragraphs in the essay\n"
     ]
    }
   ],
   "source": [
    "str1=\"write an ielts essay on \"\n",
    "str2=\"Protect wild animals\"\n",
    "str3=\" don't add titles, and heading of each paragraphs in the essay\"\n",
    "str4=str1+str2+str3\n",
    "print(str4)\n",
    "essay_list=[str4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_280715/2088402255.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':reply,'topic':modified_string},ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "with open('../other/key1.h', 'r') as file:\n",
    " data = file.read()\n",
    "openai.api_key=data\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "]\n",
    "for i in range(len(essay_list)):\n",
    "    message = essay_list[i]\n",
    "\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "            )\n",
    "    chat_completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages\n",
    "        )\n",
    "    reply = chat_completion.choices[0].message.content\n",
    "    #print(reply)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    #substring = \"essay on \"\n",
    "    #modified_string = message.replace(substring, \"\")\n",
    "\n",
    "    excluded_beginning = \"write an ielts essay on \"\n",
    "    excluded_end = \" don't add titles, and heading of each paragraphs in the essay\"\n",
    "\n",
    "# Copy the string excluding the specified parts\n",
    "    modified_string = message[len(excluded_beginning):-len(excluded_end)]\n",
    "\n",
    "    lines = reply.splitlines()\n",
    "    if lines[0].startswith('As a helpful assistant, I will provide you with an essay on the topic'):\n",
    "        print(lines[1:])\n",
    "        print(lines[2:])\n",
    "        str1=\"\\n\".join(lines[2:])\n",
    "    elif lines[0].startswith(modified_string):\n",
    "        str1=\"\\n\".join(lines[2:])\n",
    "    else:\n",
    "       str1=reply\n",
    "    df=df.append({'essay':reply,'topic':modified_string},ignore_index=True)\n",
    "#print(\"message without essay\", message, modified_string)\n",
    "#print(\"the actual essay\", reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               essay  \\\n",
      "0  Family History and Future Generations\\n\\nIntro...   \n",
      "\n",
      "                                               topic  \n",
      "0  learn about family history or focus on present...  \n",
      "learn about family history or focus on present and future generation\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(df)\n",
    "print(modified_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In recent years, the conservation and protection of wild animals have become increasingly significant topics of discussion. It is crucial for humans to recognize the importance of safeguarding these creatures and take necessary measures to ensure their survival. This essay delves into the reasons why protecting wild animals is crucial, the threats they face, and the potential solutions.\n",
      "\n",
      "Firstly, wild animals play a vital role in maintaining ecological balance. Various species contribute to the delicate balance of their respective ecosystems by regulating populations of other organisms and preserving biodiversity. For instance, predatory animals help regulate the population of herbivores, thereby preventing overgrazing and the consequent degradation of vegetation.\n",
      "\n",
      "Furthermore, wildlife tourism has emerged as a significant source of income for many countries. Protected areas, such as national parks and wildlife reserves, attract millions of tourists annually. These visitors contribute to local economies by spending on accommodation, transportation, and various tourist services. Therefore, protecting wild animals not only helps maintain ecological equilibrium but also bolsters economic growth.\n",
      "\n",
      "However, despite their undeniable importance, wild animals face numerous threats that compromise their survival. Habitat loss due to deforestation, urbanization, and industrialization is one of the most significant challenges. Human activities encroach upon natural habitats, leaving animals with limited spaces to forage, breed, and roam freely. Additionally, wildlife trafficking and hunting for commercial purposes pose a severe threat to many endangered species.\n",
      "\n",
      "To address these challenges, several steps can be taken. First, governments should implement stricter regulations and penalties against wildlife trafficking. By increasing law enforcement efforts and imposing harsher punishments, the immoral practices of capturing and trading wild animals can be curtailed. Second, conservation organizations should work in collaboration with governments to establish and maintain protected areas that provide safe habitats for endangered species. These areas should also be equipped with proper monitoring systems, preventing unauthorized activities and ensuring the well-being of the animals.\n",
      "\n",
      "Furthermore, raising public awareness regarding the importance of protecting wild animals is vital. Education campaigns and programs at schools, universities, and public forums can play a pivotal role in informing the public about the consequences of wildlife exploitation. This awareness can lead to improved attitudes and responsible behavior towards wild animals and their habitats.\n",
      "\n",
      "In conclusion, the protection of wild animals is of utmost importance for the maintenance of ecological balance and the welfare of our planet. The threats faced by these creatures necessitate immediate action. Governments, conservation organizations, and the public must work collaboratively to implement stricter laws, establish protected areas, and raise awareness to ensure the preservation and safeguarding of wild animals for generations to come.\n",
      "Protect wild animals\n"
     ]
    }
   ],
   "source": [
    "ess=df.loc[:,'essay']\n",
    "print(ess[0])\n",
    "\n",
    "t=df.loc[:,'topic']\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#if first line is title/repeatation of the topic\n",
    "lines = reply.splitlines()\n",
    "ess=\"\\n\".join(lines[2:])\n",
    "print(ess)\n",
    "data.loc[:,'essay']=ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "data\n",
    "ess=data.loc[:,'essay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.loc[:,'essay']=ess\n",
    "data.loc[:,'topic']=modified_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Television has become a prevalent form of ente...</td>\n",
       "      <td>TELEVISION DOMINATES THE FREE-TIME OF TOO MANY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay  \\\n",
       "0  Television has become a prevalent form of ente...   \n",
       "\n",
       "                                               topic  \n",
       "0  TELEVISION DOMINATES THE FREE-TIME OF TOO MANY...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#print(data)\n",
    "print(data.loc[:,'topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat  0  times | most common phrase is ** in recent years the conservation ** |number of repeataion happened:  1  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.5051334702258727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_280715/4191345339.py:248: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if nlp(w2).similarity(nlp(w1))>0:\n",
      "/tmp/ipykernel_280715/4191345339.py:251: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  elif nlp(w2).similarity(nlp(w1))<0:\n",
      "/tmp/ipykernel_280715/4191345339.py:257: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  total+=nlp(w2).similarity(nlp(w1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the CSV file is: 40\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ess)):\n",
    "    pc=para_cnt(ess[i])\n",
    "    data.loc[i,'para_count']=pc\n",
    "\n",
    "    wc=word_cnt(ess[i])\n",
    "    data.loc[i,'word_count']=wc\n",
    "\n",
    "    ns=sen_num(ess[i])\n",
    "    data.loc[i,'sentence_count']=ns\n",
    "\n",
    "    avg,md,mx,mn=sen_len(ess[i])\n",
    "    data.loc[i,'sentence_len(mean)']=avg\n",
    "    data.loc[i,'sentence_len(mode)']=md\n",
    "    data.loc[i,'sentence_len(max)']=mx\n",
    "    data.loc[i,'sentence_len(min)']=mn\n",
    "\n",
    "    sw=stop_w(ess[i])\n",
    "    swr=stop_w_r(ess[i])\n",
    "    data.loc[i,'stop_words']=sw\n",
    "    data.loc[i,'sw_rates']=swr\n",
    "\n",
    "    uw=uniq_w(ess[i])\n",
    "    data.loc[i,'unique_words']=uw\n",
    "\n",
    "    uw=uniq_w(ess[i])\n",
    "    mf,n,v,a,p=most_frequent(ess[i])\n",
    "    mvc,mv=get_modal_verbs(ess[i])\n",
    "    data.loc[i,'noun']=n\n",
    "    data.loc[i,'verb']=v\n",
    "    data.loc[i,'adj']=a\n",
    "    data.loc[i,'pron']=p,\n",
    "    data.loc[i,'modal verb']=mvc\n",
    "    data.loc[i,'most_freq']=mf\n",
    "\n",
    "    tr,mr,rep_list=rep_count(ess[i])\n",
    "    data.loc[i,'number_of_repeatation']=tr\n",
    "    data.loc[i,'max_occurance_of_most_frequent_phrase']=mr\n",
    "\n",
    "    asc=avg_sentence_complexity(ess[i])\n",
    "    data.loc[i,'avg sentence complexity']=asc\n",
    "\n",
    "    l_d=lexical_diversity(ess[i])\n",
    "    data.loc[i,'lexical diversity']=l_d\n",
    "\n",
    "    sensim,sensim_list=sentence_similarity(ess[i])\n",
    "    sensim_list.sort()\n",
    "    sensim_mod=statistics.mode(sensim_list)\n",
    "    sensim_med=statistics.median(sensim_list)\n",
    "    data.loc[i,'sentence similarity mean']=sensim\n",
    "    data.loc[i,'sentence similarity median']=sensim_med\n",
    "    data.loc[i,'sentence similarity mode']=sensim_mod\n",
    "\n",
    "    pos,neg,tot,p_s,n_s=word_similarity(ess[i])\n",
    "    data.loc[i,'word_sim_p_count']=pos\n",
    "    data.loc[i,'word_sim_p_score']=p_s\n",
    "    data.loc[i,'word_sim_n_count']=neg\n",
    "    data.loc[i,'word_sim_n_score']=n_s\n",
    "    data.loc[i,'word_sim_score']=tot\n",
    "\n",
    "    f_g, ari, c_l = calculate_readability(ess[i])\n",
    "    data.loc[i,'flesch grade']=f_g\n",
    "    data.loc[i,'Automated Readability']=ari\n",
    "    data.loc[i,'coleman_liau']=c_l\n",
    "\n",
    "    coh_score=calculate_cohesion(ess[i])\n",
    "    data.loc[i,'cohesion score']=coh_score\n",
    "\n",
    "    uniq = calculate_uniqueness_score(ess[i])\n",
    "    data.loc[i,'uniqueness of words']=uniq\n",
    "\n",
    "    emot = detect_emotional_tone(ess[i])\n",
    "    data.loc[i,'positive_sentiment']=emot['pos']\n",
    "    data.loc[i,'negative_sentiment']=emot['neg']\n",
    "    data.loc[i,'neutral_sentiment']=emot['neu']\n",
    "\n",
    "    cnt=discourse_marker_check(ess[i])\n",
    "    data.loc[i,'discourse_marker']=cnt\n",
    "\n",
    "    data.loc[i,'label']=1\n",
    "\n",
    "print(f\"The number of columns in the CSV file is: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>topic</th>\n",
       "      <th>para_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>sentence_len(mean)</th>\n",
       "      <th>sentence_len(mode)</th>\n",
       "      <th>sentence_len(max)</th>\n",
       "      <th>sentence_len(min)</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch grade</th>\n",
       "      <th>Automated Readability</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>cohesion score</th>\n",
       "      <th>uniqueness of words</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>discourse_marker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In recent years, the conservation and protecti...</td>\n",
       "      <td>Protect wild animals</td>\n",
       "      <td>7.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.16</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.33</td>\n",
       "      <td>4.684587</td>\n",
       "      <td>0.505133</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.755</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay                 topic  \\\n",
       "0  In recent years, the conservation and protecti...  Protect wild animals   \n",
       "\n",
       "   para_count  word_count  sentence_count  sentence_len(mean)  \\\n",
       "0         7.0       429.0            25.0               17.16   \n",
       "\n",
       "   sentence_len(mode)  sentence_len(max)  sentence_len(min)  stop_words  ...  \\\n",
       "0                20.0               32.0                9.0       168.0  ...   \n",
       "\n",
       "   flesch grade  Automated Readability  coleman_liau  cohesion score  \\\n",
       "0          13.5                   17.0         18.33        4.684587   \n",
       "\n",
       "   uniqueness of words  positive_sentiment  negative_sentiment  \\\n",
       "0             0.505133               0.161               0.084   \n",
       "\n",
       "   neutral_sentiment  discourse_marker  label  \n",
       "0              0.755              56.0    1.0  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../csv/gpt/csv_gpt3-1.csv',mode='a',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../csv/gpt/csv_gpt3-1_2.csv',mode='a',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../csv/gpt/csv_gpt3-1.csv')\n",
    "ess=data.loc[:,'essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "def grammar_check1(text):\n",
    "# Create separate LanguageTool instances for GB and US English\n",
    "    tool_gb = language_tool_python.LanguageTool('en-GB')\n",
    "    tool_us = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "\n",
    "# Check for GB English grammar errors\n",
    "    matches_gb = tool_gb.check(text)\n",
    "    print(\"gb \",matches_gb)\n",
    "\n",
    "# Check for US English grammar errors\n",
    "    matches_us = tool_us.check(text)\n",
    "    print(\"us \",matches_us)\n",
    "\n",
    "    common_errors = [\n",
    "    error for error in matches_gb\n",
    "    if any(error.matches(match) for match in matches_us)\n",
    "    ]\n",
    "\n",
    "    print(\"Errors common to GB and US English:\")\n",
    "    for error in common_errors:\n",
    "        print(error)\n",
    "\n",
    "# Filter errors that do not match in either GB or US English\n",
    "#    non_matching_errors = [\n",
    "#        error for error in matches_gb + matches_us\n",
    "#        if not any(error.matches(match) for match in matches_gb + matches_us)\n",
    "#    ]\n",
    "\n",
    "#    print(\"Errors not matching in either GB or US English:\")\n",
    "#    for error in non_matching_errors:\n",
    "#        print(error)\n",
    "\n",
    "for i in range(len(ess)):\n",
    "    grammar_check1(ess[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
