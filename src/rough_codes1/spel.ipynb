{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grammar and spelling check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gingerit.gingerit import GingerIt\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'human/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEnter a filename: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mhuman/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mfilename, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m  data \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m my_tool \u001b[39m=\u001b[39m language_tool_python\u001b[39m.\u001b[39mLanguageTool(\u001b[39m'\u001b[39m\u001b[39men-GB\u001b[39m\u001b[39m'\u001b[39m) \n",
      "File \u001b[0;32m~/es/es_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'human/'"
     ]
    }
   ],
   "source": [
    "filename = input('Enter a filename: ')\n",
    "with open('human/'+filename, 'r') as file:\n",
    " data = file.read()\n",
    "my_tool = language_tool_python.LanguageTool('en-GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input('Enter a filename: ')\n",
    "with open('input/'+filename, 'r') as file:\n",
    " data = file.read()\n",
    "my_tool = language_tool_python.LanguageTool('en-US') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language tool python\n",
      "Offset 584, length 17, Rule ID: LARGE_NUMBER_OF\n",
      "Message: Specify a number, remove phrase, or simply use “many” or “numerous”\n",
      "Suggestion: many; numerous\n",
      "...of tourists. Take Cambodia for example, a large number of visitors coming to visit the Angkowat a...\n",
      "                                           ^^^^^^^^^^^^^^^^^\n",
      "many\n",
      "Offset 631, length 8, Rule ID: MORFOLOGIK_RULE_EN_GB\n",
      "Message: Possible spelling mistake found.\n",
      "Suggestion: Angola; Angkor; Angolan; Angora; Anakiwa; Aegrotat; Angoras\n",
      "... number of visitors coming to visit the Angkowat ancient temple need services like resta...\n",
      "                                           ^^^^^^^^\n",
      "Angola\n",
      "Offset 887, length 9, Rule ID: SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA\n",
      "Message: A comma may be missing after the conjunctive/linking adverb ‘Therefore’.\n",
      "Suggestion: Therefore,\n",
      "...ove infrastructure and living standard. Therefore tourism has clearly improved lives in t...\n",
      "                                           ^^^^^^^^^\n",
      "Therefore,\n",
      "Offset 1033, length 4, Rule ID: COMMA_COMPOUND_SENTENCE_2\n",
      "Message: Use a comma before “and” if it connects two independent clauses (unless they are closely connected and short).\n",
      "Suggestion: , and\n",
      "...many cultural values have been preserved and natural environments have been protecte...\n",
      "                                           ^^^^\n",
      ", and\n",
      "Offset 1157, length 5, Rule ID: MORFOLOGIK_RULE_EN_GB\n",
      "Message: Possible spelling mistake found.\n",
      "Suggestion: Strong; Wrong; Prong; Thong; Throng; Tong; Tring; Tron\n",
      "...al costumes and natural scenes, namely 'Trong Dong' drum performance and 'Ha Long' ba...\n",
      "                                           ^^^^^\n",
      "Strong\n",
      "ginger\n",
      "Cambodia, for instead of Cambodia for   Take Cambodia for example, a large number of visitors coming to visit the Angkowat ancient temple need services like restaurants, hotels, souvenir shops and other stores.\n",
      "the infrastructure instead of infrastructure   These demands trigger related business in the surrounding settings which in turn create many jobs for local people improve infrastructure and living standard.\n",
      "settings, which instead of settings which   These demands trigger related business in the surrounding settings which in turn create many jobs for local people improve infrastructure and living standard.\n",
      "the tourism industry instead of tourism industry   Secondly, through tourism industry, many cultural values have been preserved and natural environments have been protected.\n",
      "environmental instead of environment   To conclude, as far as I am concerned, international tourism has both triggered economic development and maintained cultural and environment values of the tourist countries.\n",
      "error of language tool  ['many', 'Angola', 'Therefore', '', 'Strong']\n",
      "error of ginger  ['Cambodia', 'the infrastructure', 'settings', 'the tourism industry', 'environmental']\n",
      "union error and number of error\n",
      "{'Therefore', '', 'Angola', 'Strong', 'settings', 'the tourism industry', 'environmental', 'the infrastructure', 'Cambodia', 'many'}   10\n"
     ]
    }
   ],
   "source": [
    "print(\"language tool python\")\n",
    "matches = my_tool.check(data)\n",
    "list_s1=[]\n",
    "for match in matches:\n",
    "    print(match)\n",
    "    #print(match.ruleId)\n",
    "    #print('Message:',match.message,\"\\n\")\n",
    "    print(match.replacements[0])\n",
    "    chopped_string = match.replacements[0].split(\",\")[0]\n",
    "    list_s1.append(chopped_string)\n",
    "\n",
    "#gingerit\n",
    "print(\"ginger\")\n",
    "list_sent = sent_tokenize(data)\n",
    "i=0\n",
    "list_s2=[]\n",
    "for sent in list_sent:\n",
    "    #print (\"\\nNEW\")    \n",
    "    ginger_parser = GingerIt()\n",
    "    ginger_grammar_results = ginger_parser.parse(sent)\n",
    "\n",
    "#pprint.pprint(ginger_grammar_results)\n",
    "\n",
    "    ginger_corrections = ginger_grammar_results['corrections']\n",
    "    for correction in ginger_corrections:\n",
    "        #print(\"\\t(Char #\" + str(correction['start']) + \") Use '\" + correction['correct'] + \"' instead of '\" + correction['text'] + \"'\")\n",
    "        if correction['correct'] != correction['text']:\n",
    "            i+=1\n",
    "            print(correction['correct'] + \" instead of \" + correction['text'], \" \",sent)\n",
    "            #print(correction['correct'])\n",
    "            chopped_string = correction['correct'].split(\",\")[0]\n",
    "            list_s2.append(chopped_string)\n",
    "\n",
    "error=set(list_s1).union(set(list_s2))\n",
    "\n",
    "print(\"error of language tool \",list_s1)\n",
    "print(\"error of ginger \",list_s2)\n",
    "print(\"union error and number of error\")\n",
    "print(error,\" \", len(error))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chatgpt uses americal english so language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It is every student's desire to study at a good university and experience a new environment. While some students study and live overseas to achieve this, some prefer to study home because of the difficulties of living and studying overseas. In my opinion, one who studies overseas will gain many skills throughout this experience for several reasons.\\n\\nFirst, studying at an overseas university gives individuals the opportunity to improve social skills by interacting and communicating with students from different origins and cultures. Compared to the peers studying in the home country, it will be more likely for the one who is living overseas to be successful in adapting himself/herself into new environments and situations in life.\\n\\nSecond, living and studying overseas is an irreplaceable experience when it comes to learn standing on your own feet. One who is living overseas will of course struggle with loneliness, living away from family and friends but those difficulties will turn into valuable experiences in the following steps of life. Moreover, the one will learn living without depending on anyone else.\\n\\nAlso, employers are mostly looking for people who have international and language skills. Becoming successful in this study will give the student an edge in job market. Therefore, one who has studied and lived overseas will become more eligible for the job than his/her peers.\\n\\nIn conclusion, there are many difficulties a student might face when studying and living overseas. However, living and studying overseas gives the individual a new perspective on the subject that is studied or in general life.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tool = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input('Enter a filename: ')\n",
    "with open('input/'+filename, 'r') as file:\n",
    " data = file.read()\n",
    "matches = my_tool.check(data)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)\n",
    "    print('Rule ID:',match.ruleId)\n",
    "    print('Message:',match.message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Technology has become an integral part of our lives, and it has greatly improved many aspects of our existence. From transportation to communication, medicine to entertainment, technology has made significant strides in making our lives easier and more convenient. However, it is important to recognize that technology alone cannot solve all the world's problems.\\nOne of the main limitations of technology is its inability to address social and cultural issues. While technology can provide tools to aid in communication and collaboration, it cannot change deeply ingrained attitudes and beliefs. For example, technology cannot solve issues such as racism or discrimination, as these issues are rooted in societal attitudes and beliefs that cannot be changed solely through technological advancements.\\nAdditionally, technology can also exacerbate existing problems, such as inequality and poverty. The digital divide, for example, refers to the gap between those who have access to technology and those who do not, and it can exacerbate existing socio-economic disparities. Furthermore, technology can also lead to job displacement, as automation and artificial intelligence are replacing human labor in many industries.\\nFurthermore, technology cannot replace human compassion and empathy. Many of the world's problems such as poverty, hunger, and homelessness, require not just technical solutions but also a human touch. The role of human compassion and empathy in addressing these issues cannot be replaced by technology, despite the advancements in the field of Artificial intelligence.\\nIn conclusion, while technology has greatly improved many aspects of our lives, it is important to recognize that it cannot solve all the world's problems. Social and cultural issues, inequality, and poverty, as well as the need for human compassion and empathy, cannot be addressed solely through technological advancements. To address these issues, a combination of technical solutions and human efforts is required.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for human generated text we use en-GB or en-US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tool = language_tool_python.LanguageTool('en-GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input('Enter a filename: ')\n",
    "with open('human/'+filename, 'r') as file:\n",
    " data = file.read()\n",
    "matches = my_tool.check(data)\n",
    "list_s1=[]\n",
    "for match in matches:\n",
    "    print(match)\n",
    "    #print(match.ruleId)\n",
    "    #print('Message:',match.message,\"\\n\")\n",
    "    print(\"####################### \",match.context)\n",
    "    print(match.replacements[0])\n",
    "    chopped_string = match.replacements[0].split(\",\")[0]\n",
    "    list_s1.append(chopped_string)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However', 'day when', 'Thus']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input('Enter a filename: ')\n",
    "with open('human/'+filename, 'r') as file:\n",
    " data = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sent = sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "However, I\n",
      "\n",
      "NEW\n",
      "role\n",
      "\n",
      "NEW\n",
      "Australia, for\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "Thus, this\n",
      "\n",
      "NEW\n",
      "the origin\n",
      "since\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "parents\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "tremendously\n",
      "\n",
      "NEW\n",
      "\n",
      "NEW\n",
      "societies\n",
      "cultural\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from gingerit.gingerit import GingerIt\n",
    "import pprint\n",
    "import spacy\n",
    "#text = \"I would like to buy one apples\"\n",
    "\n",
    "\n",
    "#text=\"It is always said that competition can effectively promote the development of economy. In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers. However, when we discuss the issue of competition or cooperation, what we are concerned about is not the whole society, but the development of an individual's whole life. From this point of view, I firmly believe that we should attach more importance to cooperation during primary education. First of all, through cooperation, children can learn about interpersonal skills which are significant in the future life of all students.\"\n",
    "i=0\n",
    "list_s2=[]\n",
    "for sent in list_sent:\n",
    "    print (\"\\nNEW\")\n",
    "    #print(sent)\n",
    "    \n",
    "    ginger_parser = GingerIt()\n",
    "    ginger_grammar_results = ginger_parser.parse(sent)\n",
    "\n",
    "#pprint.pprint(ginger_grammar_results)\n",
    "\n",
    "    ginger_corrections = ginger_grammar_results['corrections']\n",
    "\n",
    "    #pprint.pprint(ginger_corrections)\n",
    "\n",
    "    #print(\"\\nNumber of grammar issues found with Ginger: \" + str(len(ginger_corrections)) + \"\\n\")\n",
    "\n",
    "    for correction in ginger_corrections:\n",
    "        #print(\"\\t(Char #\" + str(correction['start']) + \") Use '\" + correction['correct'] + \"' instead of '\" + correction['text'] + \"'\")\n",
    "        if correction['correct'] != correction['text']:\n",
    "            i+=1\n",
    "            #print(correction['correct'] + \" instead of \" + correction['text'])\n",
    "            print(correction['correct'])\n",
    "            chopped_string = correction['correct'].split(\",\")[0]\n",
    "            list_s2.append(chopped_string)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However',\n",
       " 'role',\n",
       " 'Australia',\n",
       " 'Thus',\n",
       " 'the origin',\n",
       " 'since',\n",
       " 'parents',\n",
       " 'tremendously',\n",
       " 'societies',\n",
       " 'cultural']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The last decade has seen an increasing number of tourists travelling to visit natural wonder sights, ancient heritages and different cultures around the world. While some people might think that this international tourism has negative effects on the destination countries, I would contend that it has contributed to the economic development as well as preserved the culture and environment of the tourist destinations.\\n\\nFirstly, international tourism promotes many aspects of the destination country's economy in order to serve various demands of tourists. Take Cambodia for example, a large number of visitors coming to visit the Angkowat ancient temple need services like restaurants, hotels, souvenir shops and other stores. These demands trigger related business in the surrounding settings which in turn create many jobs for local people improve infrastructure and living standard. Therefore tourism has clearly improved lives in the tourist country.\\n\\nSecondly, through tourism industry, many cultural values have been preserved and natural environments have been protected. For instance, in Vietnam, many cultural costumes and natural scenes, namely 'Trong Dong' drum performance and 'Ha Long' bay, are being encouraged to preserve and funded by the tourism ministry. Without this support and profit from tourism, many traditional cultures would disappear due to its low income works. Thus, tourism has survived many non-tangible cultural values and beauty scenes.\\n\\nTo conclude, as far as I am concerned, international tourism has both triggered economic development and maintained cultural and environment values of the tourist countries. In addition, the authorities should adequately support these sustainable developments.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'However', 'Thus'}\n"
     ]
    }
   ],
   "source": [
    "common_elements = set(list_s1).intersection(list_s2)\n",
    "print(common_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"It is always said that competition can effectively promote the development of economy. In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers.\"\n",
    "ginger_parser = GingerIt()\n",
    "ginger_grammar_results = ginger_parser.parse(text)\n",
    "for correction in ginger_corrections:\n",
    "        #print(\"\\t(Char #\" + str(correction['start']) + \") Use '\" + correction['correct'] + \"' instead of '\" + correction['text'] + \"'\")\n",
    "        if correction['correct'] != correction['text']:\n",
    "            print(correction['correct'] + \" instead of \" + correction['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found.\n",
      "No errors found.\n",
      "No errors found.\n",
      "No errors found.\n",
      "Often, this adverbial phrase is redundant. Consider using an alternative. \t First of all, through cooperation, children can learn about interpersonal skills which are significant in the future life of all students.\n",
      "No errors found.\n",
      "No errors found.\n",
      "No errors found.\n",
      "No errors found.\n",
      "A comma may be missing after the conjunctive/linking adverb ‘Hence’. \t Hence it is always said that competition makes the society more effective.\n",
      "Did you mean simply “consider the”? You do not need the word “about” here. \t However, when we consider about the question that how to win the game, we always find that we need the cooperation.\n",
      "No errors found.\n",
      "If the term is a proper noun, use initial capitals. \t Take Olympic games which is a form of competition for instance, it is hard to imagine how an athlete could win the game without the training of his or her coach, and the help of other professional staffs such as the people who take care of his diet, and those who are in charge of the medical care.\n",
      "Use a comma before ‘but’ if it connects two independent clauses (unless they are closely connected and short). \t The winner is the athlete but the success belongs to the whole team.\n",
      "A comma may be missing after the conjunctive/linking adverb ‘Therefore’. \t Therefore without the cooperation, there would be no victory of competition.\n",
      "Possible missing comma found. \t Consequently, no matter from the view of individual development or the relationship between competition and cooperation we can receive the same conclusion that a more cooperative attitudes towards life is more profitable in one's success.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://languagetool.org/api/v2/check\"\n",
    "\n",
    "text = \"During the process of cooperation, children can learn about how to listen to opinions of others, how to communicate with others, how to think comprehensively, and even how to compromise with other team members when conflicts occurred.\"\n",
    "for sent in list_sent:\n",
    "    data = {\"text\": sent, \"language\": \"en-US\"}\n",
    "\n",
    "    response = requests.post(url, data=data)\n",
    "    result = response.json()\n",
    "\n",
    "# Ch    eck if there are any errors\n",
    "    if result[\"matches\"]:\n",
    "        print(result[\"matches\"][0][\"message\"],\"\\t\", sent)\n",
    "    else:\n",
    "        print(\"No errors found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Consequently, no matter from the view of individual development or the relationship between competition and cooperation we can receive the same conclusion that a more cooperative attitudes towards life is more profitable in one's success.\",\n",
       " 'language': 'en-US'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar error detected:\n",
      "Original text: It is always said that competition can effectively promote the development of economy.\n",
      "Corrected text: It is always said that competition can effectively promote the development of the economy.\n",
      "Grammar error detected:\n",
      "Original text: In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers.\n",
      "Corrected text: In order to survive in the competition, companies continue to improve their products and services, and as a result, the whole society prospers.\n",
      "Grammar error detected:\n",
      "Original text: First of all, through cooperation, children can learn about interpersonal skills which are significant in the future life of all students.\n",
      "Corrected text: First of all, through cooperation, children can learn about interpersonal skills which are significant in their future life.\n",
      "Grammar error detected:\n",
      "Original text: During the process of cooperation, children can learn about how to listen to opinions of others, how to communicate with others, how to think comprehensively, and even how to compromise with other team members when conflicts occurred.\n",
      "Corrected text: During the process of cooperation, children can learn about how to listen to the opinions of others, how to communicate with others, how to think comprehensively, and even how to compromise with other team members when conflicts occur.\n",
      "Grammar error detected:\n",
      "Original text: All of these skills help them to get on well with other people and will benefit them for the whole life.\n",
      "Corrected text: All of these skills help them to get along well with other people and will benefit them for the whole life.\n",
      "Grammar error detected:\n",
      "Original text: On the other hand, the significance of competition is that how to become more excellence to gain the victory.\n",
      "Corrected text: On the other hand, the significance of competition is that it teaches us how to become more excellence to gain the victory.\n",
      "Grammar error detected:\n",
      "Original text: Hence it is always said that competition makes the society more effective.\n",
      "Corrected text: Hence, it is always said that competition makes the society more effective.\n",
      "Grammar error detected:\n",
      "Original text: However, when we consider about the question that how to win the game, we always find that we need the cooperation.\n",
      "Corrected text: However, when we consider the question of how to win the game, we always find that we need cooperation.\n",
      "Grammar error detected:\n",
      "Original text: Take Olympic games which is a form of competition for instance, it is hard to imagine how an athlete could win the game without the training of his or her coach, and the help of other professional staffs such as the people who take care of his diet, and those who are in charge of the medical care.\n",
      "Corrected text: Take the Olympic Games for instance, it is hard to imagine how an athlete could win the competition without the training of his or her coach, and the help of other professional staff such as the people who take care of his diet, and those who are in charge of the medical care.\n",
      "Grammar error detected:\n",
      "Original text: The winner is the athlete but the success belongs to the whole team.\n",
      "Corrected text: The winner is the athlete, but the success belongs to the whole team.\n",
      "Grammar error detected:\n",
      "Original text: Therefore without the cooperation, there would be no victory of competition.\n",
      "Corrected text: Therefore, without the cooperation, there would be no victory in competition.\n",
      "Grammar error detected:\n",
      "Original text: Consequently, no matter from the view of individual development or the relationship between competition and cooperation we can receive the same conclusion that a more cooperative attitudes towards life is more profitable in one's success.\n",
      "Corrected text: Consequently, no matter from the view of individual development or the relationship between competition and cooperation, we can receive the same conclusion that a more cooperative attitude towards life is more profitable in one's success.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import re\n",
    "\n",
    "with open('other/key', 'r') as file:\n",
    " k = file.read()\n",
    "openai.api_key=k\n",
    "\n",
    "def check_grammar(text):\n",
    "    # Remove any URLs from the text (OpenAI API cannot handle URLs)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Generate a corrected version of the text using OpenAI's GPT-3 model\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=f\"Correct the grammar in the following text:\\n{text}\\n\\n\",\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    # Extract the corrected text from the API response\n",
    "    corrected_text = response.choices[0].text.strip()\n",
    "\n",
    "    # Print any grammar errors\n",
    "    if corrected_text != text:\n",
    "        print(f\"Grammar error detected:\\nOriginal text: {text}\\nCorrected text: {corrected_text}\")\n",
    "\n",
    "filename = input('Enter a filename: ')\n",
    "with open('human/'+filename, 'r') as file:\n",
    " data = file.read()\n",
    "list_sent = sent_tokenize(data)\n",
    "for sent in list_sent:\n",
    "    check_grammar(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar mistakes found:\n",
      "--------------------------------------------------\n",
      "Message: Often, this adverbial phrase is redundant. Consider using an alternative.\n",
      "Context: {'text': '... cooperation during primary education.  First of all, through cooperation, children can lear...', 'offset': 43, 'length': 12}\n",
      "Replacements: [{'value': 'First'}, {'value': 'Firstly'}, {'value': 'Foremost'}]\n",
      "--------------------------------------------------\n",
      "Message: A comma may be missing after the conjunctive/linking adverb ‘Hence’.\n",
      "Context: {'text': '...me more excellence to gain the victory. Hence it is always said that competition make...', 'offset': 43, 'length': 5}\n",
      "Replacements: [{'value': 'Hence,'}]\n",
      "--------------------------------------------------\n",
      "Message: Did you mean simply “consider the”? You do not need the word “about” here.\n",
      "Context: {'text': '...ociety more effective. However, when we consider about the question that how to win the game, we a...', 'offset': 43, 'length': 18}\n",
      "Replacements: [{'value': 'consider the'}]\n",
      "--------------------------------------------------\n",
      "Message: If the term is a proper noun, use initial capitals.\n",
      "Context: {'text': '... is, the more competition we need. Take Olympic games which is a form of competition for inst...', 'offset': 43, 'length': 13}\n",
      "Replacements: [{'value': 'Olympic Games'}]\n",
      "--------------------------------------------------\n",
      "Message: Use a comma before ‘but’ if it connects two independent clauses (unless they are closely connected and short).\n",
      "Context: {'text': '... medical care. The winner is the athlete but the success belongs to the whole team. ...', 'offset': 43, 'length': 4}\n",
      "Replacements: [{'value': ', but'}]\n",
      "--------------------------------------------------\n",
      "Message: A comma may be missing after the conjunctive/linking adverb ‘Therefore’.\n",
      "Context: {'text': '... the success belongs to the whole team. Therefore without the cooperation, there would be...', 'offset': 43, 'length': 9}\n",
      "Replacements: [{'value': 'Therefore,'}]\n",
      "--------------------------------------------------\n",
      "Message: Possible missing comma found.\n",
      "Context: {'text': '...he relationship between competition and cooperation we can receive the same conclusion that...', 'offset': 43, 'length': 11}\n",
      "Replacements: [{'value': 'cooperation,'}]\n",
      "--------------------------------------------------\n",
      "Message: The plural noun “attitudes” cannot be used with the article “a”. Did you mean “a more cooperative attitude” or “more cooperative attitudes”?\n",
      "Context: {'text': \"...we can receive the same conclusion that a more cooperative attitudes towards life is more profitable in one'...\", 'offset': 43, 'length': 28}\n",
      "Replacements: [{'value': 'a more cooperative attitude'}, {'value': 'more cooperative attitudes'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "filename = input('Enter a filename: ')\n",
    "with open('human/'+filename, 'r') as file:\n",
    " text = file.read()\n",
    "\n",
    "\n",
    "# Set up API endpoint and parameters\n",
    "api_url = \"https://api.languagetool.org/v2/check\"\n",
    "\n",
    "params = {\n",
    "        \"language\": \"en-US\",\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "# Send request to API and get response\n",
    "response = requests.post(api_url, data=params)\n",
    "data = response.json()\n",
    "\n",
    "# Print grammar mistakes, if any\n",
    "matches = data.get(\"matches\")\n",
    "if matches:\n",
    "    print(\"Grammar mistakes found:\")\n",
    "    for match in matches:\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Message: {match['message']}\")\n",
    "            print(f\"Context: {match['context']}\")\n",
    "            print(f\"Replacements: {match.get('replacements', [])}\")\n",
    "else:\n",
    "    print(\"No grammar mistakes found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement grammar-check (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for grammar-check\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install grammar-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
      "  Cloning https://github.com/PrithivirajDamodaran/Gramformer.git to /tmp/pip-req-build-kw9ut_k0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PrithivirajDamodaran/Gramformer.git /tmp/pip-req-build-kw9ut_k0\n",
      "  Resolved https://github.com/PrithivirajDamodaran/Gramformer.git to commit 23425cd2e98a919384cab6156af8adf1c9d0639a\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting errant\n",
      "  Downloading errant-2.3.3-py3-none-any.whl (499 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.5/499.5 kB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz>=2.0.0\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy<3,>=2.2.0\n",
      "  Downloading spacy-2.3.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Levenshtein==0.20.9\n",
      "  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (1.24.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.10.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (23.0)\n",
      "Requirement already satisfied: requests in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (4.64.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (2022.10.31)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./es_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->gramformer==1.0) (4.5.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (58.1.0)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (0.7.9)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (3.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./es_env/lib/python3.9/site-packages (from requests->transformers->gramformer==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./es_env/lib/python3.9/site-packages (from requests->transformers->gramformer==1.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./es_env/lib/python3.9/site-packages (from requests->transformers->gramformer==1.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./es_env/lib/python3.9/site-packages (from requests->transformers->gramformer==1.0) (3.0.1)\n",
      "Installing collected packages: wasabi, tokenizers, sentencepiece, plac, fuzzywuzzy, srsly, rapidfuzz, pyyaml, fsspec, filelock, catalogue, thinc, Levenshtein, huggingface-hub, transformers, spacy, python-Levenshtein, errant, gramformer\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 1.1.1\n",
      "    Uninstalling wasabi-1.1.1:\n",
      "      Successfully uninstalled wasabi-1.1.1\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 2.4.6\n",
      "    Uninstalling srsly-2.4.6:\n",
      "      Successfully uninstalled srsly-2.4.6\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 2.0.8\n",
      "    Uninstalling catalogue-2.0.8:\n",
      "      Successfully uninstalled catalogue-2.0.8\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.8\n",
      "    Uninstalling thinc-8.1.8:\n",
      "      Successfully uninstalled thinc-8.1.8\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.5.0\n",
      "    Uninstalling spacy-3.5.0:\n",
      "      Successfully uninstalled spacy-3.5.0\n",
      "\u001b[33m  DEPRECATION: gramformer is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for gramformer ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-lg 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 2.3.9 which is incompatible.\n",
      "confection 0.0.4 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Levenshtein-0.20.9 catalogue-1.0.2 errant-2.3.3 filelock-3.10.2 fsspec-2023.3.0 fuzzywuzzy-0.18.0 gramformer-1.0 huggingface-hub-0.13.3 plac-1.1.3 python-Levenshtein-0.20.9 pyyaml-6.0 rapidfuzz-2.13.7 sentencepiece-0.1.97 spacy-2.3.9 srsly-1.0.6 thinc-7.4.6 tokenizers-0.13.2 transformers-4.27.3 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./es_env/lib/python3.9/site-packages (2.3.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./es_env/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./es_env/lib/python3.9/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./es_env/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./es_env/lib/python3.9/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./es_env/lib/python3.9/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in ./es_env/lib/python3.9/site-packages (from spacy) (7.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./es_env/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./es_env/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./es_env/lib/python3.9/site-packages (from spacy) (0.7.9)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./es_env/lib/python3.9/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: setuptools in ./es_env/lib/python3.9/site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./es_env/lib/python3.9/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./es_env/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
      "  Cloning https://github.com/PrithivirajDamodaran/Gramformer.git to /tmp/pip-req-build-jxmugxp4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PrithivirajDamodaran/Gramformer.git /tmp/pip-req-build-jxmugxp4\n",
      "  Resolved https://github.com/PrithivirajDamodaran/Gramformer.git to commit 23425cd2e98a919384cab6156af8adf1c9d0639a\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers in ./es_env/lib/python3.9/site-packages (from gramformer==1.0) (4.27.3)\n",
      "Requirement already satisfied: sentencepiece in ./es_env/lib/python3.9/site-packages (from gramformer==1.0) (0.1.97)\n",
      "Requirement already satisfied: python-Levenshtein in ./es_env/lib/python3.9/site-packages (from gramformer==1.0) (0.20.9)\n",
      "Requirement already satisfied: fuzzywuzzy in ./es_env/lib/python3.9/site-packages (from gramformer==1.0) (0.18.0)\n",
      "Requirement already satisfied: tokenizers in ./es_env/lib/python3.9/site-packages (from gramformer==1.0) (0.13.2)\n",
      "Requirement already satisfied: fsspec in ./es_env/lib/python3.9/site-packages (from gramformer==1.0) (2023.3.0)\n",
      "Requirement already satisfied: errant in ./es_env/lib/python3.9/site-packages (from gramformer==1.0) (2.3.3)\n",
      "Collecting spacy<3,>=2.2.0\n",
      "  Using cached spacy-2.3.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: rapidfuzz>=2.0.0 in ./es_env/lib/python3.9/site-packages (from errant->gramformer==1.0) (2.13.7)\n",
      "Requirement already satisfied: Levenshtein==0.20.9 in ./es_env/lib/python3.9/site-packages (from python-Levenshtein->gramformer==1.0) (0.20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (2022.10.31)\n",
      "Requirement already satisfied: filelock in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (3.10.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (6.0)\n",
      "Requirement already satisfied: requests in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./es_env/lib/python3.9/site-packages (from transformers->gramformer==1.0) (1.24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./es_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->gramformer==1.0) (4.5.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.0.9)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (2.0.7)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Using cached srsly-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
      "Requirement already satisfied: setuptools in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (67.6.0)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Using cached thinc-7.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./es_env/lib/python3.9/site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (0.7.9)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./es_env/lib/python3.9/site-packages (from requests->transformers->gramformer==1.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./es_env/lib/python3.9/site-packages (from requests->transformers->gramformer==1.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./es_env/lib/python3.9/site-packages (from requests->transformers->gramformer==1.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./es_env/lib/python3.9/site-packages (from requests->transformers->gramformer==1.0) (3.0.1)\n",
      "Installing collected packages: srsly, catalogue, thinc, spacy\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 2.4.6\n",
      "    Uninstalling srsly-2.4.6:\n",
      "      Successfully uninstalled srsly-2.4.6\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 2.0.8\n",
      "    Uninstalling catalogue-2.0.8:\n",
      "      Successfully uninstalled catalogue-2.0.8\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.9\n",
      "    Uninstalling thinc-8.1.9:\n",
      "      Successfully uninstalled thinc-8.1.9\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.5.1\n",
      "    Uninstalling spacy-3.5.1:\n",
      "      Successfully uninstalled spacy-3.5.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 2.3.9 which is incompatible.\n",
      "en-core-web-lg 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 2.3.9 which is incompatible.\n",
      "confection 0.0.4 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed catalogue-1.0.2 spacy-2.3.9 srsly-1.0.6 thinc-7.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.15.1-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-2.0.1-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./es_env/lib/python3.9/site-packages (from torch) (3.10.2)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in ./es_env/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./es_env/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: setuptools in ./es_env/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.1.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./es_env/lib/python3.9/site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./es_env/lib/python3.9/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: requests in ./es_env/lib/python3.9/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./es_env/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./es_env/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./es_env/lib/python3.9/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./es_env/lib/python3.9/site-packages (from requests->torchvision) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./es_env/lib/python3.9/site-packages (from requests->torchvision) (1.26.14)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, lit, cmake, wheel, sympy, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, networkx, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision, torchaudio\n",
      "\u001b[33m  DEPRECATION: lit is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for lit ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed cmake-3.26.1 lit-16.0.0 mpmath-1.3.0 networkx-3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.11.1 torch-2.0.0 torchaudio-2.0.1 torchvision-0.15.1 triton-2.0.0 wheel-0.40.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gramformer import Gramformer\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_80621/1869320663.py\", line 1, in <module>\n",
      "    gf = Gramformer(models = 2, use_gpu=False)\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/gramformer/gramformer.py\", line 8, in __init__\n",
      "    self.annotator = errant.load('en')\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/errant/__init__.py\", line 16, in load\n",
      "    nlp = nlp or spacy.load(lang, disable=[\"ner\"])\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/spacy/__init__.py\", line 54, in load\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/spacy/util.py\", line 448, in load_model\n",
      "OSError: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "For more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/root/es/es_env/lib/python3.9/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "gf = Gramformer(models = 2, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m influent_sentences \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHe are moving here.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mI am doing fine. How is you?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwhat be the reason for everyone leave the company\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]   \n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m influent_sentence \u001b[39min\u001b[39;00m influent_sentences:\n\u001b[0;32m---> 16\u001b[0m     corrected_sentences \u001b[39m=\u001b[39m gf\u001b[39m.\u001b[39mcorrect(influent_sentence, max_candidates\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[Input] \u001b[39m\u001b[39m\"\u001b[39m, influent_sentence)\n\u001b[1;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m corrected_sentence \u001b[39min\u001b[39;00m corrected_sentences:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gf' is not defined"
     ]
    }
   ],
   "source": [
    "influent_sentences = [\n",
    "    \"He are moving here.\",\n",
    "    \"I am doing fine. How is you?\",\n",
    "    \"How is they?\",\n",
    "    \"Matt like fish\",\n",
    "    \"the collection of letters was original used by the ancient Romans\",\n",
    "    \"We enjoys horror movies\",\n",
    "    \"Anna and Mike is going skiing\",\n",
    "    \"I walk to the store and I bought milk\",\n",
    "    \" We all eat the fish and then made dessert\",\n",
    "    \"I will eat fish for dinner and drink milk\",\n",
    "    \"what be the reason for everyone leave the company\",\n",
    "]   \n",
    "\n",
    "for influent_sentence in influent_sentences:\n",
    "    corrected_sentences = gf.correct(influent_sentence, max_candidates=1)\n",
    "    print(\"[Input] \", influent_sentence)\n",
    "    for corrected_sentence in corrected_sentences:\n",
    "      print(\"[Correction] \",corrected_sentence)\n",
    "    print(\"-\" *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./es_env/lib/python3.9/site-packages (23.0.1)\n",
      "Requirement already satisfied: setuptools in ./es_env/lib/python3.9/site-packages (67.6.0)\n",
      "Requirement already satisfied: wheel in ./es_env/lib/python3.9/site-packages (0.40.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./es_env/lib/python3.9/site-packages (2.3.9)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./es_env/lib/python3.9/site-packages (from spacy) (23.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./es_env/lib/python3.9/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./es_env/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./es_env/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./es_env/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./es_env/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./es_env/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./es_env/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./es_env/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: setuptools in ./es_env/lib/python3.9/site-packages (from spacy) (67.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./es_env/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in ./es_env/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./es_env/lib/python3.9/site-packages (from spacy) (1.10.5)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Using cached thinc-8.1.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./es_env/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./es_env/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./es_env/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./es_env/lib/python3.9/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./es_env/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./es_env/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./es_env/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./es_env/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./es_env/lib/python3.9/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: catalogue, srsly, thinc, spacy\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 1.0.2\n",
      "    Uninstalling catalogue-1.0.2:\n",
      "      Successfully uninstalled catalogue-1.0.2\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.6\n",
      "    Uninstalling srsly-1.0.6:\n",
      "      Successfully uninstalled srsly-1.0.6\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.6\n",
      "    Uninstalling thinc-7.4.6:\n",
      "      Successfully uninstalled thinc-7.4.6\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.3.9\n",
      "    Uninstalling spacy-2.3.9:\n",
      "      Successfully uninstalled spacy-2.3.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "errant 2.3.3 requires spacy<3,>=2.2.0, but you have spacy 3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed catalogue-2.0.8 spacy-3.5.1 srsly-2.4.6 thinc-8.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in ./es_env/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: jinja2 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: setuptools in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./es_env/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./es_env/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./es_env/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./es_env/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./es_env/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./es_env/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./es_env/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "571e03498c9ff717ba98cc5850bdde0f83cd435f0d15600fd8a50ccbb08bc562"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
