{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cohmetrix (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cohmetrix\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install cohmetrix --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CoH-Metrix-UIMA'...\n",
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Coh-Metrix-Research-Team/CoH-Metrix-UIMA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cohmetrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Doc\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcohmetrix\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcx\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cohmetrix'"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import cohmetrix as cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def polarity_scores(doc):\n",
    "    return sentiment_analyzer.polarity_scores(doc.text)\n",
    "\n",
    "\n",
    "Doc.set_extension('polarity_scores', getter=polarity_scores, force=True)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "filename = input('Enter a filename: ')\n",
    "with open('input/'+filename, 'r') as file:\n",
    " data1 = file.read()\n",
    "doc = nlp(\"data1\")\n",
    "print(doc._.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input('Enter a filename: ')\n",
    "with open('input/'+filename, 'r') as file:\n",
    " data1 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# create get_sentiment function\n",
    "\n",
    "def get_sentiment(text):\n",
    "\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "\n",
    "    sentiment = 1 if scores['pos'] > 0 else 0\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "text1 = nlp(data1)\n",
    "d=0\n",
    "sent_tokenize1 = (list(text1.sents))\n",
    "for sent1 in sent_tokenize1:\n",
    "    d=int(get_sentiment(sent1))\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'compound': 0.6597}\n",
      "{'neg': 0.0, 'neu': 0.804, 'pos': 0.196, 'compound': 0.6808}\n",
      "{'neg': 0.121, 'neu': 0.879, 'pos': 0.0, 'compound': -0.4404}\n",
      "{'neg': 0.05, 'neu': 0.841, 'pos': 0.109, 'compound': 0.3384}\n",
      "{'neg': 0.394, 'neu': 0.606, 'pos': 0.0, 'compound': -0.3818}\n",
      "{'neg': 0.0, 'neu': 0.804, 'pos': 0.196, 'compound': 0.6808}\n",
      "{'neg': 0.292, 'neu': 0.708, 'pos': 0.0, 'compound': -0.7184}\n",
      "{'neg': 0.0, 'neu': 0.873, 'pos': 0.127, 'compound': 0.4404}\n",
      "{'neg': 0.0, 'neu': 0.896, 'pos': 0.104, 'compound': 0.4215}\n",
      "{'neg': 0.0, 'neu': 0.701, 'pos': 0.299, 'compound': 0.7501}\n",
      "{'neg': 0.08, 'neu': 0.875, 'pos': 0.045, 'compound': -0.3384}\n",
      "{'neg': 0.0, 'neu': 0.764, 'pos': 0.236, 'compound': 0.7574}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# sample text to analyze\n",
    "#text = \"I love spending time with my friends. They make me feel happy and loved.\"\n",
    "\n",
    "# analyze the sentiment of the text\n",
    "filename = input('Enter a filename: ')\n",
    "with open('input/'+filename, 'r') as file:\n",
    " data1 = file.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(data1)\n",
    "\n",
    "for sent in sentences:\n",
    "    sentiment_scores = sia.polarity_scores(sent)\n",
    "# print the sentiment scores\n",
    "    print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.6705}\n",
      "{'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.4404}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.859, 'pos': 0.141, 'compound': 0.4201}\n",
      "{'neg': 0.0, 'neu': 0.917, 'pos': 0.083, 'compound': 0.2023}\n",
      "{'neg': 0.0, 'neu': 0.881, 'pos': 0.119, 'compound': 0.5251}\n",
      "{'neg': 0.067, 'neu': 0.933, 'pos': 0.0, 'compound': -0.3818}\n",
      "{'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.7783}\n",
      "{'neg': 0.0, 'neu': 0.529, 'pos': 0.471, 'compound': 0.924}\n",
      "{'neg': 0.0, 'neu': 0.764, 'pos': 0.236, 'compound': 0.5256}\n",
      "{'neg': 0.0, 'neu': 0.84, 'pos': 0.16, 'compound': 0.5859}\n",
      "{'neg': 0.0, 'neu': 0.783, 'pos': 0.217, 'compound': 0.3612}\n",
      "{'neg': 0.021, 'neu': 0.784, 'pos': 0.195, 'compound': 0.91}\n",
      "{'neg': 0.0, 'neu': 0.596, 'pos': 0.404, 'compound': 0.8151}\n",
      "{'neg': 0.18, 'neu': 0.82, 'pos': 0.0, 'compound': -0.296}\n",
      "{'neg': 0.055, 'neu': 0.746, 'pos': 0.199, 'compound': 0.6997}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "filename = input('Enter a filename: ')\n",
    "with open('human/'+filename, 'r') as file:\n",
    " data1 = file.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(data1)\n",
    "\n",
    "for sent in sentences:\n",
    "    sentiment_scores = sia.polarity_scores(sent)\n",
    "# print the sentiment scores\n",
    "    print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "positive_score = sentiment_scores['neg']\n",
    "print(positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohmetrix as cx\n",
    "\n",
    "# Load text data\n",
    "text = \"The quick brown fox jumps over the lazy dog. This sentence is an example of cohesion because it uses repetition to link the two clauses.\"\n",
    "\n",
    "# Compute cohesion metrics\n",
    "cm = cx.CohMetrix(text)\n",
    "lexical_cohesion = cm.get_LexicalCohesion()\n",
    "entity_cohesion = cm.get_EntityCohesion()\n",
    "syntactic_cohesion = cm.get_SyntacticCohesion()\n",
    "connective_cohesion = cm.get_ConnectiveCohesion()\n",
    "\n",
    "# Print cohesion metrics\n",
    "print(\"Lexical cohesion:\", lexical_cohesion)\n",
    "print(\"Entity cohesion:\", entity_cohesion)\n",
    "print(\"Syntactic cohesion:\", syntactic_cohesion)\n",
    "print(\"Connective cohesion:\", connective_cohesion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
