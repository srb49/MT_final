{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\somrita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\somrita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\somrita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\somrita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "import statistics\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pprint\n",
    "from spacy import displacy\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import spacy\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import language_tool_python \n",
    "from nltk.tokenize import word_tokenize\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import textstat\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import math\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "#import autosklearn.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(df, column_index, feature):\n",
    "    # Get the column names\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Insert the new feature at the specified index\n",
    "    columns.insert(column_index, feature)\n",
    "\n",
    "    # Reindex the dataframe with the updated column order\n",
    "    df = df.reindex(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def walk_tree1(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        dep=[]\n",
    "        d=0\n",
    "        for child in node.children:\n",
    "            d=walk_tree1(child, depth + 1)\n",
    "            dep.append(d)\n",
    "        return max(dep)\n",
    "    \n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "def avg_sentence_complexity(str1):\n",
    "    final_list=[]\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    temp_list=[]\n",
    "    sents=[]\n",
    "    txt = str1\n",
    "    txt=\" \".join(txt.split())\n",
    "    text = nlp(txt)\n",
    "    sent_tokenize1 = (list(text.sents))\n",
    "    sents.append(sent_tokenize1[0])\n",
    "    for sent in text.sents:\n",
    "        d=walk_tree1(sent.root, 0)\n",
    "        temp_list.append(d)\n",
    "            #displacy.render(sent, style=\"dep\")\n",
    "        final_list.append(temp_list)\n",
    "        #print(\"essay first line \",sents[len(sents)-1],\"max is \",max(temp_list),\" min is \",min(temp_list),\" mean is \", round(statistics.mean(temp_list),3),\n",
    "        #  \" median is \",statistics.median(temp_list),\" mode is \",statistics.mode(temp_list))\n",
    "    return round(statistics.mean(temp_list),3)\n",
    "\n",
    "def grammar_check(text):\n",
    "    tool_gb = language_tool_python.LanguageTool('en-GB')\n",
    "\n",
    "    substr1 = \"is American English\"\n",
    "    substr2 = \"British English\"\n",
    "    substr3 = \"Consider shortening\"\n",
    "    \n",
    "    matches = tool_gb.check(text)\n",
    "    cnt=0\n",
    "    for match in matches:\n",
    "        if substr1 in match.message or substr2 in match.message or substr3 in match.message:\n",
    "            print(\"***********this the US case/case to ignore****************\")\n",
    "            print(match)\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            cnt+=1\n",
    "            print(\"***********this other cases****************\")\n",
    "            print(match)\n",
    "            \n",
    "    print(\"total count \",cnt)\n",
    "    return(cnt)\n",
    "'''\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list1=[\" \".join(ngram) for ngram in ngrams]\n",
    "    #return [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list1\n",
    "\n",
    "def rep_count(str1):\n",
    "    tot_rp=[]\n",
    "    max_rp=[]\n",
    "    #print(ess[i],\"\\n\",\"new\",\"\\n\")\n",
    "    list1=generate_ngrams(str1, 5)\n",
    "    myset = set(list1)\n",
    "    rept = len(list1) - len(myset)\n",
    "    tot_rp.append(rept)\n",
    "    c = collections.Counter(list1)\n",
    "    c.most_common(1)[0][1]\n",
    "    max_rp.append(c.most_common(1)[0][1])\n",
    "    #print(\"\\n\\nnew essay index \",i,\": for essay \",list1[0],\":\")\n",
    "    #print(\"repeat \",rept,\" times | most common phrase is **\",c.most_common(1)[0][0], \"** |number of repeataion happened: \",c.most_common(1)[0][1],\" times\\n\")\n",
    "    #print(\"\\ncommon phrases: \\n\",c)\n",
    "    #print(\"\\nmy set: \\n\",myset)\n",
    "    #print(\"list \",tot_rp,\" \",max_rp)\n",
    "    #print(\"individual \",rept,\" \",c.most_common(1)[0][1])\n",
    "    return rept,c.most_common(1)[0][1],c.most_common(10)\n",
    "'''\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list1=[\" \".join(ngram) for ngram in ngrams]\n",
    "    #return [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list1\n",
    "\n",
    "def rep_count(str1):\n",
    "    list1=generate_ngrams(str1, 5)\n",
    "    sentence_counter = Counter(list1)\n",
    "    #print(sentence_counter)\n",
    "    total_freq=0\n",
    "    total_rep=0\n",
    "    for sentence, frequency in sentence_counter.items():\n",
    "        if frequency > 1:\n",
    "            total_freq+=frequency\n",
    "            total_rep+=1\n",
    "            #print(f\"Sentence: '{sentence}' - Frequency: {frequency}\")\n",
    "    #print(\"number of sentences appeared more than once \",total_rep,\" and total number of frequency(that are>1) \",total_freq)\n",
    "    return total_freq,total_rep, sentence_counter.most_common(1)[0][1]\n",
    "\n",
    "def most_frequent(str1):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    str1=\" \".join(str1.split())\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop!= True and token.is_punct!= True]\n",
    "    w_c=Counter(words)\n",
    "    m_f=w_c.most_common(1)[0][1]\n",
    "    m_f_r=m_f/len(words)\n",
    "    return m_f,m_f_r\n",
    "\n",
    "\n",
    "def pos1(str1):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    str1=\" \".join(str1.split())\n",
    "    text = nlp(str1)\n",
    "    \n",
    "    \n",
    "    tags=[]\n",
    "    for w in text:\n",
    "        tags.append(w.pos_)\n",
    "    tag_freq = Counter(tags)\n",
    "    #print (\"=====tag FREQUENCY=====\")\n",
    "    #print(tag_freq.values())\n",
    "    n=0\n",
    "    v=0\n",
    "    a=0\n",
    "    p=0\n",
    "    \n",
    "    w_l=word_cnt(str1)\n",
    "    for key in tag_freq:\n",
    "        #print(\"hi\")\n",
    "        if key=='NOUN':\n",
    "            n=tag_freq[key]/w_l\n",
    "            #print(\"n \",n)\n",
    "        elif key == 'VERB':\n",
    "            v=tag_freq[key]/w_l\n",
    "            #print(\"v \",v)\n",
    "        elif key =='ADJ':\n",
    "            a=tag_freq[key]/w_l\n",
    "            #print(\"a \",a)\n",
    "        elif key =='PRON':\n",
    "            p=tag_freq[key]/w_l\n",
    "\n",
    "        #elif key=='MD':\n",
    "\n",
    "            #print(\"the number of modal verbs \",key, tag_freq.values())\n",
    "            #print(\"p \",p)\n",
    "        #print(\"key is \",key,\" value \",tag_freq[key], \" frequency \", tag_freq[key]/w_l)\n",
    "    return n,v,a,p\n",
    "\n",
    "def uniq_w(str1):\n",
    "    return len(set(str1.split()))\n",
    "\n",
    "def stop_w(str1):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    return(len(words))\n",
    "\n",
    "def avg_sw(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    \n",
    "    l=len(words)\n",
    "    swr=l/n\n",
    "    return swr\n",
    "\n",
    "def sw_rate(str1):\n",
    "    nw = word_tokenize(str1)\n",
    "    n=len(nw)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    \n",
    "\n",
    "    l=len(words)\n",
    "    #print(\"n and l \",n,l)\n",
    "    swr=l/n\n",
    "    return swr\n",
    "\n",
    "def sen_len(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    i=0\n",
    "    avg=0\n",
    "    sl=[]\n",
    "    for num in number_of_sentences:\n",
    "        i+=1\n",
    "        words=num.split()\n",
    "        avg+=len(words)\n",
    "        sl.append(len(words))\n",
    "    #print(num, len(num),len(words))\n",
    "    avg=avg/i\n",
    "    md=statistics.mode(sl)\n",
    "    mx=max(sl)\n",
    "    mn=min(sl)\n",
    "    return avg,md,mx,mn\n",
    "\n",
    "def sen_num(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "    #print(n)\n",
    "    return n\n",
    "\n",
    "def word_cnt(str1):\n",
    "    words=str1.split()\n",
    "    return len(words)\n",
    "\n",
    "def lexical_diversity(str1):\n",
    "    tokens = word_tokenize(str1)\n",
    "\n",
    "    lex_div = len(set(tokens)) / len(tokens)\n",
    "    return lex_div\n",
    "\n",
    "def discourse_conn(str1):\n",
    "    tokens = nltk.word_tokenize(str1)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Identify discourse connectors based on part-of-speech patterns\n",
    "    discourse_connectors = [token for token, pos in pos_tags if pos in ['CC', 'RB', 'IN']]\n",
    "\n",
    "    # Output the identified discourse connectors\n",
    "    #print(\"Discourse connectors:\", discourse_connectors)\n",
    "    #print(\"Discourse connector count:\", len(discourse_connectors))\n",
    "    return len(discourse_connectors)\n",
    "\n",
    "def sentence_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    sent_tokenize = (list(text.sents))\n",
    "    i=0\n",
    "    sensim=0.0\n",
    "    sensim_list=[]\n",
    "    for sent2 in sent_tokenize:\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            sent1=sent2\n",
    "            continue\n",
    "        #print (sent2.similarity(sent1))\n",
    "        sensim+=sent2.similarity(sent1)\n",
    "        sensim_score=sent2.similarity(sent1)\n",
    "        sent1=sent2\n",
    "        sensim_list.append(sensim_score)\n",
    "    sensim=sensim/i\n",
    "    #print(\"average sentence similarity is \",sensim)\n",
    "\n",
    "    return sensim,sensim_list\n",
    "\n",
    "def word_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    p=0\n",
    "    ps=0\n",
    "    n=0\n",
    "    ns=0\n",
    "    total=0\n",
    "    word = [token.text for token in text if token.is_punct != True]\n",
    "    for i in range(len(word)):\n",
    "    #x.append(i)\n",
    "    #print(i)\n",
    "    #print(words[i])\n",
    "        w2=word[i]\n",
    "        if i == 0:\n",
    "            w1=w2\n",
    "            continue\n",
    "    #print (nlp(w2).similarity(nlp(w1)))\n",
    "        #print(w1+ \" \"+w2+\" \",nlp(w2).similarity(nlp(w1)))\n",
    "        if nlp(w2).similarity(nlp(w1))>0:\n",
    "            p=p+1\n",
    "            ps+=nlp(w2).similarity(nlp(w1))\n",
    "        elif nlp(w2).similarity(nlp(w1))<0:\n",
    "            n=n+1\n",
    "            ns+=nlp(w2).similarity(nlp(w1))\n",
    "    #y1=y1+float(nlp(w2).similarity(nlp(w1)))\n",
    "    #print(y1)\n",
    "    #y.append(y1)\n",
    "        total+=nlp(w2).similarity(nlp(w1))\n",
    "        w1=w2\n",
    "    #print(p,n)\n",
    "    return p,n,total,ps,ns\n",
    "\n",
    "def get_modal_verbs(text):\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    cnt=0\n",
    "    modal_verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.tag_ == 'MD':\n",
    "            modal_verbs.append(token.lemma_)\n",
    "            cnt+=1\n",
    "    w_l=word_cnt(text)\n",
    "    cnt_rate=cnt/w_l\n",
    "\n",
    "    return cnt_rate,modal_verbs\n",
    "\n",
    "def calculate_readability(text):\n",
    "    # Calculate the Flesch-Kincaid Grade Level\n",
    "    flesch_grade = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    # Calculate the Automated Readability Index (ARI)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    # Calculate the Coleman-Liau Index\n",
    "    coleman_liau = textstat.coleman_liau_index(text)\n",
    "\n",
    "    # Return the readability scores\n",
    "    return flesch_grade, ari, coleman_liau\n",
    "\n",
    "def calculate_cohesion(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Convert generator to a list of bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # Calculate bigram frequencies\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Calculate pointwise mutual information (PMI) for bigrams\n",
    "    total_bigrams = len(bigrams)\n",
    "    pmi_scores = {}\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        pmi = math.log(freq / total_bigrams / ((tokens.count(bigram[0]) / len(tokens)) * (tokens.count(bigram[1]) / len(tokens))))\n",
    "        pmi_scores[bigram] = pmi\n",
    "\n",
    "    # Calculate average PMI as a measure of cohesion\n",
    "    avg_pmi = sum(pmi_scores.values()) / len(pmi_scores)\n",
    "\n",
    "    return avg_pmi\n",
    "\n",
    "def calculate_uniqueness_score(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Calculate the frequency distribution of words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Calculate the uniqueness score as the ratio of unique words to total words\n",
    "    uniqueness_score = len(word_freq) / len(words)\n",
    "\n",
    "    return uniqueness_score\n",
    "\n",
    "def detect_emotional_tone(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "def para_cnt(str1):\n",
    "    paragraphs = str1.split(\"\\n\\n\")\n",
    "    return len(paragraphs)\n",
    "\n",
    "def discourse_marker():\n",
    "    with open('../other/da.txt',\"r\") as file:\n",
    "        text=file.read()\n",
    "    d_list = text.split(\",\")\n",
    "    #print(d_list)\n",
    "    d_list.remove('')\n",
    "    #index = d_list.index('')\n",
    "    #print(index)\n",
    "    #d_list.remove('')\n",
    "    return d_list\n",
    "\n",
    "def discourse_marker_check(essay):\n",
    "    d_list=discourse_marker()\n",
    "    count=0\n",
    "    for marker in d_list:\n",
    "        occurrences = re.findall(r'\\b' + re.escape(marker) + r'\\b', essay, flags=re.IGNORECASE)\n",
    "        count += len(occurrences)\n",
    "        #print(\"occurances \",occurrences,\" for marker \",marker)\n",
    "    #print(\"count \",count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_unknown():\n",
    "    #str1=input(\"enter the essay\")\n",
    "    with open('file.txt', 'r') as file:\n",
    "        str1 = file.read()\n",
    "    pc=para_cnt(str1)\n",
    "    wc=word_cnt(str1)\n",
    "    ns=sen_num(str1)\n",
    "    avg,md,mx,mn=sen_len(str1) \n",
    "    sw=stop_w(str1) \n",
    "    asw=avg_sw(str1)\n",
    "    swr=sw_rate(str1)\n",
    "    uw=uniq_w(str1)\n",
    "    n,v,a,p=pos1(str1)\n",
    "    mvc,mv=get_modal_verbs(str1)\n",
    "    mf,mfr=most_frequent(str1)\n",
    "    #tr,mr,rep_list=rep_count(str1)\n",
    "    tf,tr,max_rep=rep_count(str1)\n",
    "    asc=avg_sentence_complexity(str1)\n",
    "    l_d=lexical_diversity(str1)\n",
    "    sensim,sensim_list=sentence_similarity(str1)\n",
    "    sensim_list.sort()\n",
    "    sensim_mod=statistics.mode(sensim_list)\n",
    "    sensim_med=statistics.median(sensim_list)\n",
    "    pos,neg,tot,p_s,n_s=word_similarity(str1)\n",
    "    f_g, ari, c_l = calculate_readability(str1)\n",
    "    coh_score=calculate_cohesion(str1)\n",
    "    uniq = calculate_uniqueness_score(str1)\n",
    "    emot = detect_emotional_tone(str1)\n",
    "    cnt=discourse_marker_check(str1)\n",
    "    gm=grammar_check(str1)\n",
    "    '''\n",
    "    feature_list=[pc,wc,ns,avg,mx,mn,swr,uw,uniq,n,v,a,p,mvc,mf,mfr,tr,mr,\n",
    "              asc,l_d,pos,p_s,neg,n_s,tot,\n",
    "              f_g, ari, c_l,coh_score,uniq,emot['pos'],\n",
    "              emot['neg'],emot['neu'],cnt,gm]\n",
    "    '''\n",
    "\n",
    "    feature_list=[pc,wc,ns,avg,mx,asw,uw,n,v,a,p,mf,mfr,asc,l_d,\n",
    "                  pos,p_s,neg,n_s,tot,f_g,ari,c_l,emot['pos'],emot['neu'],cnt,\n",
    "                  gm,swr,tf,tr,max_rep]\n",
    "    \n",
    "    print(\"feature list len \",len(feature_list),\"**** feature list*****\",feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col=[pc,wc,ns,avg,mx,asw,uw,n,v,a,p,mf,mfr,asc,l_d,pos,p_s,neg,n_s,tot,f_g,ari,c_l,emot['pos'],emot['neu'],cnt,gm,swr,tf,tr,max_rep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_matrix(y,y_pred):\n",
    "    # Calculate performance metrics\n",
    "    accuracy = metrics.accuracy_score(y, y_pred)\n",
    "    precision = metrics.precision_score(y, y_pred, average='weighted')\n",
    "    recall = metrics.recall_score(y, y_pred, average='weighted')\n",
    "    f1_score = metrics.f1_score(y, y_pred, average='weighted')\n",
    "    # Calculate the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y, y_pred)\n",
    "    \n",
    "\n",
    "# Print the performance metrics\n",
    "    print(\"\\nAccuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "    # Print the confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\\n\",cm)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def cross_val(results,y_pred,y):\n",
    "    print(\"cross validation results of all sets\\n\")\n",
    "    print(results)\n",
    "    print(\"model's result (mean) \",np.mean(results))\n",
    "    print(\"model's result (standard deviation) \",np.std(results))\n",
    "    #for pred, actual in zip(y_pred, y):\n",
    "    #    print(\"Predicted:\", pred, \"| Actual:\", actual)\n",
    "#decision tree classifier\n",
    "\n",
    "def model_fit(x,y,kfold_validation,clf,fl):\n",
    "\n",
    "    X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    pipeline = make_pipeline(scaler, clf)\n",
    "    results=cross_val_score(pipeline,X_train,y_train,cv=kfold_validation)\n",
    "    #print(\"coss val results \",results)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    #y_pred = cross_val_predict(pipeline,X_test,y_test,cv=kfold_validation)  \n",
    "    \n",
    "    y_pred=pipeline.predict(X_test)\n",
    "    cross_val(results,y_pred,y_test)\n",
    "    a=performance_matrix(y_test,y_pred)\n",
    "\n",
    "    if fl!=[]:\n",
    "    #for unknown data\n",
    "        x_unknown = [fl]\n",
    "    # Apply the same scaling transformation on the unknown data\n",
    "        x_unknown_scaled = scaler.transform(x_unknown)\n",
    "    # Make predictions on the scaled unknown data\n",
    "        label = pipeline.predict(x_unknown_scaled)\n",
    "\n",
    "    # Print the predicted class label\n",
    "        print(\"Predicted class:\", label, \"...\",label[0])\n",
    "        return a,label[0]\n",
    "    \n",
    "    return a,3\n",
    "\n",
    "def dt(x,y,kfold_validation,fl):\n",
    "    print(\"\\n\\n------------------results for decision tree classifier------------------\")\n",
    "    clf=DecisionTreeClassifier()\n",
    "    a,f=model_fit(x,y,kfold_validation,clf,fl)\n",
    "    return a,f\n",
    "      \n",
    "def gnb(x,y,kfold_validation,fl):\n",
    "    print(\"\\n\\n------------------results for Gaussian NB classifier--------------------\")     \n",
    "    clf=GaussianNB()\n",
    "    a,f=model_fit(x,y,kfold_validation,clf,fl)\n",
    "    return a,f\n",
    "\n",
    "def svm(x,y,kfold_validation,fl):\n",
    "    print(\"\\n\\n------------------results for svm classifier----------------------------\")\n",
    "    clf=SVC()\n",
    "    a,f=model_fit(x,y,kfold_validation,clf,fl)\n",
    "    return a,f\n",
    "\n",
    "def rfc(x,y,kfold_validation,fl):\n",
    "    print(\"\\n\\n------------------results for random forest classifier------------------\") \n",
    "    clf=RandomForestClassifier()\n",
    "    a,f=model_fit(x,y,kfold_validation,clf,fl)   \n",
    "    return a,f\n",
    "\n",
    "def knn(x,y,kfold_validation,fl):\n",
    "    print(\"\\n\\n------------------results for knn classifier----------------------------\")\n",
    "    clf=KNeighborsClassifier()\n",
    "    a,f=model_fit(x,y,kfold_validation,clf,fl) \n",
    "    return a,f   \n",
    "\n",
    "def gbs(x,y,kfold_validation,fl):\n",
    "    print(\"\\n\\n------------------results for Gradient Boosting classifier--------------\")\n",
    "    clf=GradientBoostingClassifier()\n",
    "    a,f=model_fit(x,y,kfold_validation,clf,fl)    \n",
    "    return a,f\n",
    "\n",
    "def lr(x,y,kfold_validation,fl):\n",
    "    print(\"\\n\\n------------------results for logistic regression classifier------------\")\n",
    "    clf=LogisticRegression()\n",
    "    a,f=model_fit(x,y,kfold_validation,clf,fl)\n",
    "    return a,f\n",
    "    \n",
    "    \n",
    "def all_classifiers(x,y):\n",
    "    fl=[]\n",
    "    acc=[]\n",
    "    pred=[]\n",
    "    f=3\n",
    "    ch=input(\"do you want to test on unknown data?(y/n)\")\n",
    "    if ch=='y':\n",
    "        fl=pred_unknown()\n",
    "    kfold_validation=KFold(10)\n",
    "    \n",
    "    a1,f=dt(x,y,kfold_validation,fl)\n",
    "    acc.append(a1)\n",
    "    pred.append(f)\n",
    "    if f!=3: print('dt pred ',f,' \\n')\n",
    "    \n",
    "    \n",
    "    a1,f=gnb(x,y,kfold_validation,fl)\n",
    "    if f!=3: print('gnb pred ',f,' \\n')\n",
    "    acc.append(a1)\n",
    "    pred.append(f)\n",
    "\n",
    "    a1,f=svm(x,y,kfold_validation,fl)\n",
    "    if f!=3: print('svm pred ',f,' \\n')\n",
    "    acc.append(a1)\n",
    "    pred.append(f)\n",
    "\n",
    "    a1,f=rfc(x,y,kfold_validation,fl)\n",
    "    if f!=3: print('rfc pred ',f,' \\n')\n",
    "    acc.append(a1)\n",
    "    pred.append(f)\n",
    "\n",
    "\n",
    "    a1,f=knn(x,y,kfold_validation,fl)\n",
    "    if f!=3: print('knn pred ',f,' \\n')\n",
    "    acc.append(a1)\n",
    "    pred.append(f)\n",
    "\n",
    "    #mlpc(x,y,kfold_validation)\n",
    "    a1,f=gbs(x,y,kfold_validation,fl)\n",
    "    if f!=3: print('gbs pred ',f,' \\n')\n",
    "    acc.append(a1)\n",
    "    pred.append(f)\n",
    "\n",
    "    a1,f=lr(x,y,kfold_validation,fl)\n",
    "    if f!=3: print('lr pred ',f,' \\n')\n",
    "    acc.append(a1)\n",
    "    pred.append(f)\n",
    "\n",
    "    if acc == [] or pred == []:\n",
    "        return 0,0,f\n",
    "    else:\n",
    "        return acc,pred,f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../csv/combined_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame()\n",
    "#col=['para_count', 'word_count', 'sentence_count', 'sentence_len(mean)', 'sentence_len(max)', 'sw_rates', 'unique_words', 'unique_words_rates', 'noun', 'verb', 'adj', 'pron', 'occurrance_of_most_freq_word', 'rate_of_occurrance', 'number_of_repeatation', 'max_occurance_of_most_frequent_phrase', 'avg sentence complexity', 'lexical diversity', 'word_sim_p_count', 'word_sim_p_score', 'word_sim_n_count', 'word_sim_n_score', 'word_sim_score', 'flesch grade', 'Automated Readability', 'coleman_liau', 'positive_sentiment', 'neutral_sentiment', 'discourse_marker', 'grammar']\n",
    "col=['para_count', 'word_count', 'sentence_count', 'sentence_len(mean)', 'sentence_len(max)', 'avg_stop_word', 'unique_words', 'noun', 'verb', 'adj', 'pron', 'occurrance_of_most_freq_word', 'rate_of_occurrance', 'avg sentence complexity', 'lexical diversity', 'word_sim_p_count', 'word_sim_p_score', 'word_sim_n_count', 'word_sim_n_score', 'word_sim_score', 'flesch grade', 'Automated Readability', 'coleman_liau', 'positive_sentiment', 'neutral_sentiment', 'discourse_marker', 'grammar', 'sw_rate', 'sum_repeated_ngrams', 'count_repeated_ngrams', 'max_occurrence_ngrams']\n",
    "x[col]=df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#x=df3.drop(['label','essay','topic'],axis='columns')\n",
    "y=df[\"label\"]\n",
    "#auto_class(x,y)\n",
    "xData = x.to_numpy()\n",
    "yData = y.to_numpy()\n",
    "a,pr,f=all_classifiers(xData,yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier_data.pkl', 'wb') as file:\n",
    "    pickle.dump((a, pr), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier_data.pkl', 'rb') as file:\n",
    "    l_acc, l_pred = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if f == 0 or f==1:\n",
    "    print(a,\"\\n\",pr)\n",
    "    classifier_data = list(zip(a, pr))\n",
    "\n",
    "# Sort classifiers based on accuracy\n",
    "    sorted_classifiers = sorted(classifier_data, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Consider only the top three classifiers\n",
    "    top_three_classifiers = sorted_classifiers[:3]\n",
    "    print('top 3 classifier ',top_three_classifiers)\n",
    "\n",
    "    # Count the votes from the top three classifiers\n",
    "    vote_count = {0: 0, 1: 0}  # Dictionary to count votes\n",
    "    for _, p in top_three_classifiers:\n",
    "        vote_count[p] += 1\n",
    "\n",
    "    # Choosing the majority vote\n",
    "    majority_vote = max(vote_count, key=vote_count.get)\n",
    "\n",
    "    print(\"Majority vote from top three classifiers:\", majority_vote)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
