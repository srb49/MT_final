{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import statistics\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import language_tool_python  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gingerit.gingerit import GingerIt\n",
    "import pprint\n",
    "from spacy import displacy\n",
    "from sklearn.svm import SVC\n",
    "import textstat\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import math\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(df, column_index, feature):\n",
    "    # Get the column names\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Insert the new feature at the specified index\n",
    "    columns.insert(column_index, feature)\n",
    "\n",
    "    # Reindex the dataframe with the updated column order\n",
    "    df = df.reindex(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def walk_tree1(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        dep=[]\n",
    "        d=0\n",
    "        for child in node.children:\n",
    "            d=walk_tree1(child, depth + 1)\n",
    "            dep.append(d)\n",
    "        return max(dep)\n",
    "    \n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "def avg_sentence_complexity(str1):\n",
    "    final_list=[]\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    temp_list=[]\n",
    "    sents=[]\n",
    "    txt = str1\n",
    "    txt=\" \".join(txt.split())\n",
    "    text = nlp(txt)\n",
    "    sent_tokenize1 = (list(text.sents))\n",
    "    sents.append(sent_tokenize1[0])\n",
    "    for sent in text.sents:\n",
    "        d=walk_tree1(sent.root, 0)\n",
    "        temp_list.append(d)\n",
    "            #displacy.render(sent, style=\"dep\")\n",
    "        final_list.append(temp_list)\n",
    "        #print(\"essay first line \",sents[len(sents)-1],\"max is \",max(temp_list),\" min is \",min(temp_list),\" mean is \", round(statistics.mean(temp_list),3),\n",
    "        #  \" median is \",statistics.median(temp_list),\" mode is \",statistics.mode(temp_list))\n",
    "    return round(statistics.mean(temp_list),3)\n",
    "\n",
    "def grammar_check(str1):\n",
    "\n",
    "    my_tool = language_tool_python.LanguageTool('en-GB')###########\n",
    "    matches = my_tool.check(str1)\n",
    "    list_s1=[]\n",
    "    if(matches):\n",
    "        fs=str1.split(\".\")[0]\n",
    "        #print(i,\" index | essay first sentences \",fs)\n",
    "        for match in matches:\n",
    "            print(match)\n",
    "    #print(match.ruleId)\n",
    "    #print('Message:',match.message,\"\\n\")\n",
    "            #print(\"####################### \",match.context)\n",
    "            if match.replacements:\n",
    "                chopped_string = match.replacements[0].replace(',','').strip()\n",
    "                list_s1.append(chopped_string)\n",
    "            else:\n",
    "                list_s1.append(match.ruleId)    \n",
    "        #print(\"final list\",list_s1)\n",
    "    return len(list_s1)\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list1=[\" \".join(ngram) for ngram in ngrams]\n",
    "    #return [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list1\n",
    "\n",
    "def rep_count(str1):\n",
    "    tot_rp=[]\n",
    "    max_rp=[]\n",
    "    #print(ess[i],\"\\n\",\"new\",\"\\n\")\n",
    "    list1=generate_ngrams(str1, 5)\n",
    "    myset = set(list1)\n",
    "    rept = len(list1) - len(myset)\n",
    "    tot_rp.append(rept)\n",
    "    c = collections.Counter(list1)\n",
    "    c.most_common(1)[0][1]\n",
    "    max_rp.append(c.most_common(1)[0][1])\n",
    "    #print(\"\\n\\nnew essay index \",i,\": for essay \",list1[0],\":\")\n",
    "    print(\"repeat \",rept,\" times | most common phrase is **\",c.most_common(1)[0][0], \"** |number of repeataion happened: \",c.most_common(1)[0][1],\" times\\n\")\n",
    "    #print(\"\\ncommon phrases: \\n\",c)\n",
    "    #print(\"\\nmy set: \\n\",myset)\n",
    "    #print(\"list \",tot_rp,\" \",max_rp)\n",
    "    #print(\"individual \",rept,\" \",c.most_common(1)[0][1])\n",
    "    return rept,c.most_common(1)[0][1],c.most_common(10)\n",
    "\n",
    "def most_frequent(str1):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    str1=\" \".join(str1.split())\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop!= True and token.is_punct!= True]\n",
    "    w_c=Counter(words)\n",
    "    m_f=w_c.most_common(1)[0][1]\n",
    "    \n",
    "    tags=[]\n",
    "    for w in text:\n",
    "        tags.append(w.pos_)\n",
    "    tag_freq = Counter(tags)\n",
    "    #print (\"=====tag FREQUENCY=====\")\n",
    "    #print(tag_freq.values())\n",
    "\n",
    "    w_l=word_cnt(str1)\n",
    "    for key in tag_freq:\n",
    "        #print(\"hi\")\n",
    "        if key=='NOUN':\n",
    "            n=tag_freq[key]/w_l\n",
    "            #print(\"n \",n)\n",
    "        elif key == 'VERB':\n",
    "            v=tag_freq[key]/w_l\n",
    "            #print(\"v \",v)\n",
    "        elif key =='ADJ':\n",
    "            a=tag_freq[key]/w_l\n",
    "            #print(\"a \",a)\n",
    "        elif key =='PRON':\n",
    "            p=tag_freq[key]/w_l\n",
    "\n",
    "        elif key=='MD':\n",
    "            print(\"the number of modal verbs \",key, tag_freq.values())\n",
    "            #print(\"p \",p)\n",
    "        #print(\"key is \",key,\" value \",tag_freq[key], \" frequency \", tag_freq[key]/w_l)\n",
    "    return m_f,n,v,a,p\n",
    "\n",
    "def uniq_w(str1):\n",
    "    return len(set(str1.split()))\n",
    "\n",
    "def stop_w(str1):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    return(len(words))\n",
    "\n",
    "def stop_w_r(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    \n",
    "    l=len(words)\n",
    "    swr=l/n\n",
    "    return swr\n",
    "\n",
    "def sen_len(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    i=0\n",
    "    avg=0\n",
    "    sl=[]\n",
    "    for num in number_of_sentences:\n",
    "        i+=1\n",
    "        words=num.split()\n",
    "        avg+=len(words)\n",
    "        sl.append(len(words))\n",
    "    #print(num, len(num),len(words))\n",
    "    avg=avg/i\n",
    "    md=statistics.mode(sl)\n",
    "    mx=max(sl)\n",
    "    mn=min(sl)\n",
    "    return avg,md,mx,mn\n",
    "\n",
    "def sen_num(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "    #print(n)\n",
    "    return n\n",
    "\n",
    "def word_cnt(str1):\n",
    "    words=str1.split()\n",
    "    return len(words)\n",
    "\n",
    "def lexical_diversity(str1):\n",
    "    tokens = word_tokenize(str1)\n",
    "\n",
    "    lex_div = len(set(tokens)) / len(tokens)\n",
    "    print(\"Lexical Diversity Score of essay:\", lex_div)\n",
    "    return lex_div\n",
    "\n",
    "def discourse_conn(str1):\n",
    "    tokens = nltk.word_tokenize(str1)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Identify discourse connectors based on part-of-speech patterns\n",
    "    discourse_connectors = [token for token, pos in pos_tags if pos in ['CC', 'RB', 'IN']]\n",
    "\n",
    "    # Output the identified discourse connectors\n",
    "    print(\"Discourse connectors:\", discourse_connectors)\n",
    "    print(\"Discourse connector count:\", len(discourse_connectors))\n",
    "    return len(discourse_connectors)\n",
    "\n",
    "def sentence_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    sent_tokenize = (list(text.sents))\n",
    "    i=0\n",
    "    sensim=0.0\n",
    "    sensim_list=[]\n",
    "    for sent2 in sent_tokenize:\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            sent1=sent2\n",
    "            continue\n",
    "        #print (sent2.similarity(sent1))\n",
    "        sensim+=sent2.similarity(sent1)\n",
    "        sensim_score=sent2.similarity(sent1)\n",
    "        sent1=sent2\n",
    "        sensim_list.append(sensim_score)\n",
    "    sensim=sensim/i\n",
    "    #print(\"average sentence similarity is \",sensim)\n",
    "\n",
    "    return sensim,sensim_list\n",
    "\n",
    "def word_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    p=0\n",
    "    ps=0\n",
    "    n=0\n",
    "    ns=0\n",
    "    total=0\n",
    "    word = [token.text for token in text if token.is_punct != True]\n",
    "    for i in range(len(word)):\n",
    "    #x.append(i)\n",
    "    #print(i)\n",
    "    #print(words[i])\n",
    "        w2=word[i]\n",
    "        if i == 0:\n",
    "            w1=w2\n",
    "            continue\n",
    "    #print (nlp(w2).similarity(nlp(w1)))\n",
    "        #print(w1+ \" \"+w2+\" \",nlp(w2).similarity(nlp(w1)))\n",
    "        if nlp(w2).similarity(nlp(w1))>0:\n",
    "            p=p+1\n",
    "            ps+=nlp(w2).similarity(nlp(w1))\n",
    "        elif nlp(w2).similarity(nlp(w1))<0:\n",
    "            n=n+1\n",
    "            ns+=nlp(w2).similarity(nlp(w1))\n",
    "    #y1=y1+float(nlp(w2).similarity(nlp(w1)))\n",
    "    #print(y1)\n",
    "    #y.append(y1)\n",
    "        total+=nlp(w2).similarity(nlp(w1))\n",
    "        w1=w2\n",
    "    #print(p,n)\n",
    "    return p,n,total,ps,ns\n",
    "\n",
    "def get_modal_verbs(text):\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    cnt=0\n",
    "    modal_verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.tag_ == 'MD':\n",
    "            modal_verbs.append(token.lemma_)\n",
    "            cnt+=1\n",
    "\n",
    "    return cnt,modal_verbs\n",
    "\n",
    "def calculate_readability(text):\n",
    "    # Calculate the Flesch-Kincaid Grade Level\n",
    "    flesch_grade = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    # Calculate the Automated Readability Index (ARI)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    # Calculate the Coleman-Liau Index\n",
    "    coleman_liau = textstat.coleman_liau_index(text)\n",
    "\n",
    "    # Return the readability scores\n",
    "    return flesch_grade, ari, coleman_liau\n",
    "\n",
    "def calculate_cohesion(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Convert generator to a list of bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # Calculate bigram frequencies\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Calculate pointwise mutual information (PMI) for bigrams\n",
    "    total_bigrams = len(bigrams)\n",
    "    pmi_scores = {}\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        pmi = math.log(freq / total_bigrams / ((tokens.count(bigram[0]) / len(tokens)) * (tokens.count(bigram[1]) / len(tokens))))\n",
    "        pmi_scores[bigram] = pmi\n",
    "\n",
    "    # Calculate average PMI as a measure of cohesion\n",
    "    avg_pmi = sum(pmi_scores.values()) / len(pmi_scores)\n",
    "\n",
    "    return avg_pmi\n",
    "\n",
    "def calculate_uniqueness_score(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Calculate the frequency distribution of words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Calculate the uniqueness score as the ratio of unique words to total words\n",
    "    uniqueness_score = len(word_freq) / len(words)\n",
    "\n",
    "    return uniqueness_score\n",
    "\n",
    "def detect_emotional_tone(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "def para_cnt(str1):\n",
    "    paragraphs = str1.split(\"\\n\\n\")\n",
    "    return len(paragraphs)\n",
    "\n",
    "def discourse_marker():\n",
    "    with open('../other/da.txt',\"r\") as file:\n",
    "        text=file.read()\n",
    "    d_list = text.split(\",\")\n",
    "    #print(d_list)\n",
    "    d_list.remove('')\n",
    "    #index = d_list.index('')\n",
    "    #print(index)\n",
    "    #d_list.remove('')\n",
    "    return d_list\n",
    "\n",
    "def discourse_marker_check(essay):\n",
    "    d_list=discourse_marker()\n",
    "    count=0\n",
    "    for marker in d_list:\n",
    "        occurrences = re.findall(r'\\b' + re.escape(marker) + r'\\b', essay, flags=re.IGNORECASE)\n",
    "        count += len(occurrences)\n",
    "        #print(\"occurances \",occurrences,\" for marker \",marker)\n",
    "    #print(\"count \",count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['essay','topic'])\n",
    "df.to_csv('../csv/human/csv_human-1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "df = pd.DataFrame()\n",
    "for path in glob.glob('../human/input/*.txt'):   \n",
    "    with open(path) as file:\n",
    "        first_line = file.readline().strip()\n",
    "        remaining_content = file.read()\n",
    "        if remaining_content.startswith('\\n'):\n",
    "            remaining_content = remaining_content[1:]\n",
    "    df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
    "    '''ch=input(\"enter e\")\n",
    "    if ch =='e' or ch =='E':\n",
    "        break\n",
    "    else:\n",
    "        continue'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../csv/human/csv_human-1.csv',mode='a',header=None,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or read from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "data\n",
    "ess=data.loc[:,'essay']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paragraph count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    pc=para_cnt(ess[i])\n",
    "    data.loc[i,'para_count']=pc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    wc=word_cnt(ess[i])\n",
    "    data.loc[i,'word_count']=wc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    ns=sen_num(ess[i])\n",
    "    data.loc[i,'sentence_count']=ns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    avg,md,mx,mn=sen_len(ess[i])\n",
    "    data.loc[i,'sentence_len(mean)']=avg\n",
    "    data.loc[i,'sentence_len(mode)']=md\n",
    "    data.loc[i,'sentence_len(max)']=mx\n",
    "    data.loc[i,'sentence_len(min)']=mn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    sw=stop_w(ess[i])\n",
    "    swr=stop_w_r(ess[i])\n",
    "    data.loc[i,'stop_words']=sw\n",
    "    data.loc[i,'sw_rates']=swr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../csv/human/csv_human-1.csv',index=False)\n",
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    uw=uniq_w(ess[i])\n",
    "    data.loc[i,'unique_words']=uw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos and most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    uw=uniq_w(ess[i])\n",
    "    mf,n,v,a,p=most_frequent(ess[i])\n",
    "    mvc,mv=get_modal_verbs(ess[i])\n",
    "    data.loc[i,'noun']=n\n",
    "    data.loc[i,'verb']=v\n",
    "    data.loc[i,'adj']=a\n",
    "    data.loc[i,'pron']=p,\n",
    "    data.loc[i,'modal verb']=mvc\n",
    "    data.loc[i,'most_freq']=mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../csv/human/csv_human-1.csv',index=False)\n",
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeatation of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    tr,mr,rep_list=rep_count(ess[i])\n",
    "    data.loc[i,'number_of_repeatation']=tr\n",
    "    data.loc[i,'max_occurance_of_most_frequent_phrase']=mr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    asc=avg_sentence_complexity(ess[i])\n",
    "    data.loc[i,'avg sentence complexity']=asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../csv/human/csv_human-1.csv',index=False)\n",
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    l_d=lexical_diversity(ess[i])\n",
    "    data.loc[i,'lexical diversity']=l_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    sensim,sensim_list=sentence_similarity(ess[i])\n",
    "    sensim_list.sort()\n",
    "    sensim_mod=statistics.mode(sensim_list)\n",
    "    sensim_med=statistics.median(sensim_list)\n",
    "    data.loc[i,'sentence similarity mean']=sensim\n",
    "    data.loc[i,'sentence similarity median']=sensim_med\n",
    "    data.loc[i,'sentence similarity mode']=sensim_mod,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    pos,neg,tot,p_s,n_s=word_similarity(ess[i])\n",
    "    data.loc[i,'word_sim_p_count']=pos\n",
    "    data.loc[i,'word_sim_p_score']=p_s\n",
    "    data.loc[i,'word_sim_n_count']=neg\n",
    "    data.loc[i,'word_sim_n_score']=n_s\n",
    "    data.loc[i,'word_sim_score']=tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../csv/human/csv_human-1.csv',index=False)\n",
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    f_g, ari, c_l = calculate_readability(ess[i])\n",
    "    data.loc[i,'flesch grade']=f_g\n",
    "    data.loc[i,'Automated Readability']=ari\n",
    "    data.loc[i,'coleman_liau']=c_l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cohesion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    coh_score=calculate_cohesion(ess[i])\n",
    "    data.loc[i,'cohesion score']=coh_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    uniq = calculate_uniqueness_score(ess[i])\n",
    "    data.loc[i,'uniqueness of words']=uniq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emotional tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    emot = detect_emotional_tone(ess[i])\n",
    "    data.loc[i,'positive_sentiment']=emot['pos']\n",
    "    data.loc[i,'negative_sentiment']=emot['neg']\n",
    "    data.loc[i,'neutral_sentiment']=emot['neu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grammar and spelling \n",
    "***GB and US***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "def grammar_check1(text):\n",
    "# Create separate LanguageTool instances for GB and US English\n",
    "    tool_gb = language_tool_python.LanguageTool('en-GB')\n",
    "    tool_us = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "\n",
    "# Check for GB English grammar errors\n",
    "    matches_gb = tool_gb.check(text)\n",
    "    print(\"gb \",matches_gb)\n",
    "\n",
    "# Check for US English grammar errors\n",
    "    matches_us = tool_us.check(text)\n",
    "    print(\"us \",matches_us)\n",
    "\n",
    "    common_errors = [\n",
    "    error for error in matches_gb\n",
    "    if any(error.matches(match) for match in matches_us)\n",
    "    ]\n",
    "\n",
    "    print(\"Errors common to GB and US English:\")\n",
    "    for error in common_errors:\n",
    "        print(error)\n",
    "\n",
    "# Filter errors that do not match in either GB or US English\n",
    "#    non_matching_errors = [\n",
    "#        error for error in matches_gb + matches_us\n",
    "#        if not any(error.matches(match) for match in matches_gb + matches_us)\n",
    "#    ]\n",
    "\n",
    "#    print(\"Errors not matching in either GB or US English:\")\n",
    "#    for error in non_matching_errors:\n",
    "#        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']\n",
    "for i in range(len(ess)):\n",
    "    print(\"essay \",i,\" and topic is \",data.loc[i,\"topic\"])\n",
    "    g_s=grammar_check1(ess[i])\n",
    "    #data.loc[i,'grammar&spel']=g_s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discourse annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']\n",
    "########### entering new column ###################\n",
    "new_feature = 'discourse_marker'\n",
    "column_index = data.shape[1]-1\n",
    "data = add_feature(data, column_index, new_feature)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"\\nAfter adding new feature:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    cnt=discourse_marker_check(ess[i])\n",
    "    data.loc[i,'discourse_marker']=cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>topic</th>\n",
       "      <th>para_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>sentence_len(mean)</th>\n",
       "      <th>sentence_len(mode)</th>\n",
       "      <th>sentence_len(max)</th>\n",
       "      <th>sentence_len(min)</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>sw_rates</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>adj</th>\n",
       "      <th>pron</th>\n",
       "      <th>modal verb</th>\n",
       "      <th>most_freq</th>\n",
       "      <th>number_of_repeatation</th>\n",
       "      <th>max_occurance_of_most_frequent_phrase</th>\n",
       "      <th>avg sentence complexity</th>\n",
       "      <th>lexical diversity</th>\n",
       "      <th>sentence similarity mean</th>\n",
       "      <th>sentence similarity median</th>\n",
       "      <th>sentence similarity mode</th>\n",
       "      <th>word_sim_p_count</th>\n",
       "      <th>word_sim_p_score</th>\n",
       "      <th>word_sim_n_count</th>\n",
       "      <th>word_sim_n_score</th>\n",
       "      <th>word_sim_score</th>\n",
       "      <th>flesch grade</th>\n",
       "      <th>Automated Readability</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>cohesion score</th>\n",
       "      <th>uniqueness of words</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>discourse_marker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recently, a great deal of attention has been p...</td>\n",
       "      <td>Alternative forms of transport to alleviate tr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>9.650000</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.306701</td>\n",
       "      <td>0.128866</td>\n",
       "      <td>0.097938</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.400</td>\n",
       "      <td>0.444186</td>\n",
       "      <td>0.835664</td>\n",
       "      <td>0.880775</td>\n",
       "      <td>0.799130</td>\n",
       "      <td>380.0</td>\n",
       "      <td>150.254682</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.333771</td>\n",
       "      <td>149.920911</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>13.69</td>\n",
       "      <td>4.182405</td>\n",
       "      <td>0.444186</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.834</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gun is definitely not a kind of normal good in...</td>\n",
       "      <td>Gun control</td>\n",
       "      <td>5.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>8.937500</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.296429</td>\n",
       "      <td>0.121429</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.188</td>\n",
       "      <td>0.532915</td>\n",
       "      <td>0.849118</td>\n",
       "      <td>0.905018</td>\n",
       "      <td>0.882263</td>\n",
       "      <td>276.0</td>\n",
       "      <td>108.484310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.484310</td>\n",
       "      <td>8.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>10.84</td>\n",
       "      <td>4.028763</td>\n",
       "      <td>0.532915</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.664</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The increasing population and the technologica...</td>\n",
       "      <td>Government should make more effort to promote ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.667</td>\n",
       "      <td>0.480287</td>\n",
       "      <td>0.844501</td>\n",
       "      <td>0.912760</td>\n",
       "      <td>0.817024</td>\n",
       "      <td>254.0</td>\n",
       "      <td>107.812128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.812128</td>\n",
       "      <td>11.1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.83</td>\n",
       "      <td>3.702329</td>\n",
       "      <td>0.480287</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.801</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Technology, in the past two hundred years, has...</td>\n",
       "      <td>technology that widen the gap between rich and...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>27.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>11.222222</td>\n",
       "      <td>223.0</td>\n",
       "      <td>0.257895</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.778</td>\n",
       "      <td>0.486047</td>\n",
       "      <td>0.864111</td>\n",
       "      <td>0.920834</td>\n",
       "      <td>0.860589</td>\n",
       "      <td>375.0</td>\n",
       "      <td>160.342955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>160.342955</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.8</td>\n",
       "      <td>11.08</td>\n",
       "      <td>4.393480</td>\n",
       "      <td>0.486047</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.764</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For most folks on Earth, the need to gather in...</td>\n",
       "      <td>Sporting events easing international tensions</td>\n",
       "      <td>4.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.244624</td>\n",
       "      <td>0.131720</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.867</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.864177</td>\n",
       "      <td>0.935544</td>\n",
       "      <td>0.844644</td>\n",
       "      <td>363.0</td>\n",
       "      <td>146.918488</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.445256</td>\n",
       "      <td>146.473232</td>\n",
       "      <td>11.8</td>\n",
       "      <td>15.9</td>\n",
       "      <td>13.18</td>\n",
       "      <td>4.533942</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.786</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>In this world, there is no one who does not kn...</td>\n",
       "      <td>Giving aid to poorer countries has more negati...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.166667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.265092</td>\n",
       "      <td>0.139108</td>\n",
       "      <td>0.162730</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.778</td>\n",
       "      <td>0.523474</td>\n",
       "      <td>0.850632</td>\n",
       "      <td>0.909514</td>\n",
       "      <td>0.859106</td>\n",
       "      <td>376.0</td>\n",
       "      <td>151.923564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>151.923564</td>\n",
       "      <td>11.6</td>\n",
       "      <td>14.1</td>\n",
       "      <td>12.71</td>\n",
       "      <td>4.280784</td>\n",
       "      <td>0.523474</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.716</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>In present day and age, students are normally ...</td>\n",
       "      <td>Benefits of students doing extra jobs</td>\n",
       "      <td>5.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.222222</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.274566</td>\n",
       "      <td>0.158960</td>\n",
       "      <td>0.098266</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.944</td>\n",
       "      <td>0.509044</td>\n",
       "      <td>0.870809</td>\n",
       "      <td>0.922498</td>\n",
       "      <td>0.876775</td>\n",
       "      <td>341.0</td>\n",
       "      <td>138.879977</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.171047</td>\n",
       "      <td>138.708930</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.10</td>\n",
       "      <td>4.160972</td>\n",
       "      <td>0.509044</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.760</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Preserving wild animals in zoos is an issue wh...</td>\n",
       "      <td>zoos should be built to protect rural animals</td>\n",
       "      <td>4.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.846154</td>\n",
       "      <td>19.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>12.076923</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>0.148387</td>\n",
       "      <td>0.122581</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.357</td>\n",
       "      <td>0.483680</td>\n",
       "      <td>0.853539</td>\n",
       "      <td>0.928610</td>\n",
       "      <td>0.867761</td>\n",
       "      <td>306.0</td>\n",
       "      <td>124.190676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>124.190676</td>\n",
       "      <td>12.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.49</td>\n",
       "      <td>3.940267</td>\n",
       "      <td>0.483680</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.765</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Today, we no longer fear any contagious diseas...</td>\n",
       "      <td>Those who use drugs should be automatically se...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.619048</td>\n",
       "      <td>15.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>12.238095</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.210162</td>\n",
       "      <td>0.157044</td>\n",
       "      <td>0.076212</td>\n",
       "      <td>0.071594</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.571</td>\n",
       "      <td>0.472803</td>\n",
       "      <td>0.871059</td>\n",
       "      <td>0.909387</td>\n",
       "      <td>0.859901</td>\n",
       "      <td>427.0</td>\n",
       "      <td>189.225096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.014288</td>\n",
       "      <td>189.210807</td>\n",
       "      <td>10.1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.34</td>\n",
       "      <td>4.489194</td>\n",
       "      <td>0.472803</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.752</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>My view is that the government should give pri...</td>\n",
       "      <td>Do arts and music improve the quality of life?</td>\n",
       "      <td>4.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.375000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.306383</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.140426</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.750</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.799730</td>\n",
       "      <td>0.915417</td>\n",
       "      <td>0.884183</td>\n",
       "      <td>235.0</td>\n",
       "      <td>94.977588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.112045</td>\n",
       "      <td>94.865543</td>\n",
       "      <td>14.8</td>\n",
       "      <td>16.6</td>\n",
       "      <td>11.38</td>\n",
       "      <td>4.121992</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.788</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                essay  \\\n",
       "0   Recently, a great deal of attention has been p...   \n",
       "1   Gun is definitely not a kind of normal good in...   \n",
       "2   The increasing population and the technologica...   \n",
       "3   Technology, in the past two hundred years, has...   \n",
       "4   For most folks on Earth, the need to gather in...   \n",
       "..                                                ...   \n",
       "85  In this world, there is no one who does not kn...   \n",
       "86  In present day and age, students are normally ...   \n",
       "87  Preserving wild animals in zoos is an issue wh...   \n",
       "88  Today, we no longer fear any contagious diseas...   \n",
       "89  My view is that the government should give pri...   \n",
       "\n",
       "                                                topic  para_count  word_count  \\\n",
       "0   Alternative forms of transport to alleviate tr...         5.0       388.0   \n",
       "1                                         Gun control         5.0       280.0   \n",
       "2   Government should make more effort to promote ...         4.0       256.0   \n",
       "3   technology that widen the gap between rich and...         6.0       380.0   \n",
       "4       Sporting events easing international tensions         4.0       372.0   \n",
       "..                                                ...         ...         ...   \n",
       "85  Giving aid to poorer countries has more negati...         6.0       381.0   \n",
       "86              Benefits of students doing extra jobs         5.0       346.0   \n",
       "87      zoos should be built to protect rural animals         4.0       310.0   \n",
       "88  Those who use drugs should be automatically se...         4.0       433.0   \n",
       "89     Do arts and music improve the quality of life?         4.0       235.0   \n",
       "\n",
       "    sentence_count  sentence_len(mean)  sentence_len(mode)  sentence_len(max)  \\\n",
       "0             20.0           19.400000                13.0               38.0   \n",
       "1             16.0           17.500000                19.0               38.0   \n",
       "2             15.0           17.066667                20.0               24.0   \n",
       "3             18.0           21.111111                27.0               38.0   \n",
       "4             15.0           24.800000                28.0               42.0   \n",
       "..             ...                 ...                 ...                ...   \n",
       "85            18.0           21.166667                20.0               34.0   \n",
       "86            18.0           19.222222                16.0               28.0   \n",
       "87            13.0           23.846154                19.0               47.0   \n",
       "88            21.0           20.619048                15.0               53.0   \n",
       "89             8.0           29.375000                13.0               48.0   \n",
       "\n",
       "    sentence_len(min)  stop_words   sw_rates  unique_words      noun  \\\n",
       "0                12.0       193.0   9.650000         199.0  0.306701   \n",
       "1                10.0       143.0   8.937500         178.0  0.296429   \n",
       "2                 8.0       135.0   9.000000         138.0  0.261719   \n",
       "3                10.0       202.0  11.222222         223.0  0.257895   \n",
       "4                14.0       177.0  11.800000         227.0  0.244624   \n",
       "..                ...         ...        ...           ...       ...   \n",
       "85                7.0       189.0  10.500000         228.0  0.265092   \n",
       "86               11.0       171.0   9.500000         204.0  0.274566   \n",
       "87               12.0       157.0  12.076923         169.0  0.251613   \n",
       "88                6.0       257.0  12.238095         236.0  0.210162   \n",
       "89               13.0       122.0  15.250000         154.0  0.306383   \n",
       "\n",
       "        verb       adj      pron  modal verb  most_freq  \\\n",
       "0   0.128866  0.097938  0.030928         9.0       11.0   \n",
       "1   0.121429  0.082143  0.025000         9.0        9.0   \n",
       "2   0.132812  0.148438  0.023438         9.0        8.0   \n",
       "3   0.105263  0.110526  0.050000         5.0        8.0   \n",
       "4   0.131720  0.112903  0.021505         6.0       10.0   \n",
       "..       ...       ...       ...         ...        ...   \n",
       "85  0.139108  0.162730  0.031496         6.0       15.0   \n",
       "86  0.158960  0.098266  0.034682         8.0       10.0   \n",
       "87  0.148387  0.122581  0.032258        12.0       11.0   \n",
       "88  0.157044  0.076212  0.071594        12.0        7.0   \n",
       "89  0.106383  0.140426  0.025532         4.0        4.0   \n",
       "\n",
       "    number_of_repeatation  max_occurance_of_most_frequent_phrase  \\\n",
       "0                     5.0                                    2.0   \n",
       "1                     0.0                                    1.0   \n",
       "2                     1.0                                    2.0   \n",
       "3                     0.0                                    1.0   \n",
       "4                     0.0                                    1.0   \n",
       "..                    ...                                    ...   \n",
       "85                    0.0                                    1.0   \n",
       "86                    0.0                                    1.0   \n",
       "87                    0.0                                    1.0   \n",
       "88                    0.0                                    1.0   \n",
       "89                    3.0                                    2.0   \n",
       "\n",
       "    avg sentence complexity  lexical diversity  sentence similarity mean  \\\n",
       "0                     5.400           0.444186                  0.835664   \n",
       "1                     5.188           0.532915                  0.849118   \n",
       "2                     5.667           0.480287                  0.844501   \n",
       "3                     5.778           0.486047                  0.864111   \n",
       "4                     5.867           0.544118                  0.864177   \n",
       "..                      ...                ...                       ...   \n",
       "85                    5.778           0.523474                  0.850632   \n",
       "86                    5.944           0.509044                  0.870809   \n",
       "87                    6.357           0.483680                  0.853539   \n",
       "88                    5.571           0.472803                  0.871059   \n",
       "89                    7.750           0.575290                  0.799730   \n",
       "\n",
       "    sentence similarity median  sentence similarity mode  word_sim_p_count  \\\n",
       "0                     0.880775                  0.799130             380.0   \n",
       "1                     0.905018                  0.882263             276.0   \n",
       "2                     0.912760                  0.817024             254.0   \n",
       "3                     0.920834                  0.860589             375.0   \n",
       "4                     0.935544                  0.844644             363.0   \n",
       "..                         ...                       ...               ...   \n",
       "85                    0.909514                  0.859106             376.0   \n",
       "86                    0.922498                  0.876775             341.0   \n",
       "87                    0.928610                  0.867761             306.0   \n",
       "88                    0.909387                  0.859901             427.0   \n",
       "89                    0.915417                  0.884183             235.0   \n",
       "\n",
       "    word_sim_p_score  word_sim_n_count  word_sim_n_score  word_sim_score  \\\n",
       "0         150.254682               3.0         -0.333771      149.920911   \n",
       "1         108.484310               0.0          0.000000      108.484310   \n",
       "2         107.812128               0.0          0.000000      107.812128   \n",
       "3         160.342955               0.0          0.000000      160.342955   \n",
       "4         146.918488               4.0         -0.445256      146.473232   \n",
       "..               ...               ...               ...             ...   \n",
       "85        151.923564               0.0          0.000000      151.923564   \n",
       "86        138.879977               2.0         -0.171047      138.708930   \n",
       "87        124.190676               0.0          0.000000      124.190676   \n",
       "88        189.225096               1.0         -0.014288      189.210807   \n",
       "89         94.977588               1.0         -0.112045       94.865543   \n",
       "\n",
       "    flesch grade  Automated Readability  coleman_liau  cohesion score  \\\n",
       "0           12.0                   13.9         13.69        4.182405   \n",
       "1            8.9                   11.1         10.84        4.028763   \n",
       "2           11.1                   11.5         11.83        3.702329   \n",
       "3           11.5                   12.8         11.08        4.393480   \n",
       "4           11.8                   15.9         13.18        4.533942   \n",
       "..           ...                    ...           ...             ...   \n",
       "85          11.6                   14.1         12.71        4.280784   \n",
       "86          12.0                   14.2         14.10        4.160972   \n",
       "87          12.6                   14.0         11.49        3.940267   \n",
       "88          10.1                   11.0          9.34        4.489194   \n",
       "89          14.8                   16.6         11.38        4.121992   \n",
       "\n",
       "    uniqueness of words  positive_sentiment  negative_sentiment  \\\n",
       "0              0.444186               0.100               0.066   \n",
       "1              0.532915               0.053               0.282   \n",
       "2              0.480287               0.173               0.026   \n",
       "3              0.486047               0.198               0.038   \n",
       "4              0.544118               0.160               0.053   \n",
       "..                  ...                 ...                 ...   \n",
       "85             0.523474               0.160               0.124   \n",
       "86             0.509044               0.194               0.046   \n",
       "87             0.483680               0.144               0.090   \n",
       "88             0.472803               0.127               0.121   \n",
       "89             0.575290               0.140               0.072   \n",
       "\n",
       "    neutral_sentiment  discourse_marker  label  \n",
       "0               0.834              49.0    0.0  \n",
       "1               0.664              32.0    0.0  \n",
       "2               0.801              28.0    0.0  \n",
       "3               0.764              47.0    0.0  \n",
       "4               0.786              55.0    0.0  \n",
       "..                ...               ...    ...  \n",
       "85              0.716              49.0    0.0  \n",
       "86              0.760              54.0    0.0  \n",
       "87              0.765              50.0    0.0  \n",
       "88              0.752              62.0    0.0  \n",
       "89              0.788              29.0    0.0  \n",
       "\n",
       "[90 rows x 40 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../csv/human/csv_human-1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>topic</th>\n",
       "      <th>para_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>sentence_len(mean)</th>\n",
       "      <th>sentence_len(mode)</th>\n",
       "      <th>sentence_len(max)</th>\n",
       "      <th>sentence_len(min)</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>sw_rates</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>adj</th>\n",
       "      <th>pron</th>\n",
       "      <th>modal verb</th>\n",
       "      <th>most_freq</th>\n",
       "      <th>number_of_repeatation</th>\n",
       "      <th>max_occurance_of_most_frequent_phrase</th>\n",
       "      <th>avg sentence complexity</th>\n",
       "      <th>lexical diversity</th>\n",
       "      <th>sentence similarity mean</th>\n",
       "      <th>sentence similarity median</th>\n",
       "      <th>sentence similarity mode</th>\n",
       "      <th>word_sim_p_count</th>\n",
       "      <th>word_sim_p_score</th>\n",
       "      <th>word_sim_n_count</th>\n",
       "      <th>word_sim_n_score</th>\n",
       "      <th>word_sim_score</th>\n",
       "      <th>flesch grade</th>\n",
       "      <th>Automated Readability</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>cohesion score</th>\n",
       "      <th>uniqueness of words</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>discourse_marker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recently, a great deal of attention has been p...</td>\n",
       "      <td>Alternative forms of transport to alleviate tr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>9.650000</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.306701</td>\n",
       "      <td>0.128866</td>\n",
       "      <td>0.097938</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.400</td>\n",
       "      <td>0.444186</td>\n",
       "      <td>0.835664</td>\n",
       "      <td>0.880775</td>\n",
       "      <td>0.799130</td>\n",
       "      <td>380.0</td>\n",
       "      <td>150.254682</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.333771</td>\n",
       "      <td>149.920911</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>13.69</td>\n",
       "      <td>4.182405</td>\n",
       "      <td>0.444186</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.834</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gun is definitely not a kind of normal good in...</td>\n",
       "      <td>Gun control</td>\n",
       "      <td>5.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>8.937500</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.296429</td>\n",
       "      <td>0.121429</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.188</td>\n",
       "      <td>0.532915</td>\n",
       "      <td>0.849118</td>\n",
       "      <td>0.905018</td>\n",
       "      <td>0.882263</td>\n",
       "      <td>276.0</td>\n",
       "      <td>108.484310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.484310</td>\n",
       "      <td>8.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>10.84</td>\n",
       "      <td>4.028763</td>\n",
       "      <td>0.532915</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.664</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The increasing population and the technologica...</td>\n",
       "      <td>Government should make more effort to promote ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.667</td>\n",
       "      <td>0.480287</td>\n",
       "      <td>0.844501</td>\n",
       "      <td>0.912760</td>\n",
       "      <td>0.817024</td>\n",
       "      <td>254.0</td>\n",
       "      <td>107.812128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.812128</td>\n",
       "      <td>11.1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.83</td>\n",
       "      <td>3.702329</td>\n",
       "      <td>0.480287</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.801</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Technology, in the past two hundred years, has...</td>\n",
       "      <td>technology that widen the gap between rich and...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>27.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>11.222222</td>\n",
       "      <td>223.0</td>\n",
       "      <td>0.257895</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.110526</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.778</td>\n",
       "      <td>0.486047</td>\n",
       "      <td>0.864111</td>\n",
       "      <td>0.920834</td>\n",
       "      <td>0.860589</td>\n",
       "      <td>375.0</td>\n",
       "      <td>160.342955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>160.342955</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.8</td>\n",
       "      <td>11.08</td>\n",
       "      <td>4.393480</td>\n",
       "      <td>0.486047</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.764</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For most folks on Earth, the need to gather in...</td>\n",
       "      <td>Sporting events easing international tensions</td>\n",
       "      <td>4.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.244624</td>\n",
       "      <td>0.131720</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.867</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.864177</td>\n",
       "      <td>0.935544</td>\n",
       "      <td>0.844644</td>\n",
       "      <td>363.0</td>\n",
       "      <td>146.918488</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.445256</td>\n",
       "      <td>146.473232</td>\n",
       "      <td>11.8</td>\n",
       "      <td>15.9</td>\n",
       "      <td>13.18</td>\n",
       "      <td>4.533942</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.786</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>In this world, there is no one who does not kn...</td>\n",
       "      <td>Giving aid to poorer countries has more negati...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.166667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.265092</td>\n",
       "      <td>0.139108</td>\n",
       "      <td>0.162730</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.778</td>\n",
       "      <td>0.523474</td>\n",
       "      <td>0.850632</td>\n",
       "      <td>0.909514</td>\n",
       "      <td>0.859106</td>\n",
       "      <td>376.0</td>\n",
       "      <td>151.923564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>151.923564</td>\n",
       "      <td>11.6</td>\n",
       "      <td>14.1</td>\n",
       "      <td>12.71</td>\n",
       "      <td>4.280784</td>\n",
       "      <td>0.523474</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.716</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>In present day and age, students are normally ...</td>\n",
       "      <td>Benefits of students doing extra jobs</td>\n",
       "      <td>5.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.222222</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.274566</td>\n",
       "      <td>0.158960</td>\n",
       "      <td>0.098266</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.944</td>\n",
       "      <td>0.509044</td>\n",
       "      <td>0.870809</td>\n",
       "      <td>0.922498</td>\n",
       "      <td>0.876775</td>\n",
       "      <td>341.0</td>\n",
       "      <td>138.879977</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.171047</td>\n",
       "      <td>138.708930</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.10</td>\n",
       "      <td>4.160972</td>\n",
       "      <td>0.509044</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.760</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Preserving wild animals in zoos is an issue wh...</td>\n",
       "      <td>zoos should be built to protect rural animals</td>\n",
       "      <td>4.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.846154</td>\n",
       "      <td>19.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>12.076923</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>0.148387</td>\n",
       "      <td>0.122581</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.357</td>\n",
       "      <td>0.483680</td>\n",
       "      <td>0.853539</td>\n",
       "      <td>0.928610</td>\n",
       "      <td>0.867761</td>\n",
       "      <td>306.0</td>\n",
       "      <td>124.190676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>124.190676</td>\n",
       "      <td>12.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.49</td>\n",
       "      <td>3.940267</td>\n",
       "      <td>0.483680</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.765</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Today, we no longer fear any contagious diseas...</td>\n",
       "      <td>Those who use drugs should be automatically se...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.619048</td>\n",
       "      <td>15.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>12.238095</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.210162</td>\n",
       "      <td>0.157044</td>\n",
       "      <td>0.076212</td>\n",
       "      <td>0.071594</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.571</td>\n",
       "      <td>0.472803</td>\n",
       "      <td>0.871059</td>\n",
       "      <td>0.909387</td>\n",
       "      <td>0.859901</td>\n",
       "      <td>427.0</td>\n",
       "      <td>189.225096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.014288</td>\n",
       "      <td>189.210807</td>\n",
       "      <td>10.1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.34</td>\n",
       "      <td>4.489194</td>\n",
       "      <td>0.472803</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.752</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>My view is that the government should give pri...</td>\n",
       "      <td>Do arts and music improve the quality of life?</td>\n",
       "      <td>4.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.375000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.306383</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.140426</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.750</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.799730</td>\n",
       "      <td>0.915417</td>\n",
       "      <td>0.884183</td>\n",
       "      <td>235.0</td>\n",
       "      <td>94.977588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.112045</td>\n",
       "      <td>94.865543</td>\n",
       "      <td>14.8</td>\n",
       "      <td>16.6</td>\n",
       "      <td>11.38</td>\n",
       "      <td>4.121992</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.788</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                essay  \\\n",
       "0   Recently, a great deal of attention has been p...   \n",
       "1   Gun is definitely not a kind of normal good in...   \n",
       "2   The increasing population and the technologica...   \n",
       "3   Technology, in the past two hundred years, has...   \n",
       "4   For most folks on Earth, the need to gather in...   \n",
       "..                                                ...   \n",
       "85  In this world, there is no one who does not kn...   \n",
       "86  In present day and age, students are normally ...   \n",
       "87  Preserving wild animals in zoos is an issue wh...   \n",
       "88  Today, we no longer fear any contagious diseas...   \n",
       "89  My view is that the government should give pri...   \n",
       "\n",
       "                                                topic  para_count  word_count  \\\n",
       "0   Alternative forms of transport to alleviate tr...         5.0       388.0   \n",
       "1                                         Gun control         5.0       280.0   \n",
       "2   Government should make more effort to promote ...         4.0       256.0   \n",
       "3   technology that widen the gap between rich and...         6.0       380.0   \n",
       "4       Sporting events easing international tensions         4.0       372.0   \n",
       "..                                                ...         ...         ...   \n",
       "85  Giving aid to poorer countries has more negati...         6.0       381.0   \n",
       "86              Benefits of students doing extra jobs         5.0       346.0   \n",
       "87      zoos should be built to protect rural animals         4.0       310.0   \n",
       "88  Those who use drugs should be automatically se...         4.0       433.0   \n",
       "89     Do arts and music improve the quality of life?         4.0       235.0   \n",
       "\n",
       "    sentence_count  sentence_len(mean)  sentence_len(mode)  sentence_len(max)  \\\n",
       "0             20.0           19.400000                13.0               38.0   \n",
       "1             16.0           17.500000                19.0               38.0   \n",
       "2             15.0           17.066667                20.0               24.0   \n",
       "3             18.0           21.111111                27.0               38.0   \n",
       "4             15.0           24.800000                28.0               42.0   \n",
       "..             ...                 ...                 ...                ...   \n",
       "85            18.0           21.166667                20.0               34.0   \n",
       "86            18.0           19.222222                16.0               28.0   \n",
       "87            13.0           23.846154                19.0               47.0   \n",
       "88            21.0           20.619048                15.0               53.0   \n",
       "89             8.0           29.375000                13.0               48.0   \n",
       "\n",
       "    sentence_len(min)  stop_words   sw_rates  unique_words      noun  \\\n",
       "0                12.0       193.0   9.650000         199.0  0.306701   \n",
       "1                10.0       143.0   8.937500         178.0  0.296429   \n",
       "2                 8.0       135.0   9.000000         138.0  0.261719   \n",
       "3                10.0       202.0  11.222222         223.0  0.257895   \n",
       "4                14.0       177.0  11.800000         227.0  0.244624   \n",
       "..                ...         ...        ...           ...       ...   \n",
       "85                7.0       189.0  10.500000         228.0  0.265092   \n",
       "86               11.0       171.0   9.500000         204.0  0.274566   \n",
       "87               12.0       157.0  12.076923         169.0  0.251613   \n",
       "88                6.0       257.0  12.238095         236.0  0.210162   \n",
       "89               13.0       122.0  15.250000         154.0  0.306383   \n",
       "\n",
       "        verb       adj      pron  modal verb  most_freq  \\\n",
       "0   0.128866  0.097938  0.030928         9.0       11.0   \n",
       "1   0.121429  0.082143  0.025000         9.0        9.0   \n",
       "2   0.132812  0.148438  0.023438         9.0        8.0   \n",
       "3   0.105263  0.110526  0.050000         5.0        8.0   \n",
       "4   0.131720  0.112903  0.021505         6.0       10.0   \n",
       "..       ...       ...       ...         ...        ...   \n",
       "85  0.139108  0.162730  0.031496         6.0       15.0   \n",
       "86  0.158960  0.098266  0.034682         8.0       10.0   \n",
       "87  0.148387  0.122581  0.032258        12.0       11.0   \n",
       "88  0.157044  0.076212  0.071594        12.0        7.0   \n",
       "89  0.106383  0.140426  0.025532         4.0        4.0   \n",
       "\n",
       "    number_of_repeatation  max_occurance_of_most_frequent_phrase  \\\n",
       "0                     5.0                                    2.0   \n",
       "1                     0.0                                    1.0   \n",
       "2                     1.0                                    2.0   \n",
       "3                     0.0                                    1.0   \n",
       "4                     0.0                                    1.0   \n",
       "..                    ...                                    ...   \n",
       "85                    0.0                                    1.0   \n",
       "86                    0.0                                    1.0   \n",
       "87                    0.0                                    1.0   \n",
       "88                    0.0                                    1.0   \n",
       "89                    3.0                                    2.0   \n",
       "\n",
       "    avg sentence complexity  lexical diversity  sentence similarity mean  \\\n",
       "0                     5.400           0.444186                  0.835664   \n",
       "1                     5.188           0.532915                  0.849118   \n",
       "2                     5.667           0.480287                  0.844501   \n",
       "3                     5.778           0.486047                  0.864111   \n",
       "4                     5.867           0.544118                  0.864177   \n",
       "..                      ...                ...                       ...   \n",
       "85                    5.778           0.523474                  0.850632   \n",
       "86                    5.944           0.509044                  0.870809   \n",
       "87                    6.357           0.483680                  0.853539   \n",
       "88                    5.571           0.472803                  0.871059   \n",
       "89                    7.750           0.575290                  0.799730   \n",
       "\n",
       "    sentence similarity median  sentence similarity mode  word_sim_p_count  \\\n",
       "0                     0.880775                  0.799130             380.0   \n",
       "1                     0.905018                  0.882263             276.0   \n",
       "2                     0.912760                  0.817024             254.0   \n",
       "3                     0.920834                  0.860589             375.0   \n",
       "4                     0.935544                  0.844644             363.0   \n",
       "..                         ...                       ...               ...   \n",
       "85                    0.909514                  0.859106             376.0   \n",
       "86                    0.922498                  0.876775             341.0   \n",
       "87                    0.928610                  0.867761             306.0   \n",
       "88                    0.909387                  0.859901             427.0   \n",
       "89                    0.915417                  0.884183             235.0   \n",
       "\n",
       "    word_sim_p_score  word_sim_n_count  word_sim_n_score  word_sim_score  \\\n",
       "0         150.254682               3.0         -0.333771      149.920911   \n",
       "1         108.484310               0.0          0.000000      108.484310   \n",
       "2         107.812128               0.0          0.000000      107.812128   \n",
       "3         160.342955               0.0          0.000000      160.342955   \n",
       "4         146.918488               4.0         -0.445256      146.473232   \n",
       "..               ...               ...               ...             ...   \n",
       "85        151.923564               0.0          0.000000      151.923564   \n",
       "86        138.879977               2.0         -0.171047      138.708930   \n",
       "87        124.190676               0.0          0.000000      124.190676   \n",
       "88        189.225096               1.0         -0.014288      189.210807   \n",
       "89         94.977588               1.0         -0.112045       94.865543   \n",
       "\n",
       "    flesch grade  Automated Readability  coleman_liau  cohesion score  \\\n",
       "0           12.0                   13.9         13.69        4.182405   \n",
       "1            8.9                   11.1         10.84        4.028763   \n",
       "2           11.1                   11.5         11.83        3.702329   \n",
       "3           11.5                   12.8         11.08        4.393480   \n",
       "4           11.8                   15.9         13.18        4.533942   \n",
       "..           ...                    ...           ...             ...   \n",
       "85          11.6                   14.1         12.71        4.280784   \n",
       "86          12.0                   14.2         14.10        4.160972   \n",
       "87          12.6                   14.0         11.49        3.940267   \n",
       "88          10.1                   11.0          9.34        4.489194   \n",
       "89          14.8                   16.6         11.38        4.121992   \n",
       "\n",
       "    uniqueness of words  positive_sentiment  negative_sentiment  \\\n",
       "0              0.444186               0.100               0.066   \n",
       "1              0.532915               0.053               0.282   \n",
       "2              0.480287               0.173               0.026   \n",
       "3              0.486047               0.198               0.038   \n",
       "4              0.544118               0.160               0.053   \n",
       "..                  ...                 ...                 ...   \n",
       "85             0.523474               0.160               0.124   \n",
       "86             0.509044               0.194               0.046   \n",
       "87             0.483680               0.144               0.090   \n",
       "88             0.472803               0.127               0.121   \n",
       "89             0.575290               0.140               0.072   \n",
       "\n",
       "    neutral_sentiment  discourse_marker  label  \n",
       "0               0.834              49.0    0.0  \n",
       "1               0.664              32.0    0.0  \n",
       "2               0.801              28.0    0.0  \n",
       "3               0.764              47.0    0.0  \n",
       "4               0.786              55.0    0.0  \n",
       "..                ...               ...    ...  \n",
       "85              0.716              49.0    0.0  \n",
       "86              0.760              54.0    0.0  \n",
       "87              0.765              50.0    0.0  \n",
       "88              0.752              62.0    0.0  \n",
       "89              0.788              29.0    0.0  \n",
       "\n",
       "[90 rows x 40 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the CSV file is: 39\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of columns in the CSV file is: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay                                    Computer-a device which has given a whole new ...\n",
       "topic                                                                       computers..use\n",
       "para_count                                                                             5.0\n",
       "word_count                                                                           377.0\n",
       "sentence_count                                                                        28.0\n",
       "sentence_len(mean)                                                               13.464286\n",
       "sentence_len(mode)                                                                    17.0\n",
       "sentence_len(max)                                                                     23.0\n",
       "sentence_len(min)                                                                      5.0\n",
       "stop_words                                                                           188.0\n",
       "sw_rates                                                                          6.714286\n",
       "unique_words                                                                         240.0\n",
       "noun                                                                              0.299735\n",
       "verb                                                                              0.124668\n",
       "adj                                                                               0.098143\n",
       "pron                                                                              0.034483\n",
       "modal verb                                                                             4.0\n",
       "most_freq                                                                             12.0\n",
       "number_of_repeatation                                                                  0.0\n",
       "max_occurance_of_most_frequent_phrase                                                  1.0\n",
       "avg sentence complexity                                                              4.517\n",
       "lexical diversity                                                                 0.522523\n",
       "sentence similarity mean                                                          0.799962\n",
       "sentence similarity median                                                         0.85084\n",
       "sentence similarity mode                                                          0.682003\n",
       "word_sim_p_count                                                                     367.0\n",
       "word_sim_p_score                                                                143.338517\n",
       "word_sim_n_count                                                                       2.0\n",
       "word_sim_n_score                                                                 -0.321976\n",
       "word_sim_score                                                                  143.016542\n",
       "flesch grade                                                                           8.5\n",
       "Automated Readability                                                                  9.8\n",
       "coleman_liau                                                                         10.95\n",
       "cohesion score                                                                    4.393551\n",
       "uniqueness of words                                                               0.522523\n",
       "positive_sentiment                                                                   0.106\n",
       "negative_sentiment                                                                   0.054\n",
       "neutral_sentiment                                                                     0.84\n",
       "label                                                                                  0.0\n",
       "Name: 38, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    data.loc[i,'label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../csv/human/csv_human-1.csv',index=False)\n",
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check duplicate topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "duplicates = []\n",
    "    \n",
    "for item in topics:\n",
    "        if item in seen:\n",
    "            if item not in duplicates:\n",
    "                duplicates.append(item)\n",
    "        else:\n",
    "            seen.add(item)\n",
    "print(duplicates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
