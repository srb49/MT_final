{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import statistics\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import language_tool_python  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gingerit.gingerit import GingerIt\n",
    "import pprint\n",
    "from spacy import displacy\n",
    "from sklearn.svm import SVC\n",
    "import textstat\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import math\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last decade has seen an increasing number of tourists travelling to visit natural wonder sights, ancient heritages and different cultures around the world. While some people might think that this international tourism has negative effects on the destination countries, I would contend that it has contributed to the economic development as well as preserved the culture and environment of the tourist destinations.\n",
      "\n",
      "Firstly, international tourism promotes many aspects of the destination country's economy in order to serve various demands of tourists. Take Cambodia for example, a large number of visitors coming to visit the Angkowat ancient temple need services like restaurants, hotels, souvenir shops and other stores. These demands trigger related business in the surrounding settings which in turn create many jobs for local people improve infrastructure and living standard. Therefore tourism has clearly improved lives in the tourist country.\n",
      "\n",
      "Secondly, through tourism industry, many cultural values have been preserved and natural environments have been protected. For instance, in Vietnam, many cultural costumes and natural scenes, namely 'Trong Dong' drum performance and 'Ha Long' bay, are being encouraged to preserve and funded by the tourism ministry. Without this support and profit from tourism, many traditional cultures would disappear due to its low income works. Thus, tourism has survived many non-tangible cultural values and beauty scenes.\n",
      "\n",
      "To conclude, as far as I am concerned, international tourism has both triggered economic development and maintained cultural and environment values of the tourist countries. In addition, the authorities should adequately support these sustainable developments.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "\n",
    "# Step 2: Locate the cell whose value you want to overwrite\n",
    "# Assuming you want to update the cell in the second row (index 1) and third column (Column_Name)\n",
    "row_index = 68\n",
    "column_name = 'essay'\n",
    "print(data.at[row_index, column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the rapid growth of technologies and global market development, worldwide travelling has not been so accessible than before. Despite it is profoundly beneficial for those desire to explore the world, it is also a controversial argument that drawbacks can be seen as a result of unnecessarily frequent travel. Personally I believe negative impacts brought up by international journey outweigh its benefits.\n",
      "\n",
      "First of all, due to the dramatic drop of travelling cost on airfare and accommodation, overseas visits are more feasible nowadays which effectively accelerate worldwide economic growth. However, excessive travels may also result in unhealthy development of certain markets. We can observe all over the globe that popular cities over relay on tourists related businesses and undermine the market diversity. One obvious example is the streets of Hong Kong overloaded with pharmacies and souvenir shops. The overall market environment becomes increasingly difficult for other businesses. At the end the city will suffer from the monotonous development and lose the attraction.\n",
      "\n",
      "Further and even more importantly, one of the largest fossil fuel consumptions in the world is airlines business. It is believed that uncontrolled global trips should take a critical responsibility in our environmental pollution. Imagine how luxury it is for a weekend trip decades ago, yet it is now affordable almost for everyone with low cost flights and AirBnB. With the extra help of internet, travel lovers can even lockdown their journey solely at their fingertips. This is the reason why scientists attribute this overwhelming travel trend to the irreversible damage to our planet.\n",
      "\n",
      "In conclusion, overseas travelling is an irresistible trend for the majority which also helps in globalization. On the other hand, yet, I agree the trend also leads to more serious damages on our cultural and natural environments. It is important for all humility to get an optimal balance between economic development and environmental responsibilities.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Set the new value for the cell\n",
    "with open('68', 'r',encoding='utf-8', errors='ignore') as txt_file:\n",
    "    txt = txt_file.read()\n",
    "    #print(txt)\n",
    "data.loc[row_index, column_name] = txt\n",
    "print(data.loc[row_index, column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(df, column_index, feature):\n",
    "    # Get the column names\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Insert the new feature at the specified index\n",
    "    columns.insert(column_index, feature)\n",
    "\n",
    "    # Reindex the dataframe with the updated column order\n",
    "    df = df.reindex(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def walk_tree1(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        dep=[]\n",
    "        d=0\n",
    "        for child in node.children:\n",
    "            d=walk_tree1(child, depth + 1)\n",
    "            dep.append(d)\n",
    "        return max(dep)\n",
    "    \n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "def avg_sentence_complexity(str1):\n",
    "    final_list=[]\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    temp_list=[]\n",
    "    sents=[]\n",
    "    txt = str1\n",
    "    txt=\" \".join(txt.split())\n",
    "    text = nlp(txt)\n",
    "    sent_tokenize1 = (list(text.sents))\n",
    "    sents.append(sent_tokenize1[0])\n",
    "    for sent in text.sents:\n",
    "        d=walk_tree1(sent.root, 0)\n",
    "        temp_list.append(d)\n",
    "            #displacy.render(sent, style=\"dep\")\n",
    "        final_list.append(temp_list)\n",
    "        #print(\"essay first line \",sents[len(sents)-1],\"max is \",max(temp_list),\" min is \",min(temp_list),\" mean is \", round(statistics.mean(temp_list),3),\n",
    "        #  \" median is \",statistics.median(temp_list),\" mode is \",statistics.mode(temp_list))\n",
    "    return round(statistics.mean(temp_list),3)\n",
    "\n",
    "def grammar_check(str1):\n",
    "\n",
    "    my_tool = language_tool_python.LanguageTool('en-GB')###########\n",
    "    matches = my_tool.check(str1)\n",
    "    list_s1=[]\n",
    "    if(matches):\n",
    "        fs=str1.split(\".\")[0]\n",
    "        #print(i,\" index | essay first sentences \",fs)\n",
    "        for match in matches:\n",
    "            print(match)\n",
    "    #print(match.ruleId)\n",
    "    #print('Message:',match.message,\"\\n\")\n",
    "            #print(\"####################### \",match.context)\n",
    "            if match.replacements:\n",
    "                chopped_string = match.replacements[0].replace(',','').strip()\n",
    "                list_s1.append(chopped_string)\n",
    "            else:\n",
    "                list_s1.append(match.ruleId)    \n",
    "        #print(\"final list\",list_s1)\n",
    "    return len(list_s1)\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list1=[\" \".join(ngram) for ngram in ngrams]\n",
    "    #return [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list1\n",
    "\n",
    "def rep_count(str1):\n",
    "    tot_rp=[]\n",
    "    max_rp=[]\n",
    "    #print(ess[i],\"\\n\",\"new\",\"\\n\")\n",
    "    list1=generate_ngrams(str1, 5)\n",
    "    myset = set(list1)\n",
    "    rept = len(list1) - len(myset)\n",
    "    tot_rp.append(rept)\n",
    "    c = collections.Counter(list1)\n",
    "    c.most_common(1)[0][1]\n",
    "    max_rp.append(c.most_common(1)[0][1])\n",
    "    #print(\"\\n\\nnew essay index \",i,\": for essay \",list1[0],\":\")\n",
    "    print(\"repeat \",rept,\" times | most common phrase is **\",c.most_common(1)[0][0], \"** |number of repeataion happened: \",c.most_common(1)[0][1],\" times\\n\")\n",
    "    #print(\"\\ncommon phrases: \\n\",c)\n",
    "    #print(\"\\nmy set: \\n\",myset)\n",
    "    #print(\"list \",tot_rp,\" \",max_rp)\n",
    "    #print(\"individual \",rept,\" \",c.most_common(1)[0][1])\n",
    "    return rept,c.most_common(1)[0][1],c.most_common(10)\n",
    "\n",
    "def most_frequent(str1):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    str1=\" \".join(str1.split())\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop!= True and token.is_punct!= True]\n",
    "    w_c=Counter(words)\n",
    "    m_f=w_c.most_common(1)[0][1]\n",
    "    \n",
    "    tags=[]\n",
    "    for w in text:\n",
    "        tags.append(w.pos_)\n",
    "    tag_freq = Counter(tags)\n",
    "    #print (\"=====tag FREQUENCY=====\")\n",
    "    #print(tag_freq.values())\n",
    "    n=0\n",
    "    v=0\n",
    "    a=0\n",
    "    p=0\n",
    "    w_l=word_cnt(str1)\n",
    "    for key in tag_freq:\n",
    "        #print(\"hi\")\n",
    "        if key=='NOUN':\n",
    "            n=tag_freq[key]/w_l\n",
    "            #print(\"n \",n)\n",
    "        elif key == 'VERB':\n",
    "            v=tag_freq[key]/w_l\n",
    "            #print(\"v \",v)\n",
    "        elif key =='ADJ':\n",
    "            a=tag_freq[key]/w_l\n",
    "            #print(\"a \",a)\n",
    "        elif key =='PRON':\n",
    "            p=tag_freq[key]/w_l\n",
    "\n",
    "        elif key=='MD':\n",
    "            print(\"the number of modal verbs \",key, tag_freq.values())\n",
    "            #print(\"p \",p)\n",
    "        #print(\"key is \",key,\" value \",tag_freq[key], \" frequency \", tag_freq[key]/w_l)\n",
    "        # print(\"n, v, a, p \",n,v,a,p)\n",
    "    return m_f,n,v,a,p\n",
    "\n",
    "def uniq_w(str1):\n",
    "    return len(set(str1.split()))\n",
    "\n",
    "def stop_w(str1):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    return(len(words))\n",
    "\n",
    "def stop_w_r(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    \n",
    "    l=len(words)\n",
    "    swr=l/n\n",
    "    return swr\n",
    "\n",
    "def sen_len(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    i=0\n",
    "    avg=0\n",
    "    sl=[]\n",
    "    for num in number_of_sentences:\n",
    "        i+=1\n",
    "        words=num.split()\n",
    "        avg+=len(words)\n",
    "        sl.append(len(words))\n",
    "    #print(num, len(num),len(words))\n",
    "    avg=avg/i\n",
    "    md=statistics.mode(sl)\n",
    "    mx=max(sl)\n",
    "    mn=min(sl)\n",
    "    return avg,md,mx,mn\n",
    "\n",
    "def sen_num(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "    #print(n)\n",
    "    return n\n",
    "\n",
    "def word_cnt(str1):\n",
    "    words=str1.split()\n",
    "    return len(words)\n",
    "\n",
    "def lexical_diversity(str1):\n",
    "    tokens = word_tokenize(str1)\n",
    "\n",
    "    lex_div = len(set(tokens)) / len(tokens)\n",
    "    print(\"Lexical Diversity Score of essay:\", lex_div)\n",
    "    return lex_div\n",
    "\n",
    "def discourse_conn(str1):\n",
    "    tokens = nltk.word_tokenize(str1)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Identify discourse connectors based on part-of-speech patterns\n",
    "    discourse_connectors = [token for token, pos in pos_tags if pos in ['CC', 'RB', 'IN']]\n",
    "\n",
    "    # Output the identified discourse connectors\n",
    "    print(\"Discourse connectors:\", discourse_connectors)\n",
    "    print(\"Discourse connector count:\", len(discourse_connectors))\n",
    "    return len(discourse_connectors)\n",
    "\n",
    "def sentence_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    sent_tokenize = (list(text.sents))\n",
    "    i=0\n",
    "    sensim=0.0\n",
    "    sensim_list=[]\n",
    "    for sent2 in sent_tokenize:\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            sent1=sent2\n",
    "            continue\n",
    "        #print (sent2.similarity(sent1))\n",
    "        sensim+=sent2.similarity(sent1)\n",
    "        sensim_score=sent2.similarity(sent1)\n",
    "        sent1=sent2\n",
    "        sensim_list.append(sensim_score)\n",
    "    sensim=sensim/i\n",
    "    #print(\"average sentence similarity is \",sensim)\n",
    "\n",
    "    return sensim,sensim_list\n",
    "\n",
    "def word_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    p=0\n",
    "    ps=0\n",
    "    n=0\n",
    "    ns=0\n",
    "    total=0\n",
    "    word = [token.text for token in text if token.is_punct != True]\n",
    "    for i in range(len(word)):\n",
    "    #x.append(i)\n",
    "    #print(i)\n",
    "    #print(words[i])\n",
    "        w2=word[i]\n",
    "        if i == 0:\n",
    "            w1=w2\n",
    "            continue\n",
    "    #print (nlp(w2).similarity(nlp(w1)))\n",
    "        #print(w1+ \" \"+w2+\" \",nlp(w2).similarity(nlp(w1)))\n",
    "        if nlp(w2).similarity(nlp(w1))>0:\n",
    "            p=p+1\n",
    "            ps+=nlp(w2).similarity(nlp(w1))\n",
    "        elif nlp(w2).similarity(nlp(w1))<0:\n",
    "            n=n+1\n",
    "            ns+=nlp(w2).similarity(nlp(w1))\n",
    "    #y1=y1+float(nlp(w2).similarity(nlp(w1)))\n",
    "    #print(y1)\n",
    "    #y.append(y1)\n",
    "        total+=nlp(w2).similarity(nlp(w1))\n",
    "        w1=w2\n",
    "    #print(p,n)\n",
    "    return p,n,total,ps,ns\n",
    "\n",
    "def get_modal_verbs(text):\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    cnt=0\n",
    "    modal_verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.tag_ == 'MD':\n",
    "            modal_verbs.append(token.lemma_)\n",
    "            cnt+=1\n",
    "\n",
    "    return cnt,modal_verbs\n",
    "\n",
    "def calculate_readability(text):\n",
    "    # Calculate the Flesch-Kincaid Grade Level\n",
    "    flesch_grade = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    # Calculate the Automated Readability Index (ARI)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    # Calculate the Coleman-Liau Index\n",
    "    coleman_liau = textstat.coleman_liau_index(text)\n",
    "\n",
    "    # Return the readability scores\n",
    "    return flesch_grade, ari, coleman_liau\n",
    "\n",
    "def calculate_cohesion(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Convert generator to a list of bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # Calculate bigram frequencies\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Calculate pointwise mutual information (PMI) for bigrams\n",
    "    total_bigrams = len(bigrams)\n",
    "    pmi_scores = {}\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        pmi = math.log(freq / total_bigrams / ((tokens.count(bigram[0]) / len(tokens)) * (tokens.count(bigram[1]) / len(tokens))))\n",
    "        pmi_scores[bigram] = pmi\n",
    "\n",
    "    # Calculate average PMI as a measure of cohesion\n",
    "    avg_pmi = sum(pmi_scores.values()) / len(pmi_scores)\n",
    "\n",
    "    return avg_pmi\n",
    "\n",
    "def calculate_uniqueness_score(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Calculate the frequency distribution of words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Calculate the uniqueness score as the ratio of unique words to total words\n",
    "    uniqueness_score = len(word_freq) / len(words)\n",
    "\n",
    "    return uniqueness_score\n",
    "\n",
    "def detect_emotional_tone(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "def para_cnt(str1):\n",
    "    paragraphs = str1.split(\"\\n\\n\")\n",
    "    return len(paragraphs)\n",
    "\n",
    "def discourse_marker():\n",
    "    with open('../other/da.txt',\"r\") as file:\n",
    "        text=file.read()\n",
    "    d_list = text.split(\",\")\n",
    "    #print(d_list)\n",
    "    d_list.remove('')\n",
    "    #index = d_list.index('')\n",
    "    #print(index)\n",
    "    #d_list.remove('')\n",
    "    return d_list\n",
    "\n",
    "def discourse_marker_check(essay):\n",
    "    d_list=discourse_marker()\n",
    "    count=0\n",
    "    for marker in d_list:\n",
    "        occurrences = re.findall(r'\\b' + re.escape(marker) + r'\\b', essay, flags=re.IGNORECASE)\n",
    "        count += len(occurrences)\n",
    "        #print(\"occurances \",occurrences,\" for marker \",marker)\n",
    "    #print(\"count \",count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat  0  times | most common phrase is ** with the rapid growth of ** |number of repeataion happened:  1  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.6035502958579881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_280412/3978411362.py:252: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if nlp(w2).similarity(nlp(w1))>0:\n",
      "/tmp/ipykernel_280412/3978411362.py:255: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  elif nlp(w2).similarity(nlp(w1))<0:\n",
      "/tmp/ipykernel_280412/3978411362.py:261: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  total+=nlp(w2).similarity(nlp(w1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the CSV file is: 40\n"
     ]
    }
   ],
   "source": [
    "ess=data.loc[:,'essay']\n",
    "\n",
    "for i in range(len(ess)):\n",
    " if i==68:\n",
    "    pc=para_cnt(ess[i])\n",
    "    data.loc[i,'para_count']=pc\n",
    "\n",
    "    wc=word_cnt(ess[i])\n",
    "    data.loc[i,'word_count']=wc\n",
    "\n",
    "    ns=sen_num(ess[i])\n",
    "    data.loc[i,'sentence_count']=ns\n",
    "\n",
    "    avg,md,mx,mn=sen_len(ess[i])\n",
    "    data.loc[i,'sentence_len(mean)']=avg\n",
    "    data.loc[i,'sentence_len(mode)']=md\n",
    "    data.loc[i,'sentence_len(max)']=mx\n",
    "    data.loc[i,'sentence_len(min)']=mn\n",
    "\n",
    "    sw=stop_w(ess[i])\n",
    "    swr=stop_w_r(ess[i])\n",
    "    data.loc[i,'stop_words']=sw\n",
    "    data.loc[i,'sw_rates']=swr\n",
    "\n",
    "    uw=uniq_w(ess[i])\n",
    "    data.loc[i,'unique_words']=uw\n",
    "\n",
    "    uw=uniq_w(ess[i])\n",
    "    mf,n,v,a,p=most_frequent(ess[i])\n",
    "    mvc,mv=get_modal_verbs(ess[i])\n",
    "    data.loc[i,'noun']=n\n",
    "    data.loc[i,'verb']=v\n",
    "    data.loc[i,'adj']=a\n",
    "    data.loc[i,'pron']=p,\n",
    "    data.loc[i,'modal verb']=mvc\n",
    "    data.loc[i,'most_freq']=mf\n",
    "\n",
    "    tr,mr,rep_list=rep_count(ess[i])\n",
    "    data.loc[i,'number_of_repeatation']=tr\n",
    "    data.loc[i,'max_occurance_of_most_frequent_phrase']=mr\n",
    "\n",
    "    asc=avg_sentence_complexity(ess[i])\n",
    "    data.loc[i,'avg sentence complexity']=asc\n",
    "\n",
    "    l_d=lexical_diversity(ess[i])\n",
    "    data.loc[i,'lexical diversity']=l_d\n",
    "\n",
    "    sensim,sensim_list=sentence_similarity(ess[i])\n",
    "    sensim_list.sort()\n",
    "    sensim_mod=statistics.mode(sensim_list)\n",
    "    sensim_med=statistics.median(sensim_list)\n",
    "    data.loc[i,'sentence similarity mean']=sensim\n",
    "    data.loc[i,'sentence similarity median']=sensim_med\n",
    "    data.loc[i,'sentence similarity mode']=sensim_mod\n",
    "\n",
    "    pos,neg,tot,p_s,n_s=word_similarity(ess[i])\n",
    "    data.loc[i,'word_sim_p_count']=pos\n",
    "    data.loc[i,'word_sim_p_score']=p_s\n",
    "    data.loc[i,'word_sim_n_count']=neg\n",
    "    data.loc[i,'word_sim_n_score']=n_s\n",
    "    data.loc[i,'word_sim_score']=tot\n",
    "\n",
    "    f_g, ari, c_l = calculate_readability(ess[i])\n",
    "    data.loc[i,'flesch grade']=f_g\n",
    "    data.loc[i,'Automated Readability']=ari\n",
    "    data.loc[i,'coleman_liau']=c_l\n",
    "\n",
    "    coh_score=calculate_cohesion(ess[i])\n",
    "    data.loc[i,'cohesion score']=coh_score\n",
    "\n",
    "    uniq = calculate_uniqueness_score(ess[i])\n",
    "    data.loc[i,'uniqueness of words']=uniq\n",
    "\n",
    "    emot = detect_emotional_tone(ess[i])\n",
    "    data.loc[i,'positive_sentiment']=emot['pos']\n",
    "    data.loc[i,'negative_sentiment']=emot['neg']\n",
    "    data.loc[i,'neutral_sentiment']=emot['neu']\n",
    "\n",
    "    cnt=discourse_marker_check(ess[i])\n",
    "    data.loc[i,'discourse_marker']=cnt\n",
    "\n",
    "    data.loc[i,'label']=0\n",
    "\n",
    "print(f\"The number of columns in the CSV file is: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 essay  \\\n",
      "                                                  self   \n",
      "8    The introduction of email and text messaging h...   \n",
      "68   With the rapid growth of technologies and glob...   \n",
      "95                                                 NaN   \n",
      "135                                                NaN   \n",
      "138                                                NaN   \n",
      "\n",
      "                                                       word_count         \\\n",
      "                                                 other       self  other   \n",
      "8    Over the last half century the change in the l...      367.0  291.0   \n",
      "68   The last decade has seen an increasing number ...      310.0  251.0   \n",
      "95                                                 NaN        NaN    NaN   \n",
      "135                                                NaN        NaN    NaN   \n",
      "138                                                NaN        NaN    NaN   \n",
      "\n",
      "    sentence_count       sentence_len(mean)            sentence_len(mode)  \\\n",
      "              self other               self      other               self   \n",
      "8             18.0  14.0          20.388889  20.785714                NaN   \n",
      "68            17.0  12.0          18.235294  20.916667               21.0   \n",
      "95             NaN   NaN          26.272727  26.272727                NaN   \n",
      "135            NaN   NaN          21.235294  21.235294                NaN   \n",
      "138            NaN   NaN          25.153846  25.153846                NaN   \n",
      "\n",
      "           ... uniqueness of words           positive_sentiment         \\\n",
      "    other  ...                self     other               self  other   \n",
      "8     NaN  ...            0.507282  0.547988              0.112  0.066   \n",
      "68   10.0  ...            0.603550  0.583039              0.140  0.190   \n",
      "95    NaN  ...                 NaN       NaN                NaN    NaN   \n",
      "135   NaN  ...                 NaN       NaN                NaN    NaN   \n",
      "138   NaN  ...                 NaN       NaN                NaN    NaN   \n",
      "\n",
      "    negative_sentiment        neutral_sentiment        discourse_marker        \n",
      "                  self  other              self  other             self other  \n",
      "8                0.067  0.032             0.822  0.902             56.0  49.0  \n",
      "68               0.113  0.044             0.747  0.765             42.0  33.0  \n",
      "95                 NaN    NaN               NaN    NaN              NaN   NaN  \n",
      "135                NaN    NaN               NaN    NaN              NaN   NaN  \n",
      "138                NaN    NaN               NaN    NaN              NaN   NaN  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('../csv/human/csv_human-1.csv')\n",
    "df2 = pd.read_csv('../csv/human/csv_human-1_2.csv')\n",
    "comparison_result = df1.compare(df2)\n",
    "print(comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essay                                    With the rapid growth of technologies and glob...\n",
      "topic                                    International tourism is now more common than ...\n",
      "para_count                                                                             4.0\n",
      "word_count                                                                           310.0\n",
      "sentence_count                                                                        17.0\n",
      "sentence_len(mean)                                                               18.235294\n",
      "sentence_len(mode)                                                                    21.0\n",
      "sentence_len(max)                                                                     30.0\n",
      "sentence_len(min)                                                                     10.0\n",
      "stop_words                                                                           154.0\n",
      "sw_rates                                                                          9.058824\n",
      "unique_words                                                                         209.0\n",
      "noun                                                                              0.258065\n",
      "verb                                                                                   0.1\n",
      "adj                                                                                0.13871\n",
      "pron                                                                              0.032258\n",
      "modal verb                                                                             6.0\n",
      "most_freq                                                                              4.0\n",
      "number_of_repeatation                                                                  0.0\n",
      "max_occurance_of_most_frequent_phrase                                                  1.0\n",
      "avg sentence complexity                                                              6.188\n",
      "lexical diversity                                                                  0.60355\n",
      "sentence similarity mean                                                          0.826871\n",
      "sentence similarity median                                                         0.88663\n",
      "sentence similarity mode                                                          0.782784\n",
      "word_sim_p_count                                                                     303.0\n",
      "word_sim_p_score                                                                119.476273\n",
      "word_sim_n_count                                                                       1.0\n",
      "word_sim_n_score                                                                 -0.100471\n",
      "word_sim_score                                                                  119.375801\n",
      "flesch grade                                                                          12.7\n",
      "Automated Readability                                                                 13.8\n",
      "coleman_liau                                                                         14.39\n",
      "cohesion score                                                                    4.512625\n",
      "uniqueness of words                                                                0.60355\n",
      "positive_sentiment                                                                    0.14\n",
      "negative_sentiment                                                                   0.113\n",
      "neutral_sentiment                                                                    0.747\n",
      "discourse_marker                                                                      42.0\n",
      "label                                                                                  0.0\n",
      "Name: 68, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essay                                    The last decade has seen an increasing number ...\n",
      "topic                                    International tourism is now more common than ...\n",
      "para_count                                                                             4.0\n",
      "word_count                                                                           251.0\n",
      "sentence_count                                                                        12.0\n",
      "sentence_len(mean)                                                               20.916667\n",
      "sentence_len(mode)                                                                    10.0\n",
      "sentence_len(max)                                                                     39.0\n",
      "sentence_len(min)                                                                     10.0\n",
      "stop_words                                                                           106.0\n",
      "sw_rates                                                                          8.833333\n",
      "unique_words                                                                         166.0\n",
      "noun                                                                              0.314741\n",
      "verb                                                                              0.139442\n",
      "adj                                                                               0.135458\n",
      "pron                                                                              0.011952\n",
      "modal verb                                                                             4.0\n",
      "most_freq                                                                              8.0\n",
      "number_of_repeatation                                                                  0.0\n",
      "max_occurance_of_most_frequent_phrase                                                  1.0\n",
      "avg sentence complexity                                                              5.083\n",
      "lexical diversity                                                                 0.583039\n",
      "sentence similarity mean                                                          0.826919\n",
      "sentence similarity median                                                        0.890273\n",
      "sentence similarity mode                                                          0.865087\n",
      "word_sim_p_count                                                                     246.0\n",
      "word_sim_p_score                                                                 95.523865\n",
      "word_sim_n_count                                                                       1.0\n",
      "word_sim_n_score                                                                 -0.110712\n",
      "word_sim_score                                                                   95.413153\n",
      "flesch grade                                                                          12.6\n",
      "Automated Readability                                                                 16.8\n",
      "coleman_liau                                                                         16.07\n",
      "cohesion score                                                                    4.189364\n",
      "uniqueness of words                                                               0.583039\n",
      "positive_sentiment                                                                    0.19\n",
      "negative_sentiment                                                                   0.044\n",
      "neutral_sentiment                                                                    0.765\n",
      "discourse_marker                                                                      33.0\n",
      "label                                                                                  0.0\n",
      "Name: 68, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df1.iloc[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Write the DataFrame back to the CSV file, overwriting the original file\n",
    "#df.to_csv('your_file.csv', index=False)\n",
    "data.to_csv('../csv/human/csv_human-1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
