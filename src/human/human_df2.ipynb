{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import statistics\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import language_tool_python  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gingerit.gingerit import GingerIt\n",
    "import pprint\n",
    "from spacy import displacy\n",
    "from sklearn.svm import SVC\n",
    "import textstat\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import math\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(df, column_index, feature):\n",
    "    # Get the column names\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Insert the new feature at the specified index\n",
    "    columns.insert(column_index, feature)\n",
    "\n",
    "    # Reindex the dataframe with the updated column order\n",
    "    df = df.reindex(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def walk_tree1(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        dep=[]\n",
    "        d=0\n",
    "        for child in node.children:\n",
    "            d=walk_tree1(child, depth + 1)\n",
    "            dep.append(d)\n",
    "        return max(dep)\n",
    "    \n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "def avg_sentence_complexity(str1):\n",
    "    final_list=[]\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    temp_list=[]\n",
    "    sents=[]\n",
    "    txt = str1\n",
    "    txt=\" \".join(txt.split())\n",
    "    text = nlp(txt)\n",
    "    sent_tokenize1 = (list(text.sents))\n",
    "    sents.append(sent_tokenize1[0])\n",
    "    for sent in text.sents:\n",
    "        d=walk_tree1(sent.root, 0)\n",
    "        temp_list.append(d)\n",
    "            #displacy.render(sent, style=\"dep\")\n",
    "        final_list.append(temp_list)\n",
    "        #print(\"essay first line \",sents[len(sents)-1],\"max is \",max(temp_list),\" min is \",min(temp_list),\" mean is \", round(statistics.mean(temp_list),3),\n",
    "        #  \" median is \",statistics.median(temp_list),\" mode is \",statistics.mode(temp_list))\n",
    "    return round(statistics.mean(temp_list),3)\n",
    "\n",
    "def grammar_check(str1):\n",
    "\n",
    "    my_tool = language_tool_python.LanguageTool('en-GB')###########\n",
    "    matches = my_tool.check(str1)\n",
    "    list_s1=[]\n",
    "    if(matches):\n",
    "        fs=str1.split(\".\")[0]\n",
    "        #print(i,\" index | essay first sentences \",fs)\n",
    "        for match in matches:\n",
    "            print(match)\n",
    "    #print(match.ruleId)\n",
    "    #print('Message:',match.message,\"\\n\")\n",
    "            #print(\"####################### \",match.context)\n",
    "            if match.replacements:\n",
    "                chopped_string = match.replacements[0].replace(',','').strip()\n",
    "                list_s1.append(chopped_string)\n",
    "            else:\n",
    "                list_s1.append(match.ruleId)    \n",
    "        #print(\"final list\",list_s1)\n",
    "    return len(list_s1)\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list1=[\" \".join(ngram) for ngram in ngrams]\n",
    "    #return [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list1\n",
    "\n",
    "def rep_count(str1):\n",
    "    tot_rp=[]\n",
    "    max_rp=[]\n",
    "    #print(ess[i],\"\\n\",\"new\",\"\\n\")\n",
    "    list1=generate_ngrams(str1, 5)\n",
    "    myset = set(list1)\n",
    "    rept = len(list1) - len(myset)\n",
    "    tot_rp.append(rept)\n",
    "    c = collections.Counter(list1)\n",
    "    c.most_common(1)[0][1]\n",
    "    max_rp.append(c.most_common(1)[0][1])\n",
    "    #print(\"\\n\\nnew essay index \",i,\": for essay \",list1[0],\":\")\n",
    "    print(\"repeat \",rept,\" times | most common phrase is **\",c.most_common(1)[0][0], \"** |number of repeataion happened: \",c.most_common(1)[0][1],\" times\\n\")\n",
    "    #print(\"\\ncommon phrases: \\n\",c)\n",
    "    #print(\"\\nmy set: \\n\",myset)\n",
    "    #print(\"list \",tot_rp,\" \",max_rp)\n",
    "    #print(\"individual \",rept,\" \",c.most_common(1)[0][1])\n",
    "    return rept,c.most_common(1)[0][1],c.most_common(10)\n",
    "\n",
    "def most_frequent(str1):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    str1=\" \".join(str1.split())\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop!= True and token.is_punct!= True]\n",
    "    w_c=Counter(words)\n",
    "    m_f=w_c.most_common(1)[0][1]\n",
    "    \n",
    "    tags=[]\n",
    "    for w in text:\n",
    "        tags.append(w.pos_)\n",
    "    tag_freq = Counter(tags)\n",
    "    #print (\"=====tag FREQUENCY=====\")\n",
    "    #print(tag_freq.values())\n",
    "    n=0\n",
    "    v=0\n",
    "    a=0\n",
    "    p=0\n",
    "    w_l=word_cnt(str1)\n",
    "    for key in tag_freq:\n",
    "        #print(\"hi\")\n",
    "        if key=='NOUN':\n",
    "            n=tag_freq[key]/w_l\n",
    "            #print(\"n \",n)\n",
    "        elif key == 'VERB':\n",
    "            v=tag_freq[key]/w_l\n",
    "            #print(\"v \",v)\n",
    "        elif key =='ADJ':\n",
    "            a=tag_freq[key]/w_l\n",
    "            #print(\"a \",a)\n",
    "        elif key =='PRON':\n",
    "            p=tag_freq[key]/w_l\n",
    "\n",
    "        elif key=='MD':\n",
    "            print(\"the number of modal verbs \",key, tag_freq.values())\n",
    "            #print(\"p \",p)\n",
    "        #print(\"key is \",key,\" value \",tag_freq[key], \" frequency \", tag_freq[key]/w_l)\n",
    "        # print(\"n, v, a, p \",n,v,a,p)\n",
    "    return m_f,n,v,a,p\n",
    "\n",
    "def uniq_w(str1):\n",
    "    return len(set(str1.split()))\n",
    "\n",
    "def stop_w(str1):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    return(len(words))\n",
    "\n",
    "def stop_w_r(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    \n",
    "    l=len(words)\n",
    "    swr=l/n\n",
    "    return swr\n",
    "\n",
    "def sen_len(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    i=0\n",
    "    avg=0\n",
    "    sl=[]\n",
    "    for num in number_of_sentences:\n",
    "        i+=1\n",
    "        words=num.split()\n",
    "        avg+=len(words)\n",
    "        sl.append(len(words))\n",
    "    #print(num, len(num),len(words))\n",
    "    avg=avg/i\n",
    "    md=statistics.mode(sl)\n",
    "    mx=max(sl)\n",
    "    mn=min(sl)\n",
    "    return avg,md,mx,mn\n",
    "\n",
    "def sen_num(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "    #print(n)\n",
    "    return n\n",
    "\n",
    "def word_cnt(str1):\n",
    "    words=str1.split()\n",
    "    return len(words)\n",
    "\n",
    "def lexical_diversity(str1):\n",
    "    tokens = word_tokenize(str1)\n",
    "\n",
    "    lex_div = len(set(tokens)) / len(tokens)\n",
    "    print(\"Lexical Diversity Score of essay:\", lex_div)\n",
    "    return lex_div\n",
    "\n",
    "def discourse_conn(str1):\n",
    "    tokens = nltk.word_tokenize(str1)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Identify discourse connectors based on part-of-speech patterns\n",
    "    discourse_connectors = [token for token, pos in pos_tags if pos in ['CC', 'RB', 'IN']]\n",
    "\n",
    "    # Output the identified discourse connectors\n",
    "    print(\"Discourse connectors:\", discourse_connectors)\n",
    "    print(\"Discourse connector count:\", len(discourse_connectors))\n",
    "    return len(discourse_connectors)\n",
    "\n",
    "def sentence_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    sent_tokenize = (list(text.sents))\n",
    "    i=0\n",
    "    sensim=0.0\n",
    "    sensim_list=[]\n",
    "    for sent2 in sent_tokenize:\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            sent1=sent2\n",
    "            continue\n",
    "        #print (sent2.similarity(sent1))\n",
    "        sensim+=sent2.similarity(sent1)\n",
    "        sensim_score=sent2.similarity(sent1)\n",
    "        sent1=sent2\n",
    "        sensim_list.append(sensim_score)\n",
    "    sensim=sensim/i\n",
    "    #print(\"average sentence similarity is \",sensim)\n",
    "\n",
    "    return sensim,sensim_list\n",
    "\n",
    "def word_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    p=0\n",
    "    ps=0\n",
    "    n=0\n",
    "    ns=0\n",
    "    total=0\n",
    "    word = [token.text for token in text if token.is_punct != True]\n",
    "    for i in range(len(word)):\n",
    "    #x.append(i)\n",
    "    #print(i)\n",
    "    #print(words[i])\n",
    "        w2=word[i]\n",
    "        if i == 0:\n",
    "            w1=w2\n",
    "            continue\n",
    "    #print (nlp(w2).similarity(nlp(w1)))\n",
    "        #print(w1+ \" \"+w2+\" \",nlp(w2).similarity(nlp(w1)))\n",
    "        if nlp(w2).similarity(nlp(w1))>0:\n",
    "            p=p+1\n",
    "            ps+=nlp(w2).similarity(nlp(w1))\n",
    "        elif nlp(w2).similarity(nlp(w1))<0:\n",
    "            n=n+1\n",
    "            ns+=nlp(w2).similarity(nlp(w1))\n",
    "    #y1=y1+float(nlp(w2).similarity(nlp(w1)))\n",
    "    #print(y1)\n",
    "    #y.append(y1)\n",
    "        total+=nlp(w2).similarity(nlp(w1))\n",
    "        w1=w2\n",
    "    #print(p,n)\n",
    "    return p,n,total,ps,ns\n",
    "\n",
    "def get_modal_verbs(text):\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    cnt=0\n",
    "    modal_verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.tag_ == 'MD':\n",
    "            modal_verbs.append(token.lemma_)\n",
    "            cnt+=1\n",
    "\n",
    "    return cnt,modal_verbs\n",
    "\n",
    "def calculate_readability(text):\n",
    "    # Calculate the Flesch-Kincaid Grade Level\n",
    "    flesch_grade = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    # Calculate the Automated Readability Index (ARI)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    # Calculate the Coleman-Liau Index\n",
    "    coleman_liau = textstat.coleman_liau_index(text)\n",
    "\n",
    "    # Return the readability scores\n",
    "    return flesch_grade, ari, coleman_liau\n",
    "\n",
    "def calculate_cohesion(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Convert generator to a list of bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # Calculate bigram frequencies\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Calculate pointwise mutual information (PMI) for bigrams\n",
    "    total_bigrams = len(bigrams)\n",
    "    pmi_scores = {}\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        pmi = math.log(freq / total_bigrams / ((tokens.count(bigram[0]) / len(tokens)) * (tokens.count(bigram[1]) / len(tokens))))\n",
    "        pmi_scores[bigram] = pmi\n",
    "\n",
    "    # Calculate average PMI as a measure of cohesion\n",
    "    avg_pmi = sum(pmi_scores.values()) / len(pmi_scores)\n",
    "\n",
    "    return avg_pmi\n",
    "\n",
    "def calculate_uniqueness_score(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Calculate the frequency distribution of words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Calculate the uniqueness score as the ratio of unique words to total words\n",
    "    uniqueness_score = len(word_freq) / len(words)\n",
    "\n",
    "    return uniqueness_score\n",
    "\n",
    "def detect_emotional_tone(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "def para_cnt(str1):\n",
    "    paragraphs = str1.split(\"\\n\\n\")\n",
    "    return len(paragraphs)\n",
    "\n",
    "def discourse_marker():\n",
    "    with open('../../src/other/da.txt',\"r\") as file:\n",
    "        text=file.read()\n",
    "    d_list = text.split(\",\")\n",
    "    #print(d_list)\n",
    "    d_list.remove('')\n",
    "    #index = d_list.index('')\n",
    "    #print(index)\n",
    "    #d_list.remove('')\n",
    "    return d_list\n",
    "\n",
    "def discourse_marker_check(essay):\n",
    "    d_list=discourse_marker()\n",
    "    count=0\n",
    "    for marker in d_list:\n",
    "        occurrences = re.findall(r'\\b' + re.escape(marker) + r'\\b', essay, flags=re.IGNORECASE)\n",
    "        count += len(occurrences)\n",
    "        #print(\"occurances \",occurrences,\" for marker \",marker)\n",
    "    #print(\"count \",count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_280392/428135722.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
      "/tmp/ipykernel_280392/428135722.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
      "/tmp/ipykernel_280392/428135722.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
      "/tmp/ipykernel_280392/428135722.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
      "/tmp/ipykernel_280392/428135722.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
      "/tmp/ipykernel_280392/428135722.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
      "/tmp/ipykernel_280392/428135722.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "for infile in sorted(glob.glob('../human/input2/*'), key=numericalSort):\n",
    "    #print (\"Current File Being Processed is: \" + infile)\n",
    "    with open(infile, 'r',encoding='utf-8', errors='ignore') as file:\n",
    "        first_line = file.readline().strip()\n",
    "        remaining_content = file.read()\n",
    "        if remaining_content.startswith('\\n'):\n",
    "            remaining_content = remaining_content[1:]\n",
    "        df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
    "        '''ch=input(\"enter e\")\n",
    "        if ch =='e' or ch =='E':\n",
    "            break\n",
    "        else:\n",
    "            continue'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assuring road safety proves to be of great nec...</td>\n",
       "      <td>The only way to improve safety on our roads is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The income inequality among people has trigger...</td>\n",
       "      <td>Government should assist disadvantaged people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Choosing a major is one of the most critical d...</td>\n",
       "      <td>More people should be encouraged to do vocatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recently, our environment has been in the lime...</td>\n",
       "      <td>Living in big cities is bad for your health. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Several people believe that Maths is regarded ...</td>\n",
       "      <td>To succeed in a business, one needs to know ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Regarding whether traditional buildings should...</td>\n",
       "      <td>Some people think old buildings should be dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Some people believe that the protection of wil...</td>\n",
       "      <td>Protect wild animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay  \\\n",
       "0  Assuring road safety proves to be of great nec...   \n",
       "1  The income inequality among people has trigger...   \n",
       "2  Choosing a major is one of the most critical d...   \n",
       "3  Recently, our environment has been in the lime...   \n",
       "4  Several people believe that Maths is regarded ...   \n",
       "5  Regarding whether traditional buildings should...   \n",
       "6  Some people believe that the protection of wil...   \n",
       "\n",
       "                                               topic  \n",
       "0  The only way to improve safety on our roads is...  \n",
       "1     Government should assist disadvantaged people.  \n",
       "2  More people should be encouraged to do vocatio...  \n",
       "3  Living in big cities is bad for your health. T...  \n",
       "4  To succeed in a business, one needs to know ma...  \n",
       "5  Some people think old buildings should be dest...  \n",
       "6                               Protect wild animals  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_944/259051198.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat  0  times | most common phrase is ** assuring road safety proves to ** |number of repeataion happened:  1  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.5876288659793815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_280392/3978411362.py:252: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if nlp(w2).similarity(nlp(w1))>0:\n",
      "/tmp/ipykernel_280392/3978411362.py:255: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  elif nlp(w2).similarity(nlp(w1))<0:\n",
      "/tmp/ipykernel_280392/3978411362.py:261: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  total+=nlp(w2).similarity(nlp(w1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat  0  times | most common phrase is ** the income inequality among people ** |number of repeataion happened:  1  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.5483870967741935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_280392/3978411362.py:252: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if nlp(w2).similarity(nlp(w1))>0:\n",
      "/tmp/ipykernel_280392/3978411362.py:255: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  elif nlp(w2).similarity(nlp(w1))<0:\n",
      "/tmp/ipykernel_280392/3978411362.py:261: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  total+=nlp(w2).similarity(nlp(w1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat  0  times | most common phrase is ** choosing a major is one ** |number of repeataion happened:  1  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.48010610079575594\n",
      "repeat  0  times | most common phrase is ** recently our environment has been ** |number of repeataion happened:  1  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.5654450261780105\n",
      "repeat  0  times | most common phrase is ** several people believe that maths ** |number of repeataion happened:  1  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.5973597359735974\n",
      "repeat  0  times | most common phrase is ** regarding whether traditional buildings should ** |number of repeataion happened:  1  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.489010989010989\n",
      "repeat  2  times | most common phrase is ** waste of time and money ** |number of repeataion happened:  2  times\n",
      "\n",
      "Lexical Diversity Score of essay: 0.5139664804469274\n",
      "The number of columns in the CSV file is: 40\n"
     ]
    }
   ],
   "source": [
    "#data=df\n",
    "ess=data.loc[:,'essay']\n",
    "\n",
    "for i in range(len(ess)):\n",
    "    pc=para_cnt(ess[i])\n",
    "    data.loc[i,'para_count']=pc\n",
    "\n",
    "    wc=word_cnt(ess[i])\n",
    "    data.loc[i,'word_count']=wc\n",
    "\n",
    "    ns=sen_num(ess[i])\n",
    "    data.loc[i,'sentence_count']=ns\n",
    "\n",
    "    avg,md,mx,mn=sen_len(ess[i])\n",
    "    data.loc[i,'sentence_len(mean)']=avg\n",
    "    data.loc[i,'sentence_len(mode)']=md\n",
    "    data.loc[i,'sentence_len(max)']=mx\n",
    "    data.loc[i,'sentence_len(min)']=mn\n",
    "\n",
    "    sw=stop_w(ess[i])\n",
    "    swr=stop_w_r(ess[i])\n",
    "    data.loc[i,'stop_words']=sw\n",
    "    data.loc[i,'sw_rates']=swr\n",
    "\n",
    "    uw=uniq_w(ess[i])\n",
    "    data.loc[i,'unique_words']=uw\n",
    "\n",
    "    uw=uniq_w(ess[i])\n",
    "    mf,n,v,a,p=most_frequent(ess[i])\n",
    "    mvc,mv=get_modal_verbs(ess[i])\n",
    "    data.loc[i,'noun']=n\n",
    "    data.loc[i,'verb']=v\n",
    "    data.loc[i,'adj']=a\n",
    "    data.loc[i,'pron']=p,\n",
    "    data.loc[i,'modal verb']=mvc\n",
    "    data.loc[i,'most_freq']=mf\n",
    "\n",
    "    tr,mr,rep_list=rep_count(ess[i])\n",
    "    data.loc[i,'number_of_repeatation']=tr\n",
    "    data.loc[i,'max_occurance_of_most_frequent_phrase']=mr\n",
    "\n",
    "    asc=avg_sentence_complexity(ess[i])\n",
    "    data.loc[i,'avg sentence complexity']=asc\n",
    "\n",
    "    l_d=lexical_diversity(ess[i])\n",
    "    data.loc[i,'lexical diversity']=l_d\n",
    "\n",
    "    sensim,sensim_list=sentence_similarity(ess[i])\n",
    "    sensim_list.sort()\n",
    "    sensim_mod=statistics.mode(sensim_list)\n",
    "    sensim_med=statistics.median(sensim_list)\n",
    "    data.loc[i,'sentence similarity mean']=sensim\n",
    "    data.loc[i,'sentence similarity median']=sensim_med\n",
    "    data.loc[i,'sentence similarity mode']=sensim_mod\n",
    "\n",
    "    pos,neg,tot,p_s,n_s=word_similarity(ess[i])\n",
    "    data.loc[i,'word_sim_p_count']=pos\n",
    "    data.loc[i,'word_sim_p_score']=p_s\n",
    "    data.loc[i,'word_sim_n_count']=neg\n",
    "    data.loc[i,'word_sim_n_score']=n_s\n",
    "    data.loc[i,'word_sim_score']=tot\n",
    "\n",
    "    f_g, ari, c_l = calculate_readability(ess[i])\n",
    "    data.loc[i,'flesch grade']=f_g\n",
    "    data.loc[i,'Automated Readability']=ari\n",
    "    data.loc[i,'coleman_liau']=c_l\n",
    "\n",
    "    coh_score=calculate_cohesion(ess[i])\n",
    "    data.loc[i,'cohesion score']=coh_score\n",
    "\n",
    "    uniq = calculate_uniqueness_score(ess[i])\n",
    "    data.loc[i,'uniqueness of words']=uniq\n",
    "\n",
    "    emot = detect_emotional_tone(ess[i])\n",
    "    data.loc[i,'positive_sentiment']=emot['pos']\n",
    "    data.loc[i,'negative_sentiment']=emot['neg']\n",
    "    data.loc[i,'neutral_sentiment']=emot['neu']\n",
    "\n",
    "    cnt=discourse_marker_check(ess[i])\n",
    "    data.loc[i,'discourse_marker']=cnt\n",
    "\n",
    "    data.loc[i,'label']=0\n",
    "\n",
    "print(f\"The number of columns in the CSV file is: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>topic</th>\n",
       "      <th>para_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>sentence_len(mean)</th>\n",
       "      <th>sentence_len(mode)</th>\n",
       "      <th>sentence_len(max)</th>\n",
       "      <th>sentence_len(min)</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch grade</th>\n",
       "      <th>Automated Readability</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>cohesion score</th>\n",
       "      <th>uniqueness of words</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>discourse_marker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assuring road safety proves to be of great nec...</td>\n",
       "      <td>The only way to improve safety on our roads is...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.133333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.9</td>\n",
       "      <td>15.2</td>\n",
       "      <td>13.81</td>\n",
       "      <td>4.728559</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.752</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The income inequality among people has trigger...</td>\n",
       "      <td>Government should assist disadvantaged people.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>18.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5</td>\n",
       "      <td>13.2</td>\n",
       "      <td>13.23</td>\n",
       "      <td>4.189334</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.779</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Choosing a major is one of the most critical d...</td>\n",
       "      <td>More people should be encouraged to do vocatio...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.187500</td>\n",
       "      <td>13.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.4</td>\n",
       "      <td>13.6</td>\n",
       "      <td>12.07</td>\n",
       "      <td>4.019253</td>\n",
       "      <td>0.480106</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.792</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recently, our environment has been in the lime...</td>\n",
       "      <td>Living in big cities is bad for your health. T...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.933333</td>\n",
       "      <td>23.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.2</td>\n",
       "      <td>14.2</td>\n",
       "      <td>12.07</td>\n",
       "      <td>4.739021</td>\n",
       "      <td>0.565445</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.785</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Several people believe that Maths is regarded ...</td>\n",
       "      <td>To succeed in a business, one needs to know ma...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.625000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.2</td>\n",
       "      <td>16.31</td>\n",
       "      <td>4.465002</td>\n",
       "      <td>0.597360</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.809</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Regarding whether traditional buildings should...</td>\n",
       "      <td>Some people think old buildings should be dest...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.437500</td>\n",
       "      <td>13.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.1</td>\n",
       "      <td>11.9</td>\n",
       "      <td>10.56</td>\n",
       "      <td>4.220486</td>\n",
       "      <td>0.489011</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.830</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Some people believe that the protection of wil...</td>\n",
       "      <td>Protect wild animals</td>\n",
       "      <td>4.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.076923</td>\n",
       "      <td>11.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.7</td>\n",
       "      <td>15.9</td>\n",
       "      <td>13.29</td>\n",
       "      <td>4.457519</td>\n",
       "      <td>0.513966</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.812</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay  \\\n",
       "0  Assuring road safety proves to be of great nec...   \n",
       "1  The income inequality among people has trigger...   \n",
       "2  Choosing a major is one of the most critical d...   \n",
       "3  Recently, our environment has been in the lime...   \n",
       "4  Several people believe that Maths is regarded ...   \n",
       "5  Regarding whether traditional buildings should...   \n",
       "6  Some people believe that the protection of wil...   \n",
       "\n",
       "                                               topic  para_count  word_count  \\\n",
       "0  The only way to improve safety on our roads is...         4.0       347.0   \n",
       "1     Government should assist disadvantaged people.         4.0       280.0   \n",
       "2  More people should be encouraged to do vocatio...         5.0       339.0   \n",
       "3  Living in big cities is bad for your health. T...         4.0       344.0   \n",
       "4  To succeed in a business, one needs to know ma...         4.0       277.0   \n",
       "5  Some people think old buildings should be dest...         5.0       327.0   \n",
       "6                               Protect wild animals         4.0       313.0   \n",
       "\n",
       "   sentence_count  sentence_len(mean)  sentence_len(mode)  sentence_len(max)  \\\n",
       "0            15.0           23.133333                20.0               39.0   \n",
       "1            15.0           18.666667                18.0               40.0   \n",
       "2            16.0           21.187500                13.0               37.0   \n",
       "3            15.0           22.933333                23.0               54.0   \n",
       "4             8.0           34.625000                18.0               53.0   \n",
       "5            16.0           20.437500                13.0               47.0   \n",
       "6            13.0           24.076923                11.0               49.0   \n",
       "\n",
       "   sentence_len(min)  stop_words  ...  flesch grade  Automated Readability  \\\n",
       "0               12.0       169.0  ...          12.9                   15.2   \n",
       "1                9.0       139.0  ...          10.5                   13.2   \n",
       "2               11.0       202.0  ...          10.4                   13.6   \n",
       "3                9.0       176.0  ...          12.2                   14.2   \n",
       "4               18.0       142.0  ...          20.3                   23.2   \n",
       "5               11.0       190.0  ...          10.1                   11.9   \n",
       "6               11.0       161.0  ...          12.7                   15.9   \n",
       "\n",
       "   coleman_liau  cohesion score  uniqueness of words  positive_sentiment  \\\n",
       "0         13.81        4.728559             0.587629               0.152   \n",
       "1         13.23        4.189334             0.548387               0.139   \n",
       "2         12.07        4.019253             0.480106               0.156   \n",
       "3         12.07        4.739021             0.565445               0.076   \n",
       "4         16.31        4.465002             0.597360               0.165   \n",
       "5         10.56        4.220486             0.489011               0.088   \n",
       "6         13.29        4.457519             0.513966               0.119   \n",
       "\n",
       "   negative_sentiment  neutral_sentiment  discourse_marker  label  \n",
       "0               0.096              0.752              54.0    0.0  \n",
       "1               0.082              0.779              30.0    0.0  \n",
       "2               0.053              0.792              52.0    0.0  \n",
       "3               0.139              0.785              57.0    0.0  \n",
       "4               0.026              0.809              45.0    0.0  \n",
       "5               0.082              0.830              39.0    0.0  \n",
       "6               0.070              0.812              36.0    0.0  \n",
       "\n",
       "[7 rows x 40 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../../csv/human/csv_human-1.csv',mode='a',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../csv/human/csv_human-1.csv')\n",
    "ess=data.loc[:,'essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "def grammar_check1(text):\n",
    "# Create separate LanguageTool instances for GB and US English\n",
    "    tool_gb = language_tool_python.LanguageTool('en-GB')\n",
    "    tool_us = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "\n",
    "# Check for GB English grammar errors\n",
    "    matches_gb = tool_gb.check(text)\n",
    "    print(\"gb \",matches_gb)\n",
    "    list_gb=set(rule.ruleId for rule in tool_gb.check(text))\n",
    "\n",
    "# Check for US English grammar errors\n",
    "    matches_us = tool_us.check(text)\n",
    "    print(\"us \",matches_us)\n",
    "    list_us=set(rule.ruleId for rule in tool_us.check(text))\n",
    "\n",
    "    '''common_errors = [\n",
    "    error for error in matches_gb\n",
    "    if any(error.matches_gb(match) for match in matches_us)\n",
    "    ]'''\n",
    "\n",
    "    common_mistakes = list_gb.intersection(list_us)\n",
    "\n",
    "    print(\"Errors common to GB and US English:\")\n",
    "    print(common_mistakes)\n",
    "    '''for error in common_errors:\n",
    "        print(error)'''\n",
    "\n",
    "# Filter errors that do not match in either GB or US English\n",
    "#    non_matching_errors = [\n",
    "#        error for error in matches_gb + matches_us\n",
    "#        if not any(error.matches(match) for match in matches_gb + matches_us)\n",
    "#    ]\n",
    "\n",
    "#    print(\"Errors not matching in either GB or US English:\")\n",
    "#    for error in non_matching_errors:\n",
    "#        print(error)\n",
    "\n",
    "for i in range(len(ess)):\n",
    "    grammar_check1(ess[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
