{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import statistics\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import language_tool_python  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gingerit.gingerit import GingerIt\n",
    "import pprint\n",
    "from spacy import displacy\n",
    "from sklearn.svm import SVC\n",
    "import textstat\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import math\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_tree1(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        dep=[]\n",
    "        d=0\n",
    "        for child in node.children:\n",
    "            d=walk_tree1(child, depth + 1)\n",
    "            dep.append(d)\n",
    "        return max(dep)\n",
    "    \n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "def avg_sentence_complexity(str1):\n",
    "    final_list=[]\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    temp_list=[]\n",
    "    sents=[]\n",
    "    txt = str1\n",
    "    txt=\" \".join(txt.split())\n",
    "    text = nlp(txt)\n",
    "    sent_tokenize1 = (list(text.sents))\n",
    "    sents.append(sent_tokenize1[0])\n",
    "    for sent in text.sents:\n",
    "        d=walk_tree1(sent.root, 0)\n",
    "        temp_list.append(d)\n",
    "            #displacy.render(sent, style=\"dep\")\n",
    "        final_list.append(temp_list)\n",
    "        #print(\"essay first line \",sents[len(sents)-1],\"max is \",max(temp_list),\" min is \",min(temp_list),\" mean is \", round(statistics.mean(temp_list),3),\n",
    "        #  \" median is \",statistics.median(temp_list),\" mode is \",statistics.mode(temp_list))\n",
    "    return round(statistics.mean(temp_list),3)\n",
    "\n",
    "def grammar_check(str1):\n",
    "\n",
    "    my_tool = language_tool_python.LanguageTool('en-GB')###########\n",
    "    matches = my_tool.check(str1)\n",
    "    list_s1=[]\n",
    "    if(matches):\n",
    "        fs=str1.split(\".\")[0]\n",
    "        #print(i,\" index | essay first sentences \",fs)\n",
    "        for match in matches:\n",
    "            #print(match)\n",
    "    #print(match.ruleId)\n",
    "    #print('Message:',match.message,\"\\n\")\n",
    "            #print(\"####################### \",match.context)\n",
    "            if match.replacements:\n",
    "                chopped_string = match.replacements[0].replace(',','').strip()\n",
    "                list_s1.append(chopped_string)\n",
    "            else:\n",
    "                list_s1.append(match.ruleId)    \n",
    "        #print(\"final list\",list_s1)\n",
    "    return len(list_s1)\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list1=[\" \".join(ngram) for ngram in ngrams]\n",
    "    #return [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list1\n",
    "\n",
    "def rep_count(str1):\n",
    "    tot_rp=[]\n",
    "    max_rp=[]\n",
    "    #print(ess[i],\"\\n\",\"new\",\"\\n\")\n",
    "    list1=generate_ngrams(str1, 5)\n",
    "    myset = set(list1)\n",
    "    rept = len(list1) - len(myset)\n",
    "    tot_rp.append(rept)\n",
    "    c = collections.Counter(list1)\n",
    "    c.most_common(1)[0][1]\n",
    "    max_rp.append(c.most_common(1)[0][1])\n",
    "    #print(\"\\n\\nnew essay index \",i,\": for essay \",list1[0],\":\")\n",
    "    print(\"repeat \",rept,\" times | most common phrase is **\",c.most_common(1)[0][0], \"** |number of repeataion happened: \",c.most_common(1)[0][1],\" times\\n\")\n",
    "    #print(\"\\ncommon phrases: \\n\",c)\n",
    "    #print(\"\\nmy set: \\n\",myset)\n",
    "    #print(\"list \",tot_rp,\" \",max_rp)\n",
    "    #print(\"individual \",rept,\" \",c.most_common(1)[0][1])\n",
    "    return rept,c.most_common(1)[0][1],c.most_common(10)\n",
    "\n",
    "def most_frequent(str1):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    str1=\" \".join(str1.split())\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop!= True and token.is_punct!= True]\n",
    "    w_c=Counter(words)\n",
    "    m_f=w_c.most_common(1)[0][1]\n",
    "    \n",
    "    tags=[]\n",
    "    for w in text:\n",
    "        tags.append(w.pos_)\n",
    "    tag_freq = Counter(tags)\n",
    "    #print (\"=====tag FREQUENCY=====\")\n",
    "    #print(tag_freq.values())\n",
    "\n",
    "    w_l=word_cnt(str1)\n",
    "    for key in tag_freq:\n",
    "        #print(\"hi\")\n",
    "        if key=='NOUN':\n",
    "            n=tag_freq[key]/w_l\n",
    "            #print(\"n \",n)\n",
    "        elif key == 'VERB':\n",
    "            v=tag_freq[key]/w_l\n",
    "            #print(\"v \",v)\n",
    "        elif key =='ADJ':\n",
    "            a=tag_freq[key]/w_l\n",
    "            #print(\"a \",a)\n",
    "        elif key =='PRON':\n",
    "            p=tag_freq[key]/w_l\n",
    "\n",
    "        elif key=='MD':\n",
    "            print(\"the number of modal verbs \",key, tag_freq.values())\n",
    "            #print(\"p \",p)\n",
    "        #print(\"key is \",key,\" value \",tag_freq[key], \" frequency \", tag_freq[key]/w_l)\n",
    "    return m_f,n,v,a,p\n",
    "\n",
    "def uniq_w(str1):\n",
    "    return len(set(str1.split()))\n",
    "\n",
    "def stop_w(str1):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    return(len(words))\n",
    "\n",
    "def stop_w_r(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    text = nlp(str1)\n",
    "    words = [token.text for token in text if token.is_stop == True]\n",
    "    #print(words)\n",
    "    \n",
    "    l=len(words)\n",
    "    swr=l/n\n",
    "    return swr\n",
    "\n",
    "def sen_len(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    i=0\n",
    "    avg=0\n",
    "    sl=[]\n",
    "    for num in number_of_sentences:\n",
    "        i+=1\n",
    "        words=num.split()\n",
    "        avg+=len(words)\n",
    "        sl.append(len(words))\n",
    "    #print(num, len(num),len(words))\n",
    "    avg=avg/i\n",
    "    md=statistics.mode(sl)\n",
    "    mx=max(sl)\n",
    "    mn=min(sl)\n",
    "    return avg,md,mx,mn\n",
    "\n",
    "def sen_num(str1):\n",
    "    number_of_sentences = sent_tokenize(str1)\n",
    "    n=len(number_of_sentences)\n",
    "    #print(n)\n",
    "    return n\n",
    "\n",
    "def word_cnt(str1):\n",
    "    words=str1.split()\n",
    "    return len(words)\n",
    "\n",
    "def lexical_diversity(str1):\n",
    "    tokens = word_tokenize(str1)\n",
    "\n",
    "    lex_div = len(set(tokens)) / len(tokens)\n",
    "    print(\"Lexical Diversity Score of essay:\", lex_div)\n",
    "    return lex_div\n",
    "\n",
    "def discourse_conn(str1):\n",
    "    tokens = nltk.word_tokenize(str1)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Identify discourse connectors based on part-of-speech patterns\n",
    "    discourse_connectors = [token for token, pos in pos_tags if pos in ['CC', 'RB', 'IN']]\n",
    "\n",
    "    # Output the identified discourse connectors\n",
    "    print(\"Discourse connectors:\", discourse_connectors)\n",
    "    print(\"Discourse connector count:\", len(discourse_connectors))\n",
    "    return len(discourse_connectors)\n",
    "\n",
    "def sentence_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    sent_tokenize = (list(text.sents))\n",
    "    i=0\n",
    "    sensim=0.0\n",
    "    sensim_list=[]\n",
    "    for sent2 in sent_tokenize:\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            sent1=sent2\n",
    "            continue\n",
    "        #print (sent2.similarity(sent1))\n",
    "        sensim+=sent2.similarity(sent1)\n",
    "        sensim_score=sent2.similarity(sent1)\n",
    "        sent1=sent2\n",
    "        sensim_list.append(sensim_score)\n",
    "    sensim=sensim/i\n",
    "    #print(\"average sentence similarity is \",sensim)\n",
    "\n",
    "    return sensim,sensim_list\n",
    "\n",
    "def word_similarity(str1):\n",
    "    text = nlp(str1)\n",
    "    p=0\n",
    "    ps=0\n",
    "    n=0\n",
    "    ns=0\n",
    "    total=0\n",
    "    word = [token.text for token in text if token.is_punct != True]\n",
    "    for i in range(len(word)):\n",
    "    #x.append(i)\n",
    "    #print(i)\n",
    "    #print(words[i])\n",
    "        w2=word[i]\n",
    "        if i == 0:\n",
    "            w1=w2\n",
    "            continue\n",
    "    #print (nlp(w2).similarity(nlp(w1)))\n",
    "        #print(w1+ \" \"+w2+\" \",nlp(w2).similarity(nlp(w1)))\n",
    "        if nlp(w2).similarity(nlp(w1))>0:\n",
    "            p=p+1\n",
    "            ps+=nlp(w2).similarity(nlp(w1))\n",
    "        elif nlp(w2).similarity(nlp(w1))<0:\n",
    "            n=n+1\n",
    "            ns+=nlp(w2).similarity(nlp(w1))\n",
    "    #y1=y1+float(nlp(w2).similarity(nlp(w1)))\n",
    "    #print(y1)\n",
    "    #y.append(y1)\n",
    "        total+=nlp(w2).similarity(nlp(w1))\n",
    "        w1=w2\n",
    "    #print(p,n)\n",
    "    return p,n,total,ps,ns\n",
    "\n",
    "def get_modal_verbs(text):\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    cnt=0\n",
    "    modal_verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.tag_ == 'MD':\n",
    "            modal_verbs.append(token.lemma_)\n",
    "            cnt+=1\n",
    "\n",
    "    return cnt,modal_verbs\n",
    "\n",
    "def calculate_readability(text):\n",
    "    # Calculate the Flesch-Kincaid Grade Level\n",
    "    flesch_grade = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    # Calculate the Automated Readability Index (ARI)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    # Calculate the Coleman-Liau Index\n",
    "    coleman_liau = textstat.coleman_liau_index(text)\n",
    "\n",
    "    # Return the readability scores\n",
    "    return flesch_grade, ari, coleman_liau\n",
    "\n",
    "def calculate_cohesion(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Convert generator to a list of bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # Calculate bigram frequencies\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Calculate pointwise mutual information (PMI) for bigrams\n",
    "    total_bigrams = len(bigrams)\n",
    "    pmi_scores = {}\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        pmi = math.log(freq / total_bigrams / ((tokens.count(bigram[0]) / len(tokens)) * (tokens.count(bigram[1]) / len(tokens))))\n",
    "        pmi_scores[bigram] = pmi\n",
    "\n",
    "    # Calculate average PMI as a measure of cohesion\n",
    "    avg_pmi = sum(pmi_scores.values()) / len(pmi_scores)\n",
    "\n",
    "    return avg_pmi\n",
    "\n",
    "def calculate_uniqueness_score(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Calculate the frequency distribution of words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Calculate the uniqueness score as the ratio of unique words to total words\n",
    "    uniqueness_score = len(word_freq) / len(words)\n",
    "\n",
    "    return uniqueness_score\n",
    "\n",
    "def detect_emotional_tone(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['essay','topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if ch=='e' or 'E':\\nbreak\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = input('Enter a filename: ')\n",
    "with open('human1/'+filename, 'r') as file:\n",
    "    #txt = file.read()\n",
    "        #print(txt)\n",
    "    #first_line = file.readline()\n",
    "    #lines = file.readlines()[1:]\n",
    "    first_line = file.readline().strip()\n",
    "    remaining_content = file.read()\n",
    "#content = ''.join(lines)\n",
    "#ch=input(\"enter e\")\n",
    "'''if ch=='e' or 'E':\n",
    "break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['essay','topic'])\n",
    "df.to_csv('check_csv.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "df = pd.DataFrame()\n",
    "for path in glob.glob('./human1/*.txt'):   \n",
    "    with open(path) as file:\n",
    "        first_line = file.readline().strip()\n",
    "        remaining_content = file.read()\n",
    "        if remaining_content.startswith('\\n'):\n",
    "            remaining_content = remaining_content[1:]\n",
    "    df=df.append({'essay':remaining_content,'topic':first_line},ignore_index=True)\n",
    "    ch=input(\"enter e\")\n",
    "    if ch =='e' or ch =='E':\n",
    "        break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First of all, schools offer to students a good...</td>\n",
       "      <td>Many people believe that children should study...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To begin with, distance-learning brings signif...</td>\n",
       "      <td>A matter of considerable controversy at presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>During the history of the world, every change ...</td>\n",
       "      <td>Even though globalization affects the world's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay  \\\n",
       "0  First of all, schools offer to students a good...   \n",
       "1  To begin with, distance-learning brings signif...   \n",
       "2  During the history of the world, every change ...   \n",
       "\n",
       "                                               topic  \n",
       "0  Many people believe that children should study...  \n",
       "1  A matter of considerable controversy at presen...  \n",
       "2  Even though globalization affects the world's ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('check_csv.csv',mode='a',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During the history of the world, every change has its own positive and negative sides. Globalization as a gradual change affecting all over the world is not an exception. Although it has undeniable effects on the economics of the world; it has side effects which make it a controversial issue.\\n\\nSome people prefer to recognize globalization as a threat to ethnic and religious values of people of their country. They think that the idea of globalization put their inherited culture in danger of uncontrolled change and make them vulnerable against the attack of imperialistic governments.\\n\\nThose who disagree, believe that globalization contribute effectively to the global improvement of the world in many aspects. Developing globalization, people can have more access to many natural resources of the world and it leads to increasing the pace of scientific and economic promotions of the entire world. In addition, they admit that globalization can be considered a chance for people of each country to promote their lifestyle through the stuffs and services imported from other countries.\\n\\nMoreover, the proponents of globalization idea point out globalization results in considerable decrease in global tension due to convergence of benefits of people of the world which is a natural consequence of globalization.\\n\\nIn conclusion, I would rather classify myself in the proponents of globalization as a speeding factor of global progress. I think it is more likely to solve the problems of the world rather than intensifying them.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.at[2,'essay']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('check_csv.csv')\n",
    "ess=data.loc[:,'essay']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    wc=word_cnt(ess[i])\n",
    "    data.loc[i,'word_count']=wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('check_csv.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    ns=sen_num(ess[i])\n",
    "    data.loc[i,'sentence_count']=ns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    avg,md,mx,mn=sen_len(ess[i])\n",
    "    data.loc[i,'sentence_len(mean)']=avg\n",
    "    data.loc[i,'sentence_len(mode)']=md\n",
    "    data.loc[i,'sentence_len(max)']=mx\n",
    "    data.loc[i,'sentence_len(min)']=mn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    sw=stop_w(ess[i])\n",
    "    swr=stop_w_r(ess[i])\n",
    "    data.loc[i,'stop_words']=sw\n",
    "    data.loc[i,'sw_rates']=swr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    uw=uniq_w(ess[i])\n",
    "    data.loc[i,'unique_words']=uw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos and most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    uw=uniq_w(ess[i])\n",
    "    mf,n,v,a,p=most_frequent(ess[i])\n",
    "    mvc,mv=get_modal_verbs(ess[i])\n",
    "    data.loc[i,'noun']=n\n",
    "    data.loc[i,'verb']=v\n",
    "    data.loc[i,'adj']=a\n",
    "    data.loc[i,'pron']=p,\n",
    "    data.loc[i,'modal verb']=mvc\n",
    "    data.loc[i,'most_freq']=mf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeatation of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    tr,mr,rep_list=rep_count(ess[i])\n",
    "    data.loc[i,'total_rp']=tr\n",
    "    data.loc[i,'max_rp_count']=mr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    asc=avg_sentence_complexity(ess[i])\n",
    "    data.loc[i,'avg sentence complexity']=asc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grammar and spelling \n",
    "***GB and US***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    g_s=grammar_check(ess[i])\n",
    "    data.loc[i,'grammar&spel']=g_s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    l_d=lexical_diversity(ess[i])\n",
    "    data.loc[i,'lexical diversity']=l_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    sensim,sensim_list=sentence_similarity(ess[i])\n",
    "    sensim_list.sort()\n",
    "    sensim_mod=statistics.mode(sensim_list)\n",
    "    sensim_med=statistics.median(sensim_list)\n",
    "    data.loc[i,'sentence similarity mean']=sensim\n",
    "    data.loc[i,'sentence similarity median']=sensim_med\n",
    "    data.loc[i,'sentence similarity mode']=sensim_mod,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    pos,neg,tot,p_s,n_s=word_similarity(ess[i])\n",
    "    data.loc[i,'word_sim_p_count']=pos\n",
    "    data.loc[i,'word_sim_p_score']=p_s\n",
    "    data.loc[i,'word_sim_n_count']=neg\n",
    "    data.loc[i,'word_sim_n_score']=n_s\n",
    "    data.loc[i,'word_sim_score']=tot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    f_g, ari, c_l = calculate_readability(ess[i])\n",
    "    data.loc[i,'flesch grade']=f_g\n",
    "    data.loc[i,'Automated Readability']=ari\n",
    "    data.loc[i,'coleman_liau']=c_l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cohesion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    coh_score=calculate_cohesion(ess[i])\n",
    "    data.loc[i,'cohesion score']=coh_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    uniq = calculate_uniqueness_score(ess[i])\n",
    "    data.loc[i,'uniqueness of words']=uniq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emotional tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ess)):\n",
    "    emot = detect_emotional_tone(ess[i])\n",
    "    data.loc[i,'positive_sentiment']=emot['pos']\n",
    "    data.loc[i,'negative_sentiment']=emot['neg']\n",
    "    data.loc[i,'neutral_sentiment']=emot['neu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                  ,'discourse connectors':d_c,\n",
    "                  \n",
    "                  'cohesion score':coh_score,\n",
    "                  ,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    d_c=discourse_conn(str1)\n",
    "    \n",
    "    \n",
    "    #print(\"sensim and sensim_list\", sensim,sensim_list)\n",
    "    \n",
    "    #print(\"word similarity \",pos,neg,tot)\n",
    "    \n",
    "    f_g, ari, c_l = calculate_readability(str1)\n",
    "    coh_score=calculate_cohesion(str1)\n",
    "    uniq = calculate_uniqueness_score(str1)\n",
    "    emot = detect_emotional_tone(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that alternative forms of transport', 2),\n",
       " ('of transportation and international laws', 2),\n",
       " ('an increase in the number', 2),\n",
       " ('increase in the number of', 2),\n",
       " ('in the number of automobiles', 2),\n",
       " ('recently a great deal of', 1),\n",
       " ('a great deal of attention', 1),\n",
       " ('great deal of attention has', 1),\n",
       " ('deal of attention has been', 1),\n",
       " ('of attention has been paid', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.at[0,'total_rp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df(str1,df):\n",
    "    str1=\" \".join(str1.split())\n",
    "    ns=sen_num(str1)\n",
    "    wc=word_cnt(str1)\n",
    "    avg,md,mx,mn=sen_len(str1)\n",
    "    sw=stop_w(str1)\n",
    "    swr=stop_w_r(str1)\n",
    "    uw=uniq_w(str1)\n",
    "    mf,n,v,a,p=most_frequent(str1)\n",
    "    tr,mr,rep_list=rep_count(str1)\n",
    "    asc=avg_sentence_complexity(str1)\n",
    "    g_s=grammar_check(str1)\n",
    "    l_d=lexical_diversity(str1)\n",
    "    d_c=discourse_conn(str1)\n",
    "    sensim,sensim_list=sentence_similarity(str1)\n",
    "    sensim_list.sort()\n",
    "    sensim_mod=statistics.mode(sensim_list)\n",
    "    sensim_med=statistics.median(sensim_list)\n",
    "    #print(\"sensim and sensim_list\", sensim,sensim_list)\n",
    "    pos,neg,tot,p_s,n_s=word_similarity(str1)\n",
    "    #print(\"word similarity \",pos,neg,tot)\n",
    "    mvc,mv=get_modal_verbs(str1)\n",
    "    f_g, ari, c_l = calculate_readability(str1)\n",
    "    coh_score=calculate_cohesion(str1)\n",
    "    uniq = calculate_uniqueness_score(str1)\n",
    "    emot = detect_emotional_tone(str1)\n",
    "    \n",
    "    df=df.append({'word_count':wc,'sentence_count':ns,'sentence_len(mean)':avg,\n",
    "                  'sentence_len(mode)':md,'sentence_len(max)':mx,'sentence_len(min)':mn,'stop_words':sw,\n",
    "                  'sw_rates':swr,'unique_words':uw,'noun':n,'verb':v,'adj':a,'pron':p,'modal verb':mvc,'most_freq':mf,'total_rp':tr,'max_rp_count':mr,\n",
    "                  'avg sentence complexity':asc,'grammar&spel':g_s,'lexical diversity':l_d,'discourse connectors':d_c,'sentence similarity mean':sensim,\n",
    "                  'sentence similarity median':sensim_med,'sentence similarity mode':sensim_mod,'word_sim_p_count':pos,'word_sim_p_score':p_s,'word_sim_n_count':neg,\n",
    "                  'word_sim_n_score':n_s,'word_sim_score':tot,'flesch grade':f_g, 'Automated Readability':ari, 'coleman_liau':c_l,'cohesion score':coh_score,\n",
    "                  'uniqueness of words':uniq,'positive_sentiment':emot['pos'],'negative_sentiment':emot['neg'],'neutral_sentiment':emot['neu']},ignore_index=True)\n",
    "    return df,rep_list\n",
    "\n",
    "filename = input('Enter a filename: ')\n",
    "with open('human1/'+filename, 'r') as file:\n",
    "    txt = file.read()\n",
    "        #print(txt)\n",
    "    df,rep_list=append_df(txt,df)\n",
    "    \n",
    "#str1.append(reply)\n",
    "#strlen+=int(len(message))\n",
    "#df=df.drop(len(df)-1)\n",
    "#str1=input(\"enter the essay\")\n",
    "#df=append_df(str1,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyphen\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.14.0 textstat-0.7.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['should', 'should', 'will', 'should', 'can', 'can', 'can', 'can', 'would']\n"
     ]
    }
   ],
   "source": [
    "def get_modal_verbs(text):\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    cnt=0\n",
    "    modal_verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.tag_ == 'MD':\n",
    "            modal_verbs.append(token.lemma_)\n",
    "            cnt+=1\n",
    "\n",
    "    return cnt,modal_verbs\n",
    "\n",
    "# Example usage\n",
    "#text = \"I should have gone to the party, but I couldn't because I had to work.\"\n",
    "filename = input('Enter a filename: ')\n",
    "with open('human1/'+filename, 'r') as file:\n",
    "    txt = file.read()\n",
    "modal_verbs = get_modal_verbs(txt)\n",
    "print(modal_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should: 3\n",
      "will: 1\n",
      "can: 4\n",
      "would: 1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def count_modal_verbs(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Part-of-speech tagging\n",
    "    pos_tags = [nltk.pos_tag(sentence) for sentence in words]\n",
    "\n",
    "    # Extract modal verbs\n",
    "    modal_verbs = []\n",
    "    for pos_tags_sentence in pos_tags:\n",
    "        for word, tag in pos_tags_sentence:\n",
    "            if tag.startswith('MD'):\n",
    "                modal_verbs.append(word)\n",
    "\n",
    "    # Count the modal verbs\n",
    "    modal_verbs_count = Counter(modal_verbs)\n",
    "\n",
    "    return modal_verbs_count\n",
    "\n",
    "# Example usage\n",
    "#text = \"I should have gone to the party, but I couldn't because I had to work.\"\n",
    "modal_verbs_count = count_modal_verbs(txt)\n",
    "\n",
    "# Print the modal verbs and their counts\n",
    "for modal_verb, count in modal_verbs_count.items():\n",
    "    print(f\"{modal_verb}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recently, a great deal of attention has been paid to means of transportation. Some state that automobile is the most convenient transportation, while others assert that alternative forms of transport should be considered in view of environmental pollution. In my opinion, I strongly agree that alternative means of transportation and international laws in controlling automobiles should be introduced. In this essay, several reasons will be described carefully in the following passages.\\n\\nFirstly, an increase in the number of automobiles brings about environmental pollution. Since automobile exhaust and carbon dioxides emission have bad impacts on the air, the air gets to be polluted and it causes respiratory disease of the local inhabitants. Therefore, it is crucial to alter automobiles to the other means of transportation and to legislate to control automobiles.\\n\\nSecondly, an increase in the number of automobiles causes terrible traffic congestion. Once we get into the traffic jam, it forces us to waste our time for a while. What is worse, traffic congestion irritates drivers and as a result, it leads to an increase of traffic accidents. Thus, the methods to reduce the number of automobiles and to control car ownership and use should be immediately planned and implemented.\\n\\nIn order to solve these problems, alternative forms of transportation and international laws to control automobiles ownership and use can work effectively. With regard to the former, alternative forms which emit less carbon dioxides and gasses than automobiles are ideal. For example, rental bicycle system has been introduced in several European countries. This system is that at the corner of each district, the rental bicycles are placed and the people can ride it to the other districts with a small fee. Since bicycle expels neither carbon dioxides nor harmful gasses, it is a better form to protect environment than automobile. As for the latter point, the introduction of international laws can be also effective. For instance, there are some regulations to limit the use of automobiles by time and to allow the people possess one car only. Thus, these attempts to reduce automobiles can ease traffic congestion and environmental pollution.\\n\\nIn summary, even though some people claim the convenience of automobile, I would state that alternative forms of transport and international laws on controlling automobiles are necessary due to the environmental protection and the alleviation of traffic congestion.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch-Kincaid Grade Level: 11.2\n",
      "Automated Readability Index (ARI): 14.7\n",
      "Coleman-Liau Index: 15.37\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability(text):\n",
    "    # Calculate the Flesch-Kincaid Grade Level\n",
    "    flesch_grade = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    # Calculate the Automated Readability Index (ARI)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    # Calculate the Coleman-Liau Index\n",
    "    coleman_liau = textstat.coleman_liau_index(text)\n",
    "\n",
    "    # Return the readability scores\n",
    "    return flesch_grade, ari, coleman_liau\n",
    "\n",
    "# Example usage:\n",
    "essay = \"\"\"\n",
    "Climate change is a pressing issue that requires immediate attention. It affects various aspects of our planet, including the environment, economy, and human health. The rise in greenhouse gas emissions contributes to global warming, leading to rising temperatures, melting ice caps, and more frequent extreme weather events. It is crucial to take proactive measures to reduce carbon emissions and promote sustainable practices to mitigate the effects of climate change.\n",
    "\"\"\"\n",
    "\n",
    "flesch_grade, ari, coleman_liau = calculate_readability(essay)\n",
    "print(\"Flesch-Kincaid Grade Level:\", flesch_grade)\n",
    "print(\"Automated Readability Index (ARI):\", ari)\n",
    "print(\"Coleman-Liau Index:\", coleman_liau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohesion score: 3.763830912177253\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "'''def calculate_cohesion(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Calculate bigram frequencies\n",
    "    bigrams = nltk.bigrams(tokens)\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Calculate pointwise mutual information (PMI) for bigrams\n",
    "    total_bigrams = len(bigrams)\n",
    "    pmi_scores = {}\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        pmi = math.log(freq / total_bigrams / ((tokens.count(bigram[0]) / len(tokens)) * (tokens.count(bigram[1]) / len(tokens))))\n",
    "        pmi_scores[bigram] = pmi\n",
    "\n",
    "    # Calculate average PMI as a measure of cohesion\n",
    "    avg_pmi = sum(pmi_scores.values()) / len(pmi_scores)\n",
    "\n",
    "    return avg_pmi\n",
    "'''\n",
    "def calculate_cohesion(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Convert generator to a list of bigrams\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "\n",
    "    # Calculate bigram frequencies\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    # Calculate pointwise mutual information (PMI) for bigrams\n",
    "    total_bigrams = len(bigrams)\n",
    "    pmi_scores = {}\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        pmi = math.log(freq / total_bigrams / ((tokens.count(bigram[0]) / len(tokens)) * (tokens.count(bigram[1]) / len(tokens))))\n",
    "        pmi_scores[bigram] = pmi\n",
    "\n",
    "    # Calculate average PMI as a measure of cohesion\n",
    "    avg_pmi = sum(pmi_scores.values()) / len(pmi_scores)\n",
    "\n",
    "    return avg_pmi\n",
    "# Example usage:\n",
    "essay = 'Climate change is a pressing issue that requires immediate attention. It affects various aspects of our planet, including the environment, economy, and human health. The rise in greenhouse gas emissions contributes to global warming, leading to rising temperatures, melting ice caps, and more frequent extreme weather events. It is crucial to take proactive measures to reduce carbon emissions and promote sustainable practices to mitigate the effects of climate change.'\n",
    "\n",
    "cohesion = calculate_cohesion(essay)\n",
    "print(\"Cohesion score:\", cohesion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "tool = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style Score: 0\n"
     ]
    }
   ],
   "source": [
    "def calculate_style_score(text):\n",
    "    matches = tool.check(text)\n",
    "    style_violations = 0\n",
    "    style_score=0\n",
    "\n",
    "    for match in matches:\n",
    "        print(match)\n",
    "        if match.category == 'Style':\n",
    "            style_violations += 1\n",
    "    \n",
    "    if matches:\n",
    "        style_score = 1 - (style_violations / len(matches))  # Simple formula, adjust as needed\n",
    "\n",
    "    return style_score\n",
    "\n",
    "style_score = calculate_style_score(essay)\n",
    "\n",
    "print(\"Style Score:\", style_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7468354430379747\n",
      "Essay is: None\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_uniqueness_score(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Calculate the frequency distribution of words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Calculate the uniqueness score as the ratio of unique words to total words\n",
    "    uniqueness_score = len(word_freq) / len(words)\n",
    "\n",
    "    return uniqueness_score\n",
    "\n",
    "def detect_creativity_and_originality(text):\n",
    "    uniqueness_score = calculate_uniqueness_score(text)\n",
    "\n",
    "    # Adjust the threshold as per your requirements\n",
    "    '''threshold = 0.7\n",
    "\n",
    "    if uniqueness_score > threshold:\n",
    "        return \"Creative and Original\"\n",
    "    else:\n",
    "        return \"Generic and Repetitive\"\n",
    "    '''\n",
    "    print(uniqueness_score)\n",
    "    \n",
    "result = detect_creativity_and_originality(essay)\n",
    "print(\"Essay is:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiemnt score  {'neg': 0.0, 'neu': 0.903, 'pos': 0.097, 'compound': 0.7184}\n",
      "Emotional Tone: Positive\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def detect_emotional_tone(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    print(\"sentiemnt score \", sentiment_scores)\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "emotional_tone = detect_emotional_tone(essay)\n",
    "print(\"Emotional Tone:\", emotional_tone)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
